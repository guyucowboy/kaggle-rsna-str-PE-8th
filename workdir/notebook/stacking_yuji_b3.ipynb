{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.5 64-bit",
   "display_name": "Python 3.8.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/home/haito/kaggle/rsna-str-2/workdir\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "sys.path.append(\".\")\n",
    "from src.factory import *\n",
    "from src.utils import *\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = Path(\"../input/rsna-str-pulmonary-embolism-detection/\")\n",
    "\n",
    "train = pd.read_csv(DATADIR / \"train.csv\")\n",
    "\n",
    "pre = pd.read_csv(DATADIR / \"split.csv\")\n",
    "train = train.merge(pre, on=\"StudyInstanceUID\")\n",
    "\n",
    "portion = pd.read_csv(DATADIR / \"study_pos_portion.csv\")\n",
    "train = train.merge(portion, on=\"StudyInstanceUID\")\n",
    "\n",
    "z_pos_df = pd.read_csv(DATADIR / \"sop_to_prefix.csv\").rename(columns={'img_prefix': 'z_pos'})\n",
    "train = train.merge(z_pos_df, on=\"SOPInstanceUID\")\n",
    "\n",
    "\n",
    "### train = train.query(\"fold == 0 or fold == 1\")  # now I have fold0,1 only\n",
    "\n",
    "studies = train.StudyInstanceUID.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred(_path):\n",
    "    res = load_pickle(_path)\n",
    "    raw_pred = pd.DataFrame({\n",
    "        \"SOPInstanceUID\": res[\"ids\"],\n",
    "        **res[\"outputs\"],\n",
    "    })\n",
    "    return raw_pred\n",
    "\n",
    "def calib_p(arr, factor):  # set factor>1 to enhance positive prob\n",
    "    return arr * factor / (arr * factor + (1-arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check\n",
    "# oof_f0 = get_pred(\"output_yuji/b3_non_weight//oof_fold0_ep0.pkl\")\n",
    "# plt.hist( oof_f0.pe_present_on_image, bins=300 )\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! SET claibration value. Run src/oof_optpy to calculate the value \n",
    "oof_f0, fold0_calib_f = get_pred(\"output/final_image_level/oof_fold0.pkl\"), 3.8250639579850194\n",
    "oof_f1, fold1_calib_f = get_pred(\"output/final_image_level/oof_fold1.pkl\"), 8.555037588568537\n",
    "oof_f2, fold2_calib_f = get_pred(\"output/final_image_level/oof_fold2.pkl\"), 4.374239635034443\n",
    "oof_f3, fold3_calib_f = get_pred(\"output/final_image_level/oof_fold3.pkl\"), 7.480972390526775\n",
    "oof_f4, fold4_calib_f = get_pred(\"output/final_image_level/oof_fold4.pkl\"), 5.002262078458348\n",
    "\n",
    "# BAD\n",
    "if False:  # pick best one which yields weighted-logloss after calib\n",
    "    oof_f3, fold3_calib_f = get_pred(\"output/035_pe_present___448___apex/fold3_ep0.pt.valid.pickle\"), 6.541753595870311\n",
    "    oof_f4, fold4_calib_f = get_pred(\"output/035_pe_present___448___apex/fold4_ep0.pt.valid.pickle\"), 3.8250639579850194\n",
    "    \n",
    "if True: ### ==== do calib for each fold\n",
    "    oof_f0[\"pe_present_on_image\"] = calib_p(oof_f0[\"pe_present_on_image\"], fold0_calib_f)\n",
    "    oof_f1[\"pe_present_on_image\"] = calib_p(oof_f1[\"pe_present_on_image\"], fold1_calib_f)\n",
    "    oof_f2[\"pe_present_on_image\"] = calib_p(oof_f2[\"pe_present_on_image\"], fold2_calib_f)\n",
    "    oof_f3[\"pe_present_on_image\"] = calib_p(oof_f3[\"pe_present_on_image\"], fold3_calib_f)\n",
    "    oof_f4[\"pe_present_on_image\"] = calib_p(oof_f4[\"pe_present_on_image\"], fold4_calib_f)\n",
    "\n",
    "oof = pd.concat([oof_f0, oof_f1, oof_f2, oof_f3, oof_f4]).rename(columns={'pe_present_on_image': 'pred0'})\n",
    "\n",
    "train = train.merge(oof[['pred0', 'SOPInstanceUID']], on=\"SOPInstanceUID\")  # add pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Index(['StudyInstanceUID', 'SeriesInstanceUID', 'SOPInstanceUID',\n",
       "       'pe_present_on_image', 'negative_exam_for_pe', 'qa_motion',\n",
       "       'qa_contrast', 'flow_artifact', 'rv_lv_ratio_gte_1', 'rv_lv_ratio_lt_1',\n",
       "       'leftsided_pe', 'chronic_pe', 'true_filling_defect_not_pe',\n",
       "       'rightsided_pe', 'acute_and_chronic_pe', 'central_pe', 'indeterminate',\n",
       "       'exam_type', 'fold', 'pe_present_portion', 'z_pos', 'pred0'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "train_copyed = train.copy()\n",
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "' feature engineer '"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "\"\"\" feature engineer \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.sort_values(['StudyInstanceUID', 'z_pos'])\n",
    "train_current_z_pos = train.groupby('StudyInstanceUID')['z_pos'].shift(0)\n",
    "\n",
    "### for i in range(1, 20):\n",
    "for i in range(1, 10):\n",
    "\n",
    "    train[f'pred0_pre{i}'] = train.groupby('StudyInstanceUID')['pred0'].shift(i)\n",
    "    train[f'pred0_post{i}'] = train.groupby('StudyInstanceUID')['pred0'].shift(-i)\n",
    "\n",
    "# NORMALIZED Z POS\n",
    "z_max = train.groupby('StudyInstanceUID').z_pos.max().rename('z_pos_max')\n",
    "train = train.merge(z_max, on='StudyInstanceUID')\n",
    "train['z_pos_norm'] = train['z_pos'] / train['z_pos_max']\n",
    "train = train.drop('z_pos_max', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "        StudyInstanceUID SeriesInstanceUID SOPInstanceUID  \\\n",
       "1790589     fffda3f22362      39ca5eaafffe   29e855db7f2b   \n",
       "1790590     fffda3f22362      39ca5eaafffe   f7ca277a66c2   \n",
       "1790591     fffda3f22362      39ca5eaafffe   59714fd8dd25   \n",
       "1790592     fffda3f22362      39ca5eaafffe   b33567349fae   \n",
       "1790593     fffda3f22362      39ca5eaafffe   53d378d07811   \n",
       "\n",
       "         pe_present_on_image  negative_exam_for_pe  qa_motion  qa_contrast  \\\n",
       "1790589                    0                     1          0            0   \n",
       "1790590                    0                     1          0            0   \n",
       "1790591                    0                     1          0            0   \n",
       "1790592                    0                     1          0            0   \n",
       "1790593                    0                     1          0            0   \n",
       "\n",
       "         flow_artifact  rv_lv_ratio_gte_1  rv_lv_ratio_lt_1  ...  pred0_post5  \\\n",
       "1790589              0                  0                 0  ...          NaN   \n",
       "1790590              0                  0                 0  ...          NaN   \n",
       "1790591              0                  0                 0  ...          NaN   \n",
       "1790592              0                  0                 0  ...          NaN   \n",
       "1790593              0                  0                 0  ...          NaN   \n",
       "\n",
       "           pred0_pre6  pred0_post6    pred0_pre7  pred0_post7    pred0_pre8  \\\n",
       "1790589  1.633664e-06          NaN  1.558988e-06          NaN  3.526268e-07   \n",
       "1790590  4.497674e-07          NaN  1.633664e-06          NaN  1.558988e-06   \n",
       "1790591  6.327988e-07          NaN  4.497674e-07          NaN  1.633664e-06   \n",
       "1790592  4.913005e-06          NaN  6.327988e-07          NaN  4.497674e-07   \n",
       "1790593  2.411817e-06          NaN  4.913005e-06          NaN  6.327988e-07   \n",
       "\n",
       "         pred0_post8    pred0_pre9  pred0_post9  z_pos_norm  \n",
       "1790589          NaN  9.371637e-07          NaN    0.976190  \n",
       "1790590          NaN  3.526268e-07          NaN    0.982143  \n",
       "1790591          NaN  1.558988e-06          NaN    0.988095  \n",
       "1790592          NaN  1.633664e-06          NaN    0.994048  \n",
       "1790593          NaN  4.497674e-07          NaN    1.000000  \n",
       "\n",
       "[5 rows x 41 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>StudyInstanceUID</th>\n      <th>SeriesInstanceUID</th>\n      <th>SOPInstanceUID</th>\n      <th>pe_present_on_image</th>\n      <th>negative_exam_for_pe</th>\n      <th>qa_motion</th>\n      <th>qa_contrast</th>\n      <th>flow_artifact</th>\n      <th>rv_lv_ratio_gte_1</th>\n      <th>rv_lv_ratio_lt_1</th>\n      <th>...</th>\n      <th>pred0_post5</th>\n      <th>pred0_pre6</th>\n      <th>pred0_post6</th>\n      <th>pred0_pre7</th>\n      <th>pred0_post7</th>\n      <th>pred0_pre8</th>\n      <th>pred0_post8</th>\n      <th>pred0_pre9</th>\n      <th>pred0_post9</th>\n      <th>z_pos_norm</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1790589</th>\n      <td>fffda3f22362</td>\n      <td>39ca5eaafffe</td>\n      <td>29e855db7f2b</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>1.633664e-06</td>\n      <td>NaN</td>\n      <td>1.558988e-06</td>\n      <td>NaN</td>\n      <td>3.526268e-07</td>\n      <td>NaN</td>\n      <td>9.371637e-07</td>\n      <td>NaN</td>\n      <td>0.976190</td>\n    </tr>\n    <tr>\n      <th>1790590</th>\n      <td>fffda3f22362</td>\n      <td>39ca5eaafffe</td>\n      <td>f7ca277a66c2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>4.497674e-07</td>\n      <td>NaN</td>\n      <td>1.633664e-06</td>\n      <td>NaN</td>\n      <td>1.558988e-06</td>\n      <td>NaN</td>\n      <td>3.526268e-07</td>\n      <td>NaN</td>\n      <td>0.982143</td>\n    </tr>\n    <tr>\n      <th>1790591</th>\n      <td>fffda3f22362</td>\n      <td>39ca5eaafffe</td>\n      <td>59714fd8dd25</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>6.327988e-07</td>\n      <td>NaN</td>\n      <td>4.497674e-07</td>\n      <td>NaN</td>\n      <td>1.633664e-06</td>\n      <td>NaN</td>\n      <td>1.558988e-06</td>\n      <td>NaN</td>\n      <td>0.988095</td>\n    </tr>\n    <tr>\n      <th>1790592</th>\n      <td>fffda3f22362</td>\n      <td>39ca5eaafffe</td>\n      <td>b33567349fae</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>4.913005e-06</td>\n      <td>NaN</td>\n      <td>6.327988e-07</td>\n      <td>NaN</td>\n      <td>4.497674e-07</td>\n      <td>NaN</td>\n      <td>1.633664e-06</td>\n      <td>NaN</td>\n      <td>0.994048</td>\n    </tr>\n    <tr>\n      <th>1790593</th>\n      <td>fffda3f22362</td>\n      <td>39ca5eaafffe</td>\n      <td>53d378d07811</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>2.411817e-06</td>\n      <td>NaN</td>\n      <td>4.913005e-06</td>\n      <td>NaN</td>\n      <td>6.327988e-07</td>\n      <td>NaN</td>\n      <td>4.497674e-07</td>\n      <td>NaN</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 41 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['pred0', 'pred0_post1', 'pred0_post2', 'pred0_post3', 'pred0_post4', 'pred0_post5', 'pred0_post6', 'pred0_post7', 'pred0_post8', 'pred0_post9', 'pred0_pre1', 'pred0_pre2', 'pred0_pre3', 'pred0_pre4', 'pred0_pre5', 'pred0_pre6', 'pred0_pre7', 'pred0_pre8', 'pred0_pre9', 'z_pos_norm']\n"
     ]
    }
   ],
   "source": [
    "ids = [c for c in list(train) if 'UID' in c]\n",
    "targets = [\n",
    "    'negative_exam_for_pe',\n",
    "    'indeterminate',\n",
    "    'chronic_pe',\n",
    "    'acute_and_chronic_pe',\n",
    "    'central_pe',\n",
    "    'leftsided_pe',\n",
    "    'rightsided_pe',\n",
    "    'rv_lv_ratio_gte_1',\n",
    "    'rv_lv_ratio_lt_1',\n",
    "]\n",
    "other_targets = [c for c in list(train) if 'pe_present_on_image' in c]\n",
    "### remove_cols = ['fold', 'path', 'weight', 'qa_contrast', 'qa_motion'] + targets + ids + other_targets\n",
    "### remove_cols = ['fold', 'path', 'weight', 'qa_contrast', 'qa_motion'] + ['exam_type','flow_artifact','pe_present_portion', 'true_filling_defect_not_pe'] + targets + ids + other_targets\n",
    "remove_cols = ['fold', 'path', 'weight', 'qa_contrast', 'qa_motion'] + ['exam_type','flow_artifact','pe_present_portion', 'true_filling_defect_not_pe'] + targets + ids + other_targets + ['z_pos']\n",
    "\n",
    "features = sorted(list(set(list(train)) - set(remove_cols)))\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_copyed = features.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fobj(pred, data):\n",
    "    true = data.get_label()\n",
    "    label = 2*true - 1\n",
    "    weights = data.weights\n",
    "    response = -label / (1 + np.exp(label * pred))\n",
    "    abs_response = np.abs(response)\n",
    "    grad = response\n",
    "    hess = abs_response * (1 - abs_response)\n",
    "    return grad*weights, hess*weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "bce_func = torch.nn.BCELoss(reduction='none')\n",
    "\n",
    "def feval2(preds, data):\n",
    "    scores = bce_func(torch.FloatTensor(preds), torch.FloatTensor(data.label))\n",
    "    scores = scores * torch.FloatTensor(data.weights)\n",
    "    return 'weighted logloss', torch.mean(scores), False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "bce_func_logit = torch.nn.BCEWithLogitsLoss(reduction='none')\n",
    "\n",
    "def feval(preds, data):\n",
    "    scores = bce_func_logit(torch.FloatTensor(preds), torch.FloatTensor(data.label))\n",
    "    scores = scores * torch.FloatTensor(data.weights)\n",
    "    return 'weighted logloss', torch.mean(scores), False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features = ['pred']  # for test\n",
    "features = features_copyed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "==================0================\n",
      "    ==============fold0================\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[53]\ttrain's weighted logloss: 0.0110919\tval's weighted logloss: 0.0110957\n",
      "    ==============fold1================\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[76]\ttrain's weighted logloss: 0.0107546\tval's weighted logloss: 0.0116312\n",
      "    ==============fold2================\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[70]\ttrain's weighted logloss: 0.0108946\tval's weighted logloss: 0.01141\n",
      "    ==============fold3================\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[182]\ttrain's weighted logloss: 0.0103165\tval's weighted logloss: 0.0117116\n",
      "    ==============fold4================\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[60]\ttrain's weighted logloss: 0.01096\tval's weighted logloss: 0.0112557\n",
      "-------------------------------------------------------------------------roc_auc: 0.9623247655817194\n",
      "----------------------------------------------------------roc_auc using raw pred: 0.950916101960868\n",
      "------------------------------------------------------------------------------AP: 0.7864683523887135\n",
      "---------------------------------------------------------------AP using raw pred: 0.7370492969022756\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "import pickle\n",
    "\n",
    "oof_preds_list = []\n",
    "models_list = []\n",
    "target = 'pe_present_on_image'\n",
    "\n",
    "for i in range(1):\n",
    "    print(f'=================={i}================')\n",
    "    if i % 4 == 0:\n",
    "        params = {'boosting_type': 'gbdt',\n",
    "            'objective': 'binary',\n",
    "#             'metric': 'None',\n",
    "            'subsample': 0.75,\n",
    "            'subsample_freq': 1,\n",
    "            'learning_rate': 0.1,\n",
    "            'feature_fraction': 0.9,\n",
    "            'max_depth': 15,\n",
    "            'lambda_l1': 1,  \n",
    "            'lambda_l2': 1,\n",
    "            'verbose': 100,\n",
    "            'early_stopping_rounds': 100,\n",
    "            'verbose': -1,\n",
    "            } \n",
    "    elif i % 4 == 1:\n",
    "        params = {\n",
    "            'max_depth': 4,\n",
    "            'max_leave': int(0.2 * 2 ** 4),\n",
    "            'reg_lambda': 1,\n",
    "            'reg_alpha': 1,\n",
    "            'subsamples': 0.8,\n",
    "            'colsample_bytree': 0.7,\n",
    "            'objective': 'binary',\n",
    "            'min_data_in_leaf': 0,\n",
    "            'boosting': 'gbdt',\n",
    "            'metric': 'None',\n",
    "            'learning_rate': 0.1,\n",
    "                      }\n",
    "    elif i % 4 == 2:\n",
    "        params = {\n",
    "            'num_leaves': 19, \n",
    "            'min_data_in_leaf': 160,\n",
    "            'min_child_weight': 0.03,\n",
    "            'bagging_fraction' : 0.7,\n",
    "            'feature_fraction' : 0.8,\n",
    "            'learning_rate' : 0.1,\n",
    "            'max_depth': -1,\n",
    "            'reg_alpha': 0.02,\n",
    "            'reg_lambda': 0.12,\n",
    "            'objective': 'binary',\n",
    "            'verbose': 100,\n",
    "            'boost_from_average': False,\n",
    "            'metric': 'None',\n",
    "        }  \n",
    "    else:\n",
    "        params = {\n",
    "            'objective': \"binary\",\n",
    "            'metric': 'None',\n",
    "            'boost_from_average': \"false\",\n",
    "            'tree_learner': \"serial\",\n",
    "            'max_depth': -1,\n",
    "            'learning_rate': 0.1,\n",
    "            'num_leaves': 197,\n",
    "            'feature_fraction': 0.3,\n",
    "            'bagging_freq': 1,\n",
    "            'bagging_fraction': 0.7,\n",
    "            'min_data_in_leaf': 100,\n",
    "            'bagging_seed': 11,\n",
    "            'max_bin': 255,\n",
    "            'verbosity': -1}    \n",
    "        \n",
    "    oof_preds = np.zeros(train.shape[0])\n",
    "    val_results = {}\n",
    "    models = []\n",
    "    params['random_state'] = i\n",
    "    iter = 100000\n",
    "#     for n_fold, (trn_idx, val_idx) in enumerate(kf.split(train, train[target])):\n",
    "\n",
    "    for n_fold in range(5):\n",
    "    ### for n_fold in range(2):\n",
    "        print(f'    ==============fold{n_fold}================')\n",
    "        tr = train.query(f'fold != {n_fold}')\n",
    "        val = train.query(f'fold == {n_fold}')\n",
    "        trn_data = lgb.Dataset(tr[features], label=tr[target])\n",
    "        trn_data.weights = tr.pe_present_portion.values\n",
    "        val_data = lgb.Dataset(val[features], label=val[target])\n",
    "        val_data.weights = val.pe_present_portion.values\n",
    "        \n",
    "        clf = lgb.train(params, trn_data, num_boost_round=iter, valid_sets=[trn_data, val_data], valid_names=['train', 'val'],\n",
    "#                         verbose_eval=200, early_stopping_rounds = 10/params['learning_rate'], evals_result=val_results,)\n",
    "                        feval=feval, fobj = fobj, verbose_eval=2000, early_stopping_rounds = 10/params['learning_rate'], evals_result=val_results, )\n",
    "        file = f'lgbs/lgb_seed{i}_fold{n_fold}.pkl'\n",
    "        pickle.dump(clf, open(file, 'wb'))\n",
    "        oof_preds[train.fold==n_fold] = clf.predict(val[features])\n",
    "    oof_preds_list.append(oof_preds)\n",
    "\n",
    "print(f'-------------------------------------------------------------------------roc_auc: {roc_auc_score(train[target], np.mean(oof_preds_list, axis=0))}')\n",
    "print(f'----------------------------------------------------------roc_auc using raw pred: {roc_auc_score(train[target], train[\"pred0\"])}')\n",
    "print(f'------------------------------------------------------------------------------AP: {average_precision_score(train[target], np.mean(oof_preds_list, axis=0))}')\n",
    "print(f'---------------------------------------------------------------AP using raw pred: {average_precision_score(train[target], train[\"pred0\"])}')\n",
    "\n",
    "lgb_oof = np.mean(oof_preds_list, axis=0)\n",
    "train['lgb_preds'] = sigmoid(lgb_oof)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.011420365194671893"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "bce_func = torch.nn.BCELoss(reduction='none')\n",
    "\n",
    "lgb_losses = bce_func(torch.FloatTensor(sigmoid(lgb_oof)), torch.FloatTensor(train['pe_present_on_image']))\n",
    "\n",
    "### torch.mean(lgb_losses*train['weight'].values)\n",
    "torch.mean(lgb_losses*train['pe_present_portion'].values).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.01322563879510154"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "# no stacking result\n",
    "lgb_losses = bce_func(torch.FloatTensor(train['pred0']), torch.FloatTensor(train['pe_present_on_image']))\n",
    "torch.mean(lgb_losses*train['pe_present_portion'].values).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise \"BELOW IS NOT USED FOR FINAL SUB\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <<< BELOW IS NOT USED FOR FINAL SUB >>> stacking for PE_REPESNT -> POS_EXAM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = Path(\"../input/rsna-str-pulmonary-embolism-detection/\")\n",
    "\n",
    "train = pd.read_csv(DATADIR / \"train.csv\")\n",
    "\n",
    "pre = pd.read_csv(DATADIR / \"split.csv\")\n",
    "train = train.merge(pre, on=\"StudyInstanceUID\")\n",
    "\n",
    "portion = pd.read_csv(DATADIR / \"study_pos_portion.csv\")\n",
    "train = train.merge(portion, on=\"StudyInstanceUID\")\n",
    "\n",
    "z_pos_df = pd.read_csv(DATADIR / \"sop_to_prefix.csv\").rename(columns={'img_prefix': 'z_pos'})\n",
    "train = train.merge(z_pos_df, on=\"SOPInstanceUID\")\n",
    "\n",
    "studies = train.StudyInstanceUID.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof = pd.concat([oof_f0, oof_f1, oof_f2, oof_f3, oof_f4]).rename(columns={'pe_present_on_image': 'pred'})\n",
    "\n",
    "train = train.merge(oof[['pred', 'SOPInstanceUID']], on=\"SOPInstanceUID\")  # add pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "def grouping(df):\n",
    "    grouped = pd.DataFrame(df.groupby('StudyInstanceUID')['pred'].mean())\n",
    "    grouped = grouped.rename(columns={'pred': 'mean'})\n",
    "    count = df.groupby('StudyInstanceUID')['pred'].count()\n",
    "    grouped['count_total'] = count\n",
    "\n",
    "    for i in range(1,10):\n",
    "        count = df[df.pred>i/10].groupby('StudyInstanceUID')['pred'].count()\n",
    "        grouped[f'count_over{i/10}'] = count\n",
    "        grouped[f'count_over{i/10}_ratio'] = count / grouped['count_total']\n",
    "\n",
    "    for q in [30, 50, 70, 80, 90, 95, 99]:\n",
    "    # for q in [95]:\n",
    "        grouped[f'percentile{q}'] = df.groupby('StudyInstanceUID')['pred'].apply(lambda arr: np.percentile(arr, q))\n",
    "\n",
    "    ma = pd.DataFrame(df.groupby('StudyInstanceUID')['pred'].max())\n",
    "    grouped['max'] = ma.pred\n",
    "\n",
    "    grouped = grouped.reset_index().fillna(0)\n",
    "    return grouped\n",
    "train_grouped = grouping(train)\n",
    "\n",
    "train_grouped['fold'] = train.groupby('StudyInstanceUID')['fold'].first().values\n",
    "train_grouped['negative_exam_for_pe'] = train.groupby('StudyInstanceUID')['negative_exam_for_pe'].first().values\n",
    "train_grouped['positive_exam_for_pe'] = (1 - train.groupby('StudyInstanceUID')['negative_exam_for_pe'].first().values) * (1 - train.groupby('StudyInstanceUID')['indeterminate'].first().values)\n",
    "\n",
    "# test_grouped = grouping(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features = list(set(list(train_grouped)) - set(['StudyInstanceUID', 'negative_exam_for_pe']))\n",
    "# target = 'negative_exam_for_pe'\n",
    "\n",
    "### features = list(set(list(train_grouped)) - set(['StudyInstanceUID', 'positive_exam_for_pe', 'negative_exam_for_pe', 'fold']))\n",
    "features = list(set(list(train_grouped)) - set(['StudyInstanceUID', 'positive_exam_for_pe', 'negative_exam_for_pe', 'fold']) - set(['count_total']))\n",
    "features = sorted(features)\n",
    "target = 'positive_exam_for_pe'\n",
    "# target = 'negative_exam_for_pe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['count_over0.1', 'count_over0.1_ratio', 'count_over0.2', 'count_over0.2_ratio', 'count_over0.3', 'count_over0.3_ratio', 'count_over0.4', 'count_over0.4_ratio', 'count_over0.5', 'count_over0.5_ratio', 'count_over0.6', 'count_over0.6_ratio', 'count_over0.7', 'count_over0.7_ratio', 'count_over0.8', 'count_over0.8_ratio', 'count_over0.9', 'count_over0.9_ratio', 'max', 'mean', 'percentile30', 'percentile50', 'percentile70', 'percentile80', 'percentile90', 'percentile95', 'percentile99']\n"
     ]
    }
   ],
   "source": [
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  StudyInstanceUID      mean  count_total  count_over0.1  count_over0.1_ratio  \\\n",
       "0     0003b3d648eb  0.271391          223          121.0             0.542601   \n",
       "1     000f7f114264  0.091386          239           65.0             0.271967   \n",
       "2     00102474a2db  0.055245          326           58.0             0.177914   \n",
       "3     0038fd5f09f5  0.054540          230           34.0             0.147826   \n",
       "4     0045f113e031  0.133363          257          111.0             0.431907   \n",
       "\n",
       "   count_over0.2  count_over0.2_ratio  count_over0.3  count_over0.3_ratio  \\\n",
       "0           94.0             0.421525           80.0             0.358744   \n",
       "1           45.0             0.188285           28.0             0.117155   \n",
       "2           21.0             0.064417           10.0             0.030675   \n",
       "3           13.0             0.056522            6.0             0.026087   \n",
       "4           75.0             0.291829           45.0             0.175097   \n",
       "\n",
       "   count_over0.4  ...  percentile50  percentile70  percentile80  percentile90  \\\n",
       "0           61.0  ...      0.125400      0.373026      0.589021      0.825415   \n",
       "1           16.0  ...      0.006931      0.075649      0.177131      0.311252   \n",
       "2            4.0  ...      0.015041      0.047743      0.089943      0.159699   \n",
       "3            6.0  ...      0.014543      0.056811      0.073162      0.130984   \n",
       "4           22.0  ...      0.034198      0.186576      0.260993      0.380261   \n",
       "\n",
       "   percentile95  percentile99       max  fold  negative_exam_for_pe  \\\n",
       "0      0.907900      0.963111  0.971206     0                     1   \n",
       "1      0.457845      0.573354  0.889981     3                     1   \n",
       "2      0.251680      0.518155  0.652430     3                     1   \n",
       "3      0.203891      0.503386  0.944670     4                     1   \n",
       "4      0.478244      0.765378  0.901021     4                     1   \n",
       "\n",
       "   positive_exam_for_pe  \n",
       "0                     0  \n",
       "1                     0  \n",
       "2                     0  \n",
       "3                     0  \n",
       "4                     0  \n",
       "\n",
       "[5 rows x 32 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>StudyInstanceUID</th>\n      <th>mean</th>\n      <th>count_total</th>\n      <th>count_over0.1</th>\n      <th>count_over0.1_ratio</th>\n      <th>count_over0.2</th>\n      <th>count_over0.2_ratio</th>\n      <th>count_over0.3</th>\n      <th>count_over0.3_ratio</th>\n      <th>count_over0.4</th>\n      <th>...</th>\n      <th>percentile50</th>\n      <th>percentile70</th>\n      <th>percentile80</th>\n      <th>percentile90</th>\n      <th>percentile95</th>\n      <th>percentile99</th>\n      <th>max</th>\n      <th>fold</th>\n      <th>negative_exam_for_pe</th>\n      <th>positive_exam_for_pe</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0003b3d648eb</td>\n      <td>0.271391</td>\n      <td>223</td>\n      <td>121.0</td>\n      <td>0.542601</td>\n      <td>94.0</td>\n      <td>0.421525</td>\n      <td>80.0</td>\n      <td>0.358744</td>\n      <td>61.0</td>\n      <td>...</td>\n      <td>0.125400</td>\n      <td>0.373026</td>\n      <td>0.589021</td>\n      <td>0.825415</td>\n      <td>0.907900</td>\n      <td>0.963111</td>\n      <td>0.971206</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000f7f114264</td>\n      <td>0.091386</td>\n      <td>239</td>\n      <td>65.0</td>\n      <td>0.271967</td>\n      <td>45.0</td>\n      <td>0.188285</td>\n      <td>28.0</td>\n      <td>0.117155</td>\n      <td>16.0</td>\n      <td>...</td>\n      <td>0.006931</td>\n      <td>0.075649</td>\n      <td>0.177131</td>\n      <td>0.311252</td>\n      <td>0.457845</td>\n      <td>0.573354</td>\n      <td>0.889981</td>\n      <td>3</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00102474a2db</td>\n      <td>0.055245</td>\n      <td>326</td>\n      <td>58.0</td>\n      <td>0.177914</td>\n      <td>21.0</td>\n      <td>0.064417</td>\n      <td>10.0</td>\n      <td>0.030675</td>\n      <td>4.0</td>\n      <td>...</td>\n      <td>0.015041</td>\n      <td>0.047743</td>\n      <td>0.089943</td>\n      <td>0.159699</td>\n      <td>0.251680</td>\n      <td>0.518155</td>\n      <td>0.652430</td>\n      <td>3</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0038fd5f09f5</td>\n      <td>0.054540</td>\n      <td>230</td>\n      <td>34.0</td>\n      <td>0.147826</td>\n      <td>13.0</td>\n      <td>0.056522</td>\n      <td>6.0</td>\n      <td>0.026087</td>\n      <td>6.0</td>\n      <td>...</td>\n      <td>0.014543</td>\n      <td>0.056811</td>\n      <td>0.073162</td>\n      <td>0.130984</td>\n      <td>0.203891</td>\n      <td>0.503386</td>\n      <td>0.944670</td>\n      <td>4</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0045f113e031</td>\n      <td>0.133363</td>\n      <td>257</td>\n      <td>111.0</td>\n      <td>0.431907</td>\n      <td>75.0</td>\n      <td>0.291829</td>\n      <td>45.0</td>\n      <td>0.175097</td>\n      <td>22.0</td>\n      <td>...</td>\n      <td>0.034198</td>\n      <td>0.186576</td>\n      <td>0.260993</td>\n      <td>0.380261</td>\n      <td>0.478244</td>\n      <td>0.765378</td>\n      <td>0.901021</td>\n      <td>4</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 32 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "train_grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "==================0================\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[26]\ttrain's binary_logloss: 0.272409\tval's binary_logloss: 0.343537\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[43]\ttrain's binary_logloss: 0.242331\tval's binary_logloss: 0.30137\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[34]\ttrain's binary_logloss: 0.253284\tval's binary_logloss: 0.326764\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[27]\ttrain's binary_logloss: 0.262734\tval's binary_logloss: 0.354237\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[34]\ttrain's binary_logloss: 0.253022\tval's binary_logloss: 0.323352\n",
      "--------------------------------------------------------------------roc: 0.8960885462905748\n",
      "----------------------------------------------------------------logloss: 0.32985277258732953\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "import pickle\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "oof_preds_list = []\n",
    "test_preds_list = []\n",
    "models_list = []\n",
    "\n",
    "for i in range(1):\n",
    "    print(f'=================={i}================')\n",
    "    if i % 4 == 0:\n",
    "        params = {'boosting_type': 'gbdt',\n",
    "            'objective': 'binary',\n",
    "#             'metric': 'None',\n",
    "            'subsample': 0.75,\n",
    "            'subsample_freq': 1,\n",
    "            'learning_rate': 0.1,\n",
    "            'feature_fraction': 0.9,\n",
    "            'max_depth': 15,\n",
    "            'lambda_l1': 1,  \n",
    "            'lambda_l2': 1,\n",
    "            'verbose': 100,\n",
    "            'early_stopping_rounds': 100,\n",
    "            'verbose': -1,\n",
    "            } \n",
    "    elif i % 4 == 1:\n",
    "        params = {\n",
    "            'max_depth': 4,\n",
    "            'max_leave': int(0.2 * 2 ** 4),\n",
    "            'reg_lambda': 1,\n",
    "            'reg_alpha': 1,\n",
    "            'subsamples': 0.8,\n",
    "            'colsample_bytree': 0.7,\n",
    "            'objective': 'binary',\n",
    "            'min_data_in_leaf': 0,\n",
    "            'boosting': 'gbdt',\n",
    "#             'metric': 'None',\n",
    "            'learning_rate': 0.1,\n",
    "                      }\n",
    "    elif i % 4 == 2:\n",
    "        params = {\n",
    "            'num_leaves': 19, \n",
    "            'min_data_in_leaf': 160,\n",
    "            'min_child_weight': 0.03,\n",
    "            'bagging_fraction' : 0.7,\n",
    "            'feature_fraction' : 0.8,\n",
    "            'learning_rate' : 0.1,\n",
    "            'max_depth': -1,\n",
    "            'reg_alpha': 0.02,\n",
    "            'reg_lambda': 0.12,\n",
    "            'objective': 'binary',\n",
    "            'verbose': 100,\n",
    "            'boost_from_average': False,\n",
    "#             'metric': 'None',\n",
    "        }  \n",
    "    else:\n",
    "        params = {\n",
    "            'objective': \"binary\",\n",
    "#             'metric': 'None',\n",
    "            'boost_from_average': \"false\",\n",
    "            'tree_learner': \"serial\",\n",
    "            'max_depth': -1,\n",
    "            'learning_rate': 0.1,\n",
    "            'num_leaves': 197,\n",
    "            'feature_fraction': 0.3,\n",
    "            'bagging_freq': 1,\n",
    "            'bagging_fraction': 0.7,\n",
    "            'min_data_in_leaf': 100,\n",
    "            'bagging_seed': 11,\n",
    "            'max_bin': 255,\n",
    "            'verbosity': -1}    \n",
    "        \n",
    "    oof_preds = np.zeros(train_grouped.shape[0])\n",
    "\n",
    "    ### test_preds = np.zeros(test_grouped.shape[0])\n",
    "    val_results = {}\n",
    "    models = []\n",
    "    params['random_state'] = i\n",
    "    iter = 100000\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=72)\n",
    "\n",
    "    #for n_fold, (trn_idx, val_idx) in enumerate(kf.split(train_grouped, train_grouped[target])):\n",
    "        # tr = train_grouped.iloc[trn_idx]\n",
    "        # val = train_grouped.iloc[val_idx]\n",
    "    for n_fold in range( 5 ):\n",
    "        tr = train_grouped[train_grouped.fold != n_fold]\n",
    "        val = train_grouped[train_grouped.fold == n_fold]\n",
    "        trn_data = lgb.Dataset(tr[features], label=tr[target])\n",
    "        val_data = lgb.Dataset(val[features], label=val[target])\n",
    "        \n",
    "        clf = lgb.train(params, trn_data, num_boost_round=iter, valid_sets=[trn_data, val_data], valid_names=['train', 'val'],\n",
    "#                         feval=feval, verbose_eval=10, early_stopping_rounds = 10/params['learning_rate'], evals_result=val_results,)\n",
    "                        verbose_eval=200, early_stopping_rounds = 10/params['learning_rate'], evals_result=val_results,)\n",
    "        file = f'lgbs/posexam_lgb_seed{i}_fold{n_fold}.pkl'\n",
    "        pickle.dump(clf, open(file, 'wb'))\n",
    "        models.append(clf)\n",
    "        \n",
    "        oof_preds[train_grouped.fold == n_fold] = clf.predict(val[features])\n",
    "        ### oof_preds[val_idx] = clf.predict(val[features])\n",
    "        ### test_preds += clf.predict(test_grouped[features]) / 5\n",
    "#     models_list.append(models)\n",
    "    oof_preds_list.append(oof_preds)\n",
    "    ###test_preds_list.append(test_preds)\n",
    "\n",
    "print(f'--------------------------------------------------------------------roc: {roc_auc_score(train_grouped[target], np.mean(oof_preds_list, axis=0))}')\n",
    "print(f'----------------------------------------------------------------logloss: {log_loss(train_grouped[target], np.mean(oof_preds_list, axis=0))}')\n",
    "lgb_oof_exam = np.mean(oof_preds_list, axis=0)\n",
    "### lgb_preds = np.mean(test_preds_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.3298527002334595"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "bce_func = torch.nn.BCELoss(reduction='mean')\n",
    "lgb_losses = bce_func(torch.FloatTensor(oof_preds), torch.FloatTensor(train_grouped['positive_exam_for_pe']))\n",
    "\n",
    "torch.mean(lgb_losses).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.36254268884658813"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "bce_func = torch.nn.BCELoss(reduction='mean')\n",
    "lgb_losses = bce_func(\n",
    "    ( 1 - torch.FloatTensor(oof_preds) ) * (4911) / (4911 + 157), \n",
    "    torch.FloatTensor(train_grouped['negative_exam_for_pe']))\n",
    "\n",
    "torch.mean(lgb_losses).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # current yama's pipeline for fold0-ep1\n",
    "# def calib_p(arr, factor):  # set factor>1 to enhance positive prob\n",
    "#     return arr * factor / (arr * factor + (1-arr))\n",
    "# def post_yama(arr):\n",
    "#     return calib_p( np.percentile(arr, 95), factor=1/8.5550)\n",
    "\n",
    "# lgb_losses = bce_func(torch.FloatTensor(train[['StudyInstanceUID','pred']].groupby('StudyInstanceUID').apply(post_yama)), torch.FloatTensor(train_grouped['positive_exam_for_pe'])).item()\n",
    "# lgb_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}