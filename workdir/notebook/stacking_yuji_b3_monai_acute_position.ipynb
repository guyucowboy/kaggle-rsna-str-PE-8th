{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.5 64-bit",
   "display_name": "Python 3.8.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/home/haito/kaggle/rsna-str-2/workdir\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "========== load_dicom_array() NOT USE TRY-EXCEPT ======================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "sys.path.append(\".\")\n",
    "from src.factory import *\n",
    "from src.utils import *\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = Path(\"../input/rsna-str-pulmonary-embolism-detection/\")\n",
    "\n",
    "train = pd.read_csv(DATADIR / \"train.csv\")\n",
    "\n",
    "pre = pd.read_csv(DATADIR / \"split.csv\")\n",
    "train = train.merge(pre, on=\"StudyInstanceUID\")\n",
    "\n",
    "portion = pd.read_csv(DATADIR / \"study_pos_portion.csv\")\n",
    "train = train.merge(portion, on=\"StudyInstanceUID\")\n",
    "\n",
    "z_pos_df = pd.read_csv(DATADIR / \"sop_to_prefix.csv\").rename(columns={'img_prefix': 'z_pos'})\n",
    "train = train.merge(z_pos_df, on=\"SOPInstanceUID\")\n",
    "\n",
    "\n",
    "### train = train.query(\"fold == 0 or fold == 1\")  # now I have fold0,1 only\n",
    "\n",
    "studies = train.StudyInstanceUID.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred(_path):\n",
    "    res = load_pickle(_path)\n",
    "    raw_pred = pd.DataFrame({\n",
    "        \"SOPInstanceUID\": res[\"ids\"],\n",
    "        **res[\"outputs\"],\n",
    "    })\n",
    "    return raw_pred\n",
    "\n",
    "def calib_p(arr, factor):  # set factor>1 to enhance positive prob\n",
    "    return arr * factor / (arr * factor + (1-arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check\n",
    "# oof_f0 = get_pred(\"output_yuji/b3_non_weight//oof_fold0_ep0.pkl\")\n",
    "# plt.hist( oof_f0.pe_present_on_image, bins=300 )\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_f0, fold0_calib_f = get_pred(\"output/final_image_level/oof_fold0.pkl\"), 3.8250639579850194\n",
    "oof_f1, fold1_calib_f = get_pred(\"output/final_image_level/oof_fold1.pkl\"), 8.555037588568537\n",
    "oof_f2, fold2_calib_f = get_pred(\"output/final_image_level/oof_fold2.pkl\"), 4.374239635034443\n",
    "oof_f3, fold3_calib_f = get_pred(\"output/final_image_level/oof_fold3.pkl\"), 7.480972390526775\n",
    "oof_f4, fold4_calib_f = get_pred(\"output/final_image_level/oof_fold4.pkl\"), 5.002262078458348\n",
    "\n",
    "# BAD\n",
    "if False:  # pick best one which yields weighted-logloss after calib\n",
    "    oof_f3, fold3_calib_f = get_pred(\"output/035_pe_present___448___apex/fold3_ep0.pt.valid.pickle\"), 6.541753595870311\n",
    "    oof_f4, fold4_calib_f = get_pred(\"output/035_pe_present___448___apex/fold4_ep0.pt.valid.pickle\"), 3.8250639579850194\n",
    "\n",
    "if True: ### ==== do calib for each fold\n",
    "    oof_f0[\"pe_present_on_image\"] = calib_p(oof_f0[\"pe_present_on_image\"], fold0_calib_f)\n",
    "    oof_f1[\"pe_present_on_image\"] = calib_p(oof_f1[\"pe_present_on_image\"], fold1_calib_f)\n",
    "    oof_f2[\"pe_present_on_image\"] = calib_p(oof_f2[\"pe_present_on_image\"], fold2_calib_f)\n",
    "    oof_f3[\"pe_present_on_image\"] = calib_p(oof_f3[\"pe_present_on_image\"], fold3_calib_f)\n",
    "    oof_f4[\"pe_present_on_image\"] = calib_p(oof_f4[\"pe_present_on_image\"], fold4_calib_f)\n",
    "\n",
    "oof = pd.concat([oof_f0, oof_f1, oof_f2, oof_f3, oof_f4]).rename(columns={'pe_present_on_image': 'pred0'})\n",
    "\n",
    "train = train.merge(oof[['pred0', 'SOPInstanceUID']], on=\"SOPInstanceUID\")  # add pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  StudyInstanceUID SeriesInstanceUID SOPInstanceUID  pe_present_on_image  \\\n",
       "0     6897fa9de148      2bfbb7fd2e8b   c0f3cb036d06                    0   \n",
       "1     6897fa9de148      2bfbb7fd2e8b   f57ffd3883b6                    0   \n",
       "2     6897fa9de148      2bfbb7fd2e8b   41220fda34a3                    0   \n",
       "3     6897fa9de148      2bfbb7fd2e8b   13b685b4b14f                    0   \n",
       "4     6897fa9de148      2bfbb7fd2e8b   be0b7524ffb4                    0   \n",
       "\n",
       "   negative_exam_for_pe  qa_motion  qa_contrast  flow_artifact  \\\n",
       "0                     0          0            0              0   \n",
       "1                     0          0            0              0   \n",
       "2                     0          0            0              0   \n",
       "3                     0          0            0              0   \n",
       "4                     0          0            0              0   \n",
       "\n",
       "   rv_lv_ratio_gte_1  rv_lv_ratio_lt_1  ...  true_filling_defect_not_pe  \\\n",
       "0                  0                 1  ...                           0   \n",
       "1                  0                 1  ...                           0   \n",
       "2                  0                 1  ...                           0   \n",
       "3                  0                 1  ...                           0   \n",
       "4                  0                 1  ...                           0   \n",
       "\n",
       "   rightsided_pe  acute_and_chronic_pe  central_pe  indeterminate  exam_type  \\\n",
       "0              1                     0           0              0   positive   \n",
       "1              1                     0           0              0   positive   \n",
       "2              1                     0           0              0   positive   \n",
       "3              1                     0           0              0   positive   \n",
       "4              1                     0           0              0   positive   \n",
       "\n",
       "   fold pe_present_portion  z_pos     pred0  \n",
       "0     1            0.33871    123  0.002160  \n",
       "1     1            0.33871    114  0.000151  \n",
       "2     1            0.33871     24  0.218694  \n",
       "3     1            0.33871     23  0.272379  \n",
       "4     1            0.33871     22  0.065160  \n",
       "\n",
       "[5 rows x 22 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>StudyInstanceUID</th>\n      <th>SeriesInstanceUID</th>\n      <th>SOPInstanceUID</th>\n      <th>pe_present_on_image</th>\n      <th>negative_exam_for_pe</th>\n      <th>qa_motion</th>\n      <th>qa_contrast</th>\n      <th>flow_artifact</th>\n      <th>rv_lv_ratio_gte_1</th>\n      <th>rv_lv_ratio_lt_1</th>\n      <th>...</th>\n      <th>true_filling_defect_not_pe</th>\n      <th>rightsided_pe</th>\n      <th>acute_and_chronic_pe</th>\n      <th>central_pe</th>\n      <th>indeterminate</th>\n      <th>exam_type</th>\n      <th>fold</th>\n      <th>pe_present_portion</th>\n      <th>z_pos</th>\n      <th>pred0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>6897fa9de148</td>\n      <td>2bfbb7fd2e8b</td>\n      <td>c0f3cb036d06</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>positive</td>\n      <td>1</td>\n      <td>0.33871</td>\n      <td>123</td>\n      <td>0.002160</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>6897fa9de148</td>\n      <td>2bfbb7fd2e8b</td>\n      <td>f57ffd3883b6</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>positive</td>\n      <td>1</td>\n      <td>0.33871</td>\n      <td>114</td>\n      <td>0.000151</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>6897fa9de148</td>\n      <td>2bfbb7fd2e8b</td>\n      <td>41220fda34a3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>positive</td>\n      <td>1</td>\n      <td>0.33871</td>\n      <td>24</td>\n      <td>0.218694</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6897fa9de148</td>\n      <td>2bfbb7fd2e8b</td>\n      <td>13b685b4b14f</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>positive</td>\n      <td>1</td>\n      <td>0.33871</td>\n      <td>23</td>\n      <td>0.272379</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>6897fa9de148</td>\n      <td>2bfbb7fd2e8b</td>\n      <td>be0b7524ffb4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>positive</td>\n      <td>1</td>\n      <td>0.33871</td>\n      <td>22</td>\n      <td>0.065160</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 22 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Index(['StudyInstanceUID', 'SeriesInstanceUID', 'SOPInstanceUID',\n",
       "       'pe_present_on_image', 'negative_exam_for_pe', 'qa_motion',\n",
       "       'qa_contrast', 'flow_artifact', 'rv_lv_ratio_gte_1', 'rv_lv_ratio_lt_1',\n",
       "       'leftsided_pe', 'chronic_pe', 'true_filling_defect_not_pe',\n",
       "       'rightsided_pe', 'acute_and_chronic_pe', 'central_pe', 'indeterminate',\n",
       "       'exam_type', 'fold', 'pe_present_portion', 'z_pos', 'pred0'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "train_copyed = train.copy()\n",
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "' feature engineer '"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "\"\"\" feature engineer \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.sort_values(['StudyInstanceUID', 'z_pos'])\n",
    "train_current_z_pos = train.groupby('StudyInstanceUID')['z_pos'].shift(0)\n",
    "\n",
    "### for i in range(1, 20):\n",
    "for i in range(1, 10):\n",
    "    train[f'pred0_pre{i}'] = train.groupby('StudyInstanceUID')['pred0'].shift(i)\n",
    "    train[f'pred0_post{i}'] = train.groupby('StudyInstanceUID')['pred0'].shift(-i)\n",
    "\n",
    "# NORMALIZED Z POS\n",
    "z_max = train.groupby('StudyInstanceUID').z_pos.max().rename('z_pos_max')\n",
    "train = train.merge(z_max, on='StudyInstanceUID')\n",
    "train['z_pos_norm'] = train['z_pos'] / train['z_pos_max']\n",
    "train = train.drop('z_pos_max', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "        StudyInstanceUID SeriesInstanceUID SOPInstanceUID  \\\n",
       "1790589     fffda3f22362      39ca5eaafffe   29e855db7f2b   \n",
       "1790590     fffda3f22362      39ca5eaafffe   f7ca277a66c2   \n",
       "1790591     fffda3f22362      39ca5eaafffe   59714fd8dd25   \n",
       "1790592     fffda3f22362      39ca5eaafffe   b33567349fae   \n",
       "1790593     fffda3f22362      39ca5eaafffe   53d378d07811   \n",
       "\n",
       "         pe_present_on_image  negative_exam_for_pe  qa_motion  qa_contrast  \\\n",
       "1790589                    0                     1          0            0   \n",
       "1790590                    0                     1          0            0   \n",
       "1790591                    0                     1          0            0   \n",
       "1790592                    0                     1          0            0   \n",
       "1790593                    0                     1          0            0   \n",
       "\n",
       "         flow_artifact  rv_lv_ratio_gte_1  rv_lv_ratio_lt_1  ...  pred0_post5  \\\n",
       "1790589              0                  0                 0  ...          NaN   \n",
       "1790590              0                  0                 0  ...          NaN   \n",
       "1790591              0                  0                 0  ...          NaN   \n",
       "1790592              0                  0                 0  ...          NaN   \n",
       "1790593              0                  0                 0  ...          NaN   \n",
       "\n",
       "           pred0_pre6  pred0_post6    pred0_pre7  pred0_post7    pred0_pre8  \\\n",
       "1790589  1.633664e-06          NaN  1.558988e-06          NaN  3.526268e-07   \n",
       "1790590  4.497674e-07          NaN  1.633664e-06          NaN  1.558988e-06   \n",
       "1790591  6.327988e-07          NaN  4.497674e-07          NaN  1.633664e-06   \n",
       "1790592  4.913005e-06          NaN  6.327988e-07          NaN  4.497674e-07   \n",
       "1790593  2.411817e-06          NaN  4.913005e-06          NaN  6.327988e-07   \n",
       "\n",
       "         pred0_post8    pred0_pre9  pred0_post9  z_pos_norm  \n",
       "1790589          NaN  9.371637e-07          NaN    0.976190  \n",
       "1790590          NaN  3.526268e-07          NaN    0.982143  \n",
       "1790591          NaN  1.558988e-06          NaN    0.988095  \n",
       "1790592          NaN  1.633664e-06          NaN    0.994048  \n",
       "1790593          NaN  4.497674e-07          NaN    1.000000  \n",
       "\n",
       "[5 rows x 41 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>StudyInstanceUID</th>\n      <th>SeriesInstanceUID</th>\n      <th>SOPInstanceUID</th>\n      <th>pe_present_on_image</th>\n      <th>negative_exam_for_pe</th>\n      <th>qa_motion</th>\n      <th>qa_contrast</th>\n      <th>flow_artifact</th>\n      <th>rv_lv_ratio_gte_1</th>\n      <th>rv_lv_ratio_lt_1</th>\n      <th>...</th>\n      <th>pred0_post5</th>\n      <th>pred0_pre6</th>\n      <th>pred0_post6</th>\n      <th>pred0_pre7</th>\n      <th>pred0_post7</th>\n      <th>pred0_pre8</th>\n      <th>pred0_post8</th>\n      <th>pred0_pre9</th>\n      <th>pred0_post9</th>\n      <th>z_pos_norm</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1790589</th>\n      <td>fffda3f22362</td>\n      <td>39ca5eaafffe</td>\n      <td>29e855db7f2b</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>1.633664e-06</td>\n      <td>NaN</td>\n      <td>1.558988e-06</td>\n      <td>NaN</td>\n      <td>3.526268e-07</td>\n      <td>NaN</td>\n      <td>9.371637e-07</td>\n      <td>NaN</td>\n      <td>0.976190</td>\n    </tr>\n    <tr>\n      <th>1790590</th>\n      <td>fffda3f22362</td>\n      <td>39ca5eaafffe</td>\n      <td>f7ca277a66c2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>4.497674e-07</td>\n      <td>NaN</td>\n      <td>1.633664e-06</td>\n      <td>NaN</td>\n      <td>1.558988e-06</td>\n      <td>NaN</td>\n      <td>3.526268e-07</td>\n      <td>NaN</td>\n      <td>0.982143</td>\n    </tr>\n    <tr>\n      <th>1790591</th>\n      <td>fffda3f22362</td>\n      <td>39ca5eaafffe</td>\n      <td>59714fd8dd25</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>6.327988e-07</td>\n      <td>NaN</td>\n      <td>4.497674e-07</td>\n      <td>NaN</td>\n      <td>1.633664e-06</td>\n      <td>NaN</td>\n      <td>1.558988e-06</td>\n      <td>NaN</td>\n      <td>0.988095</td>\n    </tr>\n    <tr>\n      <th>1790592</th>\n      <td>fffda3f22362</td>\n      <td>39ca5eaafffe</td>\n      <td>b33567349fae</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>4.913005e-06</td>\n      <td>NaN</td>\n      <td>6.327988e-07</td>\n      <td>NaN</td>\n      <td>4.497674e-07</td>\n      <td>NaN</td>\n      <td>1.633664e-06</td>\n      <td>NaN</td>\n      <td>0.994048</td>\n    </tr>\n    <tr>\n      <th>1790593</th>\n      <td>fffda3f22362</td>\n      <td>39ca5eaafffe</td>\n      <td>53d378d07811</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>2.411817e-06</td>\n      <td>NaN</td>\n      <td>4.913005e-06</td>\n      <td>NaN</td>\n      <td>6.327988e-07</td>\n      <td>NaN</td>\n      <td>4.497674e-07</td>\n      <td>NaN</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 41 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['pred0', 'pred0_post1', 'pred0_post2', 'pred0_post3', 'pred0_post4', 'pred0_post5', 'pred0_post6', 'pred0_post7', 'pred0_post8', 'pred0_post9', 'pred0_pre1', 'pred0_pre2', 'pred0_pre3', 'pred0_pre4', 'pred0_pre5', 'pred0_pre6', 'pred0_pre7', 'pred0_pre8', 'pred0_pre9', 'z_pos_norm']\n"
     ]
    }
   ],
   "source": [
    "ids = [c for c in list(train) if 'UID' in c]\n",
    "targets = [\n",
    "    'negative_exam_for_pe',\n",
    "    'indeterminate',\n",
    "    'chronic_pe',\n",
    "    'acute_and_chronic_pe',\n",
    "    'central_pe',\n",
    "    'leftsided_pe',\n",
    "    'rightsided_pe',\n",
    "    'rv_lv_ratio_gte_1',\n",
    "    'rv_lv_ratio_lt_1',\n",
    "]\n",
    "other_targets = [c for c in list(train) if 'pe_present_on_image' in c]\n",
    "### remove_cols = ['fold', 'path', 'weight', 'qa_contrast', 'qa_motion'] + targets + ids + other_targets\n",
    "### remove_cols = ['fold', 'path', 'weight', 'qa_contrast', 'qa_motion'] + ['exam_type','flow_artifact','pe_present_portion', 'true_filling_defect_not_pe'] + targets + ids + other_targets\n",
    "remove_cols = ['fold', 'path', 'weight', 'qa_contrast', 'qa_motion'] + ['exam_type','flow_artifact','pe_present_portion', 'true_filling_defect_not_pe'] + targets + ids + other_targets + ['z_pos']\n",
    "\n",
    "features = sorted(list(set(list(train)) - set(remove_cols)))\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_copyed = features.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fobj(pred, data):\n",
    "    true = data.get_label()\n",
    "    label = 2*true - 1\n",
    "    weights = data.weights\n",
    "    response = -label / (1 + np.exp(label * pred))\n",
    "    abs_response = np.abs(response)\n",
    "    grad = response\n",
    "    hess = abs_response * (1 - abs_response)\n",
    "    return grad*weights, hess*weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "bce_func = torch.nn.BCELoss(reduction='none')\n",
    "\n",
    "def feval2(preds, data):\n",
    "    scores = bce_func(torch.FloatTensor(preds), torch.FloatTensor(data.label))\n",
    "    scores = scores * torch.FloatTensor(data.weights)\n",
    "    return 'weighted logloss', torch.mean(scores), False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "bce_func_logit = torch.nn.BCEWithLogitsLoss(reduction='none')\n",
    "\n",
    "def feval(preds, data):\n",
    "    scores = bce_func_logit(torch.FloatTensor(preds), torch.FloatTensor(data.label))\n",
    "    scores = scores * torch.FloatTensor(data.weights)\n",
    "    return 'weighted logloss', torch.mean(scores), False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features = ['pred']  # for comparison\n",
    "features = features_copyed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "==================0================\n",
      "    ==============fold0================\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[53]\ttrain's weighted logloss: 0.0110919\tval's weighted logloss: 0.0110957\n",
      "    ==============fold1================\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[76]\ttrain's weighted logloss: 0.0107546\tval's weighted logloss: 0.0116312\n",
      "    ==============fold2================\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[70]\ttrain's weighted logloss: 0.0108946\tval's weighted logloss: 0.01141\n",
      "    ==============fold3================\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[182]\ttrain's weighted logloss: 0.0103165\tval's weighted logloss: 0.0117116\n",
      "    ==============fold4================\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[60]\ttrain's weighted logloss: 0.01096\tval's weighted logloss: 0.0112557\n",
      "-------------------------------------------------------------------------roc_auc: 0.9623247655817194\n",
      "----------------------------------------------------------roc_auc using raw pred: 0.950916101960868\n",
      "------------------------------------------------------------------------------AP: 0.7864683523887135\n",
      "---------------------------------------------------------------AP using raw pred: 0.7370492969022756\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "import pickle\n",
    "\n",
    "# oof_preds_list = []\n",
    "# models_list = []\n",
    "# target = 'pe_present_on_image'\n",
    "\n",
    "# for i in range(1):\n",
    "#     print(f'=================={i}================')\n",
    "#     if i % 4 == 0:\n",
    "#         params = {'boosting_type': 'gbdt',\n",
    "#             'objective': 'binary',\n",
    "# #             'metric': 'None',\n",
    "#             'subsample': 0.75,\n",
    "#             'subsample_freq': 1,\n",
    "#             'learning_rate': 0.1,\n",
    "#             'feature_fraction': 0.9,\n",
    "#             'max_depth': 15,\n",
    "#             'lambda_l1': 1,  \n",
    "#             'lambda_l2': 1,\n",
    "#             'verbose': 100,\n",
    "#             'early_stopping_rounds': 100,\n",
    "#             'verbose': -1,\n",
    "#             } \n",
    "#     elif i % 4 == 1:\n",
    "#         params = {\n",
    "#             'max_depth': 4,\n",
    "#             'max_leave': int(0.2 * 2 ** 4),\n",
    "#             'reg_lambda': 1,\n",
    "#             'reg_alpha': 1,\n",
    "#             'subsamples': 0.8,\n",
    "#             'colsample_bytree': 0.7,\n",
    "#             'objective': 'binary',\n",
    "#             'min_data_in_leaf': 0,\n",
    "#             'boosting': 'gbdt',\n",
    "#             'metric': 'None',\n",
    "#             'learning_rate': 0.1,\n",
    "#                       }\n",
    "#     elif i % 4 == 2:\n",
    "#         params = {\n",
    "#             'num_leaves': 19, \n",
    "#             'min_data_in_leaf': 160,\n",
    "#             'min_child_weight': 0.03,\n",
    "#             'bagging_fraction' : 0.7,\n",
    "#             'feature_fraction' : 0.8,\n",
    "#             'learning_rate' : 0.1,\n",
    "#             'max_depth': -1,\n",
    "#             'reg_alpha': 0.02,\n",
    "#             'reg_lambda': 0.12,\n",
    "#             'objective': 'binary',\n",
    "#             'verbose': 100,\n",
    "#             'boost_from_average': False,\n",
    "#             'metric': 'None',\n",
    "#         }  \n",
    "#     else:\n",
    "#         params = {\n",
    "#             'objective': \"binary\",\n",
    "#             'metric': 'None',\n",
    "#             'boost_from_average': \"false\",\n",
    "#             'tree_learner': \"serial\",\n",
    "#             'max_depth': -1,\n",
    "#             'learning_rate': 0.1,\n",
    "#             'num_leaves': 197,\n",
    "#             'feature_fraction': 0.3,\n",
    "#             'bagging_freq': 1,\n",
    "#             'bagging_fraction': 0.7,\n",
    "#             'min_data_in_leaf': 100,\n",
    "#             'bagging_seed': 11,\n",
    "#             'max_bin': 255,\n",
    "#             'verbosity': -1}    \n",
    "        \n",
    "#     oof_preds = np.zeros(train.shape[0])\n",
    "#     val_results = {}\n",
    "#     models = []\n",
    "#     params['random_state'] = i\n",
    "#     iter = 100000\n",
    "\n",
    "#     for n_fold in range(5):\n",
    "#     ### for n_fold in range(2):\n",
    "#         print(f'    ==============fold{n_fold}================')\n",
    "#         tr = train.query(f'fold != {n_fold}')\n",
    "#         val = train.query(f'fold == {n_fold}')\n",
    "#         trn_data = lgb.Dataset(tr[features], label=tr[target])\n",
    "#         trn_data.weights = tr.pe_present_portion.values\n",
    "#         val_data = lgb.Dataset(val[features], label=val[target])\n",
    "#         val_data.weights = val.pe_present_portion.values\n",
    "        \n",
    "#         clf = lgb.train(params, trn_data, num_boost_round=iter, valid_sets=[trn_data, val_data], valid_names=['train', 'val'],\n",
    "# #                         verbose_eval=200, early_stopping_rounds = 10/params['learning_rate'], evals_result=val_results,)\n",
    "#                         feval=feval, fobj = fobj, verbose_eval=2000, early_stopping_rounds = 10/params['learning_rate'], evals_result=val_results, )\n",
    "#         file = f'lgbs/lgb_seed{i}_fold{n_fold}.pkl'\n",
    "#         pickle.dump(clf, open(file, 'wb'))\n",
    "#         oof_preds[train.fold==n_fold] = clf.predict(val[features])\n",
    "#     oof_preds_list.append(oof_preds)\n",
    "\n",
    "# print(f'-------------------------------------------------------------------------roc_auc: {roc_auc_score(train[target], np.mean(oof_preds_list, axis=0))}')\n",
    "# print(f'----------------------------------------------------------roc_auc using raw pred: {roc_auc_score(train[target], train[\"pred0\"])}')\n",
    "# print(f'------------------------------------------------------------------------------AP: {average_precision_score(train[target], np.mean(oof_preds_list, axis=0))}')\n",
    "# print(f'---------------------------------------------------------------AP using raw pred: {average_precision_score(train[target], train[\"pred0\"])}')\n",
    "\n",
    "# lgb_oof = np.mean(oof_preds_list, axis=0)\n",
    "# train['lgb_preds'] = sigmoid(lgb_oof)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.011420365194671893"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "# bce_func = torch.nn.BCELoss(reduction='none')\n",
    "\n",
    "# lgb_losses = bce_func(torch.FloatTensor(sigmoid(lgb_oof)), torch.FloatTensor(train['pe_present_on_image']))\n",
    "\n",
    "# ### torch.mean(lgb_losses*train['weight'].values)\n",
    "# torch.mean(lgb_losses*train['pe_present_portion'].values).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.01322563879510154"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "# # no stacking result\n",
    "# lgb_losses = bce_func(torch.FloatTensor(train['pred0']), torch.FloatTensor(train['pe_present_on_image']))\n",
    "# torch.mean(lgb_losses*train['pe_present_portion'].values).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raise "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <<< USED FOR FINAL SUB >>> stacking for exam-level targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = Path(\"../input/rsna-str-pulmonary-embolism-detection/\")\n",
    "\n",
    "train = pd.read_csv(DATADIR / \"train.csv\")\n",
    "\n",
    "pre = pd.read_csv(DATADIR / \"split.csv\")\n",
    "train = train.merge(pre, on=\"StudyInstanceUID\")\n",
    "\n",
    "portion = pd.read_csv(DATADIR / \"study_pos_portion.csv\")\n",
    "train = train.merge(portion, on=\"StudyInstanceUID\")\n",
    "\n",
    "z_pos_df = pd.read_csv(DATADIR / \"sop_to_prefix.csv\").rename(columns={'img_prefix': 'z_pos'})\n",
    "train = train.merge(z_pos_df, on=\"SOPInstanceUID\")\n",
    "\n",
    "studies = train.StudyInstanceUID.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof = pd.concat([oof_f0, oof_f1, oof_f2, oof_f3, oof_f4]).rename(columns={'pe_present_on_image': 'pred'})\n",
    "\n",
    "train = train.merge(oof[['pred', 'SOPInstanceUID']], on=\"SOPInstanceUID\")  # add pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" ADD MOANI \"\"\"\n",
    "if 1:  # after refactoring\n",
    "    oof_monai = pd.concat([pd.read_csv(f\"./output_jan/oof{fold}.csv\") for fold in range(5)])\n",
    "    oof_monai = oof_monai.drop(\n",
    "        ['LOGITS_rv_lv_ratio_gte_1', 'LOGITS_central_pe', 'LOGITS_leftsided_pe', \n",
    "        'LOGITS_rightsided_pe', 'LOGITS_acute_and_chronic_pe', 'LOGITS_chronic_pe', 'fold'],\n",
    "        axis=1)\n",
    "else:  # used during comp\n",
    "    oof_monai = pd.read_csv(\"output_jan/5foldmonai/monai_6targets_oof.csv\").drop(\n",
    "        ['LOGITS_rv_lv_ratio_gte_1', 'LOGITS_central_pe', 'LOGITS_leftsided_pe', \n",
    "        'LOGITS_rightsided_pe', 'LOGITS_acute_and_chronic_pe', 'LOGITS_chronic_pe', 'fold'],\n",
    "        axis=1)\n",
    "\n",
    "for key in oof_monai.columns:\n",
    "    oof_monai = oof_monai.rename(columns={key: \"monai_\" + key})\n",
    "oof_monai = oof_monai.rename(columns={\"monai_study_id\": \"StudyInstanceUID\"})\n",
    "# print( oof_monai.head() )\n",
    "\n",
    "# train = train.merge(oof_monai, on=\"StudyInstanceUID\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\" ADD chronics_b0 acute_bo \"\"\"\n",
    "# def load_5oof(model_dir):\n",
    "#     oof = [ get_pred(model_dir + \"/oof_fold%d.pkl\" % fold ) for fold in range(5) ]\n",
    "#     oof = pd.concat( oof ) #.rename(columns={})\n",
    "#     oof_neg = [ get_pred(model_dir + \"/oof_neg_fold%d.pkl\" % fold ) for fold in range(5) ]\n",
    "#     oof_neg = pd.concat( oof_neg ) #.rename(columns={})\n",
    "#     return pd.concat([ oof, oof_neg ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1790594\n",
      "1790594\n"
     ]
    }
   ],
   "source": [
    "# USED RUDING COMP\n",
    "\n",
    "# # oof_chronic = load_5oof( \"output_yuji/chronics_b0\" )\n",
    "# # for k in oof_chronic.columns:\n",
    "# #     if k == \"SOPInstanceUID\": continue\n",
    "# #     oof_chronic = oof_chronic.rename(columns={k: \"chronic_model_\" + k})\n",
    "\n",
    "# oof_position = load_5oof( \"output_yuji/position_b0/\" )\n",
    "# for k in oof_position.columns:\n",
    "#     if k == \"SOPInstanceUID\": continue\n",
    "#     oof_position = oof_position.rename(columns={k: \"position_model_\" + k})\n",
    "\n",
    "# print(len(train))\n",
    "# # train = train.merge(oof_chronic , on='SOPInstanceUID')\n",
    "# train = train.merge(oof_position, on='SOPInstanceUID')\n",
    "# print(len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AFTER REFACTORING\n",
    "oof_position = pd.concat([ get_pred(f\"output/final_position/oof_fold{fold}.pkl\") for fold in range(5) ])\n",
    "\n",
    "for k in oof_position.columns:\n",
    "    if k == \"SOPInstanceUID\": continue\n",
    "    oof_position = oof_position.rename(columns={k: \"position_model_\" + k})\n",
    "\n",
    "print(len(train))\n",
    "train = train.merge(oof_position, on='SOPInstanceUID')\n",
    "print(len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Index(['SOPInstanceUID', 'position_model_pe_present_on_image',\n",
       "       'position_model_rightsided_pe', 'position_model_leftsided_pe',\n",
       "       'position_model_central_pe'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "execution_count": 140
    }
   ],
   "source": [
    "oof_position.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Index(['StudyInstanceUID', 'SeriesInstanceUID', 'SOPInstanceUID',\n",
       "       'pe_present_on_image', 'negative_exam_for_pe', 'qa_motion',\n",
       "       'qa_contrast', 'flow_artifact', 'rv_lv_ratio_gte_1', 'rv_lv_ratio_lt_1',\n",
       "       'leftsided_pe', 'chronic_pe', 'true_filling_defect_not_pe',\n",
       "       'rightsided_pe', 'acute_and_chronic_pe', 'central_pe', 'indeterminate',\n",
       "       'exam_type', 'fold', 'pe_present_portion', 'z_pos', 'pred',\n",
       "       'position_model_pe_present_on_image', 'position_model_rightsided_pe',\n",
       "       'position_model_leftsided_pe', 'position_model_central_pe'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "execution_count": 141
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "7279\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "def grouping(df):\n",
    "    grouped = pd.DataFrame(df.groupby('StudyInstanceUID')['pred'].mean())\n",
    "    grouped = grouped.rename(columns={'pred': 'mean'})\n",
    "    count = df.groupby('StudyInstanceUID')['pred'].count()\n",
    "    grouped['count_total'] = count\n",
    "\n",
    "    for i in range(1,10):\n",
    "        count = df[df.pred>i/10].groupby('StudyInstanceUID')['pred'].count()\n",
    "        grouped[f'count_over{i/10}'] = count\n",
    "        grouped[f'count_over{i/10}_ratio'] = count / grouped['count_total']\n",
    "\n",
    "    for q in [30, 50, 70, 80, 90, 95, 99]:\n",
    "        grouped[f'percentile{q}'] = df.groupby('StudyInstanceUID')['pred'].apply(lambda arr: np.percentile(arr, q))\n",
    "\n",
    "    ma = pd.DataFrame(df.groupby('StudyInstanceUID')['pred'].max())\n",
    "    grouped['max'] = ma.pred\n",
    "\n",
    "    grouped = grouped.reset_index().fillna(0)\n",
    "    del grouped['count_total']\n",
    "    return grouped\n",
    "\n",
    "def grouping_one(df, pred_col):\n",
    "    grouped = pd.DataFrame(df.groupby('StudyInstanceUID')[pred_col].mean())\n",
    "    grouped = grouped.rename(columns={pred_col: f'{pred_col}_mean'})\n",
    "    count = df.groupby('StudyInstanceUID')[pred_col].count()\n",
    "    grouped['count_total'] = count\n",
    "\n",
    "    for i in range(1,10):\n",
    "        count = df[df[pred_col]>i/10].groupby('StudyInstanceUID')[pred_col].count()\n",
    "        grouped[f'{pred_col}_count_over{i/10}'] = count\n",
    "        grouped[f'{pred_col}_count_over{i/10}_ratio'] = count / grouped['count_total']\n",
    "    count = df[df[pred_col]==1].groupby('StudyInstanceUID')[pred_col].count()\n",
    "\n",
    "    for q in [30, 50, 70, 80, 90, 95, 99]:\n",
    "        grouped[f'{pred_col}_percentile{q}'] = df.groupby('StudyInstanceUID')[pred_col].apply(lambda arr: np.percentile(arr, q))\n",
    "\n",
    "    ma = pd.DataFrame(df.groupby('StudyInstanceUID')[pred_col].max())\n",
    "    grouped[f'{pred_col}_max'] = ma[pred_col]\n",
    "\n",
    "    grouped = grouped.reset_index().fillna(0)\n",
    "    del grouped['count_total']\n",
    "    return grouped\n",
    "\n",
    "\n",
    "train_grouped = grouping(train)\n",
    "# ADD MONAI\n",
    "train_grouped = train_grouped.merge(oof_monai, on=\"StudyInstanceUID\")\n",
    "\n",
    "print( len (train_grouped) )\n",
    "### ADD CHRONIC, POS\n",
    "# for col in ['chronic_model_pe_present_on_image', 'chronic_model_chronic_pe', 'chronic_model_acute_and_chronic_pe', 'chronic_model_acute_pe'] \\\n",
    "#     + ['position_model_pe_present_on_image', 'position_model_rightsided_pe', 'position_model_leftsided_pe', 'position_model_central_pe']:\n",
    "for col in ['position_model_pe_present_on_image', 'position_model_rightsided_pe', 'position_model_leftsided_pe', 'position_model_central_pe']:\n",
    "    train_grouped = train_grouped.merge(\n",
    "        grouping_one(train, col), on=\"StudyInstanceUID\")\n",
    "\n",
    "\n",
    "train_grouped['fold'] = train.groupby('StudyInstanceUID')['fold'].first().values\n",
    "# add target\n",
    "train_grouped['negative_exam_for_pe'] = train.groupby('StudyInstanceUID')['negative_exam_for_pe'].first().values\n",
    "train_grouped['positive_exam_for_pe'] = (1 - train.groupby('StudyInstanceUID')['negative_exam_for_pe'].first().values) * (1 - train.groupby('StudyInstanceUID')['indeterminate'].first().values)\n",
    "for key in [\n",
    "    'chronic_pe',\n",
    "    'acute_and_chronic_pe',\n",
    "    'central_pe',\n",
    "    'leftsided_pe',\n",
    "    'rightsided_pe',\n",
    "    'rv_lv_ratio_gte_1',\n",
    "    'rv_lv_ratio_lt_1',]:\n",
    "    train_grouped[ key ] = train.groupby('StudyInstanceUID')[ key ].first().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['StudyInstanceUID', 'mean', 'count_over0.1', 'count_over0.1_ratio', 'count_over0.2', 'count_over0.2_ratio', 'count_over0.3', 'count_over0.3_ratio', 'count_over0.4', 'count_over0.4_ratio', 'count_over0.5', 'count_over0.5_ratio', 'count_over0.6', 'count_over0.6_ratio', 'count_over0.7', 'count_over0.7_ratio', 'count_over0.8', 'count_over0.8_ratio', 'count_over0.9', 'count_over0.9_ratio', 'percentile30', 'percentile50', 'percentile70', 'percentile80', 'percentile90', 'percentile95', 'percentile99', 'max', 'monai_rv_lv_ratio_gte_1', 'monai_central_pe', 'monai_leftsided_pe', 'monai_rightsided_pe', 'monai_acute_and_chronic_pe', 'monai_chronic_pe', 'position_model_pe_present_on_image_mean', 'position_model_pe_present_on_image_count_over0.1', 'position_model_pe_present_on_image_count_over0.1_ratio', 'position_model_pe_present_on_image_count_over0.2', 'position_model_pe_present_on_image_count_over0.2_ratio', 'position_model_pe_present_on_image_count_over0.3', 'position_model_pe_present_on_image_count_over0.3_ratio', 'position_model_pe_present_on_image_count_over0.4', 'position_model_pe_present_on_image_count_over0.4_ratio', 'position_model_pe_present_on_image_count_over0.5', 'position_model_pe_present_on_image_count_over0.5_ratio', 'position_model_pe_present_on_image_count_over0.6', 'position_model_pe_present_on_image_count_over0.6_ratio', 'position_model_pe_present_on_image_count_over0.7', 'position_model_pe_present_on_image_count_over0.7_ratio', 'position_model_pe_present_on_image_count_over0.8', 'position_model_pe_present_on_image_count_over0.8_ratio', 'position_model_pe_present_on_image_count_over0.9', 'position_model_pe_present_on_image_count_over0.9_ratio', 'position_model_pe_present_on_image_percentile30', 'position_model_pe_present_on_image_percentile50', 'position_model_pe_present_on_image_percentile70', 'position_model_pe_present_on_image_percentile80', 'position_model_pe_present_on_image_percentile90', 'position_model_pe_present_on_image_percentile95', 'position_model_pe_present_on_image_percentile99', 'position_model_pe_present_on_image_max', 'position_model_rightsided_pe_mean', 'position_model_rightsided_pe_count_over0.1', 'position_model_rightsided_pe_count_over0.1_ratio', 'position_model_rightsided_pe_count_over0.2', 'position_model_rightsided_pe_count_over0.2_ratio', 'position_model_rightsided_pe_count_over0.3', 'position_model_rightsided_pe_count_over0.3_ratio', 'position_model_rightsided_pe_count_over0.4', 'position_model_rightsided_pe_count_over0.4_ratio', 'position_model_rightsided_pe_count_over0.5', 'position_model_rightsided_pe_count_over0.5_ratio', 'position_model_rightsided_pe_count_over0.6', 'position_model_rightsided_pe_count_over0.6_ratio', 'position_model_rightsided_pe_count_over0.7', 'position_model_rightsided_pe_count_over0.7_ratio', 'position_model_rightsided_pe_count_over0.8', 'position_model_rightsided_pe_count_over0.8_ratio', 'position_model_rightsided_pe_count_over0.9', 'position_model_rightsided_pe_count_over0.9_ratio', 'position_model_rightsided_pe_percentile30', 'position_model_rightsided_pe_percentile50', 'position_model_rightsided_pe_percentile70', 'position_model_rightsided_pe_percentile80', 'position_model_rightsided_pe_percentile90', 'position_model_rightsided_pe_percentile95', 'position_model_rightsided_pe_percentile99', 'position_model_rightsided_pe_max', 'position_model_leftsided_pe_mean', 'position_model_leftsided_pe_count_over0.1', 'position_model_leftsided_pe_count_over0.1_ratio', 'position_model_leftsided_pe_count_over0.2', 'position_model_leftsided_pe_count_over0.2_ratio', 'position_model_leftsided_pe_count_over0.3', 'position_model_leftsided_pe_count_over0.3_ratio', 'position_model_leftsided_pe_count_over0.4', 'position_model_leftsided_pe_count_over0.4_ratio', 'position_model_leftsided_pe_count_over0.5', 'position_model_leftsided_pe_count_over0.5_ratio', 'position_model_leftsided_pe_count_over0.6', 'position_model_leftsided_pe_count_over0.6_ratio', 'position_model_leftsided_pe_count_over0.7', 'position_model_leftsided_pe_count_over0.7_ratio', 'position_model_leftsided_pe_count_over0.8', 'position_model_leftsided_pe_count_over0.8_ratio', 'position_model_leftsided_pe_count_over0.9', 'position_model_leftsided_pe_count_over0.9_ratio', 'position_model_leftsided_pe_percentile30', 'position_model_leftsided_pe_percentile50', 'position_model_leftsided_pe_percentile70', 'position_model_leftsided_pe_percentile80', 'position_model_leftsided_pe_percentile90', 'position_model_leftsided_pe_percentile95', 'position_model_leftsided_pe_percentile99', 'position_model_leftsided_pe_max', 'position_model_central_pe_mean', 'position_model_central_pe_count_over0.1', 'position_model_central_pe_count_over0.1_ratio', 'position_model_central_pe_count_over0.2', 'position_model_central_pe_count_over0.2_ratio', 'position_model_central_pe_count_over0.3', 'position_model_central_pe_count_over0.3_ratio', 'position_model_central_pe_count_over0.4', 'position_model_central_pe_count_over0.4_ratio', 'position_model_central_pe_count_over0.5', 'position_model_central_pe_count_over0.5_ratio', 'position_model_central_pe_count_over0.6', 'position_model_central_pe_count_over0.6_ratio', 'position_model_central_pe_count_over0.7', 'position_model_central_pe_count_over0.7_ratio', 'position_model_central_pe_count_over0.8', 'position_model_central_pe_count_over0.8_ratio', 'position_model_central_pe_count_over0.9', 'position_model_central_pe_count_over0.9_ratio', 'position_model_central_pe_percentile30', 'position_model_central_pe_percentile50', 'position_model_central_pe_percentile70', 'position_model_central_pe_percentile80', 'position_model_central_pe_percentile90', 'position_model_central_pe_percentile95', 'position_model_central_pe_percentile99', 'position_model_central_pe_max', 'fold', 'negative_exam_for_pe', 'positive_exam_for_pe', 'chronic_pe', 'acute_and_chronic_pe', 'central_pe', 'leftsided_pe', 'rightsided_pe', 'rv_lv_ratio_gte_1', 'rv_lv_ratio_lt_1']\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  StudyInstanceUID      mean  count_over0.1  count_over0.1_ratio  \\\n",
       "0     0003b3d648eb  0.271391          121.0             0.542601   \n",
       "1     000f7f114264  0.091386           65.0             0.271967   \n",
       "2     00102474a2db  0.055245           58.0             0.177914   \n",
       "3     0038fd5f09f5  0.054540           34.0             0.147826   \n",
       "4     0045f113e031  0.133363          111.0             0.431907   \n",
       "\n",
       "   count_over0.2  count_over0.2_ratio  count_over0.3  count_over0.3_ratio  \\\n",
       "0           94.0             0.421525           80.0             0.358744   \n",
       "1           45.0             0.188285           28.0             0.117155   \n",
       "2           21.0             0.064417           10.0             0.030675   \n",
       "3           13.0             0.056522            6.0             0.026087   \n",
       "4           75.0             0.291829           45.0             0.175097   \n",
       "\n",
       "   count_over0.4  count_over0.4_ratio  ...  fold  negative_exam_for_pe  \\\n",
       "0           61.0             0.273543  ...     0                     1   \n",
       "1           16.0             0.066946  ...     3                     1   \n",
       "2            4.0             0.012270  ...     3                     1   \n",
       "3            6.0             0.026087  ...     4                     1   \n",
       "4           22.0             0.085603  ...     4                     1   \n",
       "\n",
       "   positive_exam_for_pe  chronic_pe  acute_and_chronic_pe  central_pe  \\\n",
       "0                     0           0                     0           0   \n",
       "1                     0           0                     0           0   \n",
       "2                     0           0                     0           0   \n",
       "3                     0           0                     0           0   \n",
       "4                     0           0                     0           0   \n",
       "\n",
       "   leftsided_pe  rightsided_pe  rv_lv_ratio_gte_1  rv_lv_ratio_lt_1  \n",
       "0             0              0                  0                 0  \n",
       "1             0              0                  0                 0  \n",
       "2             0              0                  0                 0  \n",
       "3             0              0                  0                 0  \n",
       "4             0              0                  0                 0  \n",
       "\n",
       "[5 rows x 152 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>StudyInstanceUID</th>\n      <th>mean</th>\n      <th>count_over0.1</th>\n      <th>count_over0.1_ratio</th>\n      <th>count_over0.2</th>\n      <th>count_over0.2_ratio</th>\n      <th>count_over0.3</th>\n      <th>count_over0.3_ratio</th>\n      <th>count_over0.4</th>\n      <th>count_over0.4_ratio</th>\n      <th>...</th>\n      <th>fold</th>\n      <th>negative_exam_for_pe</th>\n      <th>positive_exam_for_pe</th>\n      <th>chronic_pe</th>\n      <th>acute_and_chronic_pe</th>\n      <th>central_pe</th>\n      <th>leftsided_pe</th>\n      <th>rightsided_pe</th>\n      <th>rv_lv_ratio_gte_1</th>\n      <th>rv_lv_ratio_lt_1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0003b3d648eb</td>\n      <td>0.271391</td>\n      <td>121.0</td>\n      <td>0.542601</td>\n      <td>94.0</td>\n      <td>0.421525</td>\n      <td>80.0</td>\n      <td>0.358744</td>\n      <td>61.0</td>\n      <td>0.273543</td>\n      <td>...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000f7f114264</td>\n      <td>0.091386</td>\n      <td>65.0</td>\n      <td>0.271967</td>\n      <td>45.0</td>\n      <td>0.188285</td>\n      <td>28.0</td>\n      <td>0.117155</td>\n      <td>16.0</td>\n      <td>0.066946</td>\n      <td>...</td>\n      <td>3</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00102474a2db</td>\n      <td>0.055245</td>\n      <td>58.0</td>\n      <td>0.177914</td>\n      <td>21.0</td>\n      <td>0.064417</td>\n      <td>10.0</td>\n      <td>0.030675</td>\n      <td>4.0</td>\n      <td>0.012270</td>\n      <td>...</td>\n      <td>3</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0038fd5f09f5</td>\n      <td>0.054540</td>\n      <td>34.0</td>\n      <td>0.147826</td>\n      <td>13.0</td>\n      <td>0.056522</td>\n      <td>6.0</td>\n      <td>0.026087</td>\n      <td>6.0</td>\n      <td>0.026087</td>\n      <td>...</td>\n      <td>4</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0045f113e031</td>\n      <td>0.133363</td>\n      <td>111.0</td>\n      <td>0.431907</td>\n      <td>75.0</td>\n      <td>0.291829</td>\n      <td>45.0</td>\n      <td>0.175097</td>\n      <td>22.0</td>\n      <td>0.085603</td>\n      <td>...</td>\n      <td>4</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 152 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 143
    }
   ],
   "source": [
    "print( list( train_grouped.columns ) )\n",
    "train_grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "     StudyInstanceUID      mean  count_over0.1  count_over0.1_ratio  \\\n",
       "0        0003b3d648eb  0.271391          121.0             0.542601   \n",
       "1        000f7f114264  0.091386           65.0             0.271967   \n",
       "2        00102474a2db  0.055245           58.0             0.177914   \n",
       "3        0038fd5f09f5  0.054540           34.0             0.147826   \n",
       "4        0045f113e031  0.133363          111.0             0.431907   \n",
       "...               ...       ...            ...                  ...   \n",
       "7274     ffe5280cd21d  0.333355          113.0             0.470833   \n",
       "7275     ffe7efaccbb9  0.018265            9.0             0.036000   \n",
       "7276     fff1ef450040  0.028014           13.0             0.047794   \n",
       "7277     fff9536823b1  0.181569          125.0             0.322165   \n",
       "7278     fffda3f22362  0.068718           40.0             0.236686   \n",
       "\n",
       "      count_over0.2  count_over0.2_ratio  count_over0.3  count_over0.3_ratio  \\\n",
       "0              94.0             0.421525           80.0             0.358744   \n",
       "1              45.0             0.188285           28.0             0.117155   \n",
       "2              21.0             0.064417           10.0             0.030675   \n",
       "3              13.0             0.056522            6.0             0.026087   \n",
       "4              75.0             0.291829           45.0             0.175097   \n",
       "...             ...                  ...            ...                  ...   \n",
       "7274          101.0             0.420833           89.0             0.370833   \n",
       "7275            3.0             0.012000            0.0             0.000000   \n",
       "7276            6.0             0.022059            3.0             0.011029   \n",
       "7277           85.0             0.219072           74.0             0.190722   \n",
       "7278           19.0             0.112426            6.0             0.035503   \n",
       "\n",
       "      count_over0.4  count_over0.4_ratio  ...  fold  negative_exam_for_pe  \\\n",
       "0              61.0             0.273543  ...     0                     1   \n",
       "1              16.0             0.066946  ...     3                     1   \n",
       "2               4.0             0.012270  ...     3                     1   \n",
       "3               6.0             0.026087  ...     4                     1   \n",
       "4              22.0             0.085603  ...     4                     1   \n",
       "...             ...                  ...  ...   ...                   ...   \n",
       "7274           82.0             0.341667  ...     4                     0   \n",
       "7275            0.0             0.000000  ...     3                     1   \n",
       "7276            3.0             0.011029  ...     3                     1   \n",
       "7277           71.0             0.182990  ...     0                     0   \n",
       "7278            3.0             0.017751  ...     0                     1   \n",
       "\n",
       "      positive_exam_for_pe  chronic_pe  acute_and_chronic_pe  central_pe  \\\n",
       "0                        0           0                     0           0   \n",
       "1                        0           0                     0           0   \n",
       "2                        0           0                     0           0   \n",
       "3                        0           0                     0           0   \n",
       "4                        0           0                     0           0   \n",
       "...                    ...         ...                   ...         ...   \n",
       "7274                     1           0                     0           0   \n",
       "7275                     0           0                     0           0   \n",
       "7276                     0           0                     0           0   \n",
       "7277                     1           0                     0           0   \n",
       "7278                     0           0                     0           0   \n",
       "\n",
       "      leftsided_pe  rightsided_pe  rv_lv_ratio_gte_1  rv_lv_ratio_lt_1  \n",
       "0                0              0                  0                 0  \n",
       "1                0              0                  0                 0  \n",
       "2                0              0                  0                 0  \n",
       "3                0              0                  0                 0  \n",
       "4                0              0                  0                 0  \n",
       "...            ...            ...                ...               ...  \n",
       "7274             0              1                  0                 1  \n",
       "7275             0              0                  0                 0  \n",
       "7276             0              0                  0                 0  \n",
       "7277             1              1                  0                 1  \n",
       "7278             0              0                  0                 0  \n",
       "\n",
       "[7279 rows x 152 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>StudyInstanceUID</th>\n      <th>mean</th>\n      <th>count_over0.1</th>\n      <th>count_over0.1_ratio</th>\n      <th>count_over0.2</th>\n      <th>count_over0.2_ratio</th>\n      <th>count_over0.3</th>\n      <th>count_over0.3_ratio</th>\n      <th>count_over0.4</th>\n      <th>count_over0.4_ratio</th>\n      <th>...</th>\n      <th>fold</th>\n      <th>negative_exam_for_pe</th>\n      <th>positive_exam_for_pe</th>\n      <th>chronic_pe</th>\n      <th>acute_and_chronic_pe</th>\n      <th>central_pe</th>\n      <th>leftsided_pe</th>\n      <th>rightsided_pe</th>\n      <th>rv_lv_ratio_gte_1</th>\n      <th>rv_lv_ratio_lt_1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0003b3d648eb</td>\n      <td>0.271391</td>\n      <td>121.0</td>\n      <td>0.542601</td>\n      <td>94.0</td>\n      <td>0.421525</td>\n      <td>80.0</td>\n      <td>0.358744</td>\n      <td>61.0</td>\n      <td>0.273543</td>\n      <td>...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000f7f114264</td>\n      <td>0.091386</td>\n      <td>65.0</td>\n      <td>0.271967</td>\n      <td>45.0</td>\n      <td>0.188285</td>\n      <td>28.0</td>\n      <td>0.117155</td>\n      <td>16.0</td>\n      <td>0.066946</td>\n      <td>...</td>\n      <td>3</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00102474a2db</td>\n      <td>0.055245</td>\n      <td>58.0</td>\n      <td>0.177914</td>\n      <td>21.0</td>\n      <td>0.064417</td>\n      <td>10.0</td>\n      <td>0.030675</td>\n      <td>4.0</td>\n      <td>0.012270</td>\n      <td>...</td>\n      <td>3</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0038fd5f09f5</td>\n      <td>0.054540</td>\n      <td>34.0</td>\n      <td>0.147826</td>\n      <td>13.0</td>\n      <td>0.056522</td>\n      <td>6.0</td>\n      <td>0.026087</td>\n      <td>6.0</td>\n      <td>0.026087</td>\n      <td>...</td>\n      <td>4</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0045f113e031</td>\n      <td>0.133363</td>\n      <td>111.0</td>\n      <td>0.431907</td>\n      <td>75.0</td>\n      <td>0.291829</td>\n      <td>45.0</td>\n      <td>0.175097</td>\n      <td>22.0</td>\n      <td>0.085603</td>\n      <td>...</td>\n      <td>4</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>7274</th>\n      <td>ffe5280cd21d</td>\n      <td>0.333355</td>\n      <td>113.0</td>\n      <td>0.470833</td>\n      <td>101.0</td>\n      <td>0.420833</td>\n      <td>89.0</td>\n      <td>0.370833</td>\n      <td>82.0</td>\n      <td>0.341667</td>\n      <td>...</td>\n      <td>4</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7275</th>\n      <td>ffe7efaccbb9</td>\n      <td>0.018265</td>\n      <td>9.0</td>\n      <td>0.036000</td>\n      <td>3.0</td>\n      <td>0.012000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>3</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7276</th>\n      <td>fff1ef450040</td>\n      <td>0.028014</td>\n      <td>13.0</td>\n      <td>0.047794</td>\n      <td>6.0</td>\n      <td>0.022059</td>\n      <td>3.0</td>\n      <td>0.011029</td>\n      <td>3.0</td>\n      <td>0.011029</td>\n      <td>...</td>\n      <td>3</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7277</th>\n      <td>fff9536823b1</td>\n      <td>0.181569</td>\n      <td>125.0</td>\n      <td>0.322165</td>\n      <td>85.0</td>\n      <td>0.219072</td>\n      <td>74.0</td>\n      <td>0.190722</td>\n      <td>71.0</td>\n      <td>0.182990</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7278</th>\n      <td>fffda3f22362</td>\n      <td>0.068718</td>\n      <td>40.0</td>\n      <td>0.236686</td>\n      <td>19.0</td>\n      <td>0.112426</td>\n      <td>6.0</td>\n      <td>0.035503</td>\n      <td>3.0</td>\n      <td>0.017751</td>\n      <td>...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>7279 rows × 152 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 144
    }
   ],
   "source": [
    "train_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "target rv_lv_ratio_lt_1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# features = list(set(list(train_grouped)) - set(['StudyInstanceUID', 'positive_exam_for_pe', 'negative_exam_for_pe', 'fold']) - set(['count_total']))\n",
    "\n",
    "### features = list( set(list(train_grouped)) - set(['StudyInstanceUID', 'positive_exam_for_pe', 'negative_exam_for_pe', 'fold']) - set(['count_total']) - set( [\n",
    "features = list( set(list(train_grouped)) - set(['StudyInstanceUID', 'positive_exam_for_pe', 'negative_exam_for_pe', 'fold']) - set( [\n",
    "    'chronic_pe',\n",
    "    'acute_and_chronic_pe',\n",
    "    'central_pe',\n",
    "    'leftsided_pe',\n",
    "    'rightsided_pe',\n",
    "    'rv_lv_ratio_gte_1',\n",
    "    'rv_lv_ratio_lt_1',]  ) )\n",
    "features = sorted(features)\n",
    "\n",
    "### target = 'positive_exam_for_pe'\n",
    "\n",
    "targets_monai = [\n",
    "        'rv_lv_ratio_gte_1', # exam level\n",
    "        \"central_pe\",\n",
    "        \"leftsided_pe\",\n",
    "        \"rightsided_pe\",\n",
    "        \"acute_and_chronic_pe\",\n",
    "        \"chronic_pe\"\n",
    "    ]\n",
    "#target = targets_monai[ 1 ]\n",
    "target = \"rv_lv_ratio_lt_1\"\n",
    "\n",
    "print(\"target\", target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['count_over0.1', 'count_over0.1_ratio', 'count_over0.2', 'count_over0.2_ratio', 'count_over0.3', 'count_over0.3_ratio', 'count_over0.4', 'count_over0.4_ratio', 'count_over0.5', 'count_over0.5_ratio', 'count_over0.6', 'count_over0.6_ratio', 'count_over0.7', 'count_over0.7_ratio', 'count_over0.8', 'count_over0.8_ratio', 'count_over0.9', 'count_over0.9_ratio', 'max', 'mean', 'monai_acute_and_chronic_pe', 'monai_central_pe', 'monai_chronic_pe', 'monai_leftsided_pe', 'monai_rightsided_pe', 'monai_rv_lv_ratio_gte_1', 'percentile30', 'percentile50', 'percentile70', 'percentile80', 'percentile90', 'percentile95', 'percentile99', 'position_model_central_pe_count_over0.1', 'position_model_central_pe_count_over0.1_ratio', 'position_model_central_pe_count_over0.2', 'position_model_central_pe_count_over0.2_ratio', 'position_model_central_pe_count_over0.3', 'position_model_central_pe_count_over0.3_ratio', 'position_model_central_pe_count_over0.4', 'position_model_central_pe_count_over0.4_ratio', 'position_model_central_pe_count_over0.5', 'position_model_central_pe_count_over0.5_ratio', 'position_model_central_pe_count_over0.6', 'position_model_central_pe_count_over0.6_ratio', 'position_model_central_pe_count_over0.7', 'position_model_central_pe_count_over0.7_ratio', 'position_model_central_pe_count_over0.8', 'position_model_central_pe_count_over0.8_ratio', 'position_model_central_pe_count_over0.9', 'position_model_central_pe_count_over0.9_ratio', 'position_model_central_pe_max', 'position_model_central_pe_mean', 'position_model_central_pe_percentile30', 'position_model_central_pe_percentile50', 'position_model_central_pe_percentile70', 'position_model_central_pe_percentile80', 'position_model_central_pe_percentile90', 'position_model_central_pe_percentile95', 'position_model_central_pe_percentile99', 'position_model_leftsided_pe_count_over0.1', 'position_model_leftsided_pe_count_over0.1_ratio', 'position_model_leftsided_pe_count_over0.2', 'position_model_leftsided_pe_count_over0.2_ratio', 'position_model_leftsided_pe_count_over0.3', 'position_model_leftsided_pe_count_over0.3_ratio', 'position_model_leftsided_pe_count_over0.4', 'position_model_leftsided_pe_count_over0.4_ratio', 'position_model_leftsided_pe_count_over0.5', 'position_model_leftsided_pe_count_over0.5_ratio', 'position_model_leftsided_pe_count_over0.6', 'position_model_leftsided_pe_count_over0.6_ratio', 'position_model_leftsided_pe_count_over0.7', 'position_model_leftsided_pe_count_over0.7_ratio', 'position_model_leftsided_pe_count_over0.8', 'position_model_leftsided_pe_count_over0.8_ratio', 'position_model_leftsided_pe_count_over0.9', 'position_model_leftsided_pe_count_over0.9_ratio', 'position_model_leftsided_pe_max', 'position_model_leftsided_pe_mean', 'position_model_leftsided_pe_percentile30', 'position_model_leftsided_pe_percentile50', 'position_model_leftsided_pe_percentile70', 'position_model_leftsided_pe_percentile80', 'position_model_leftsided_pe_percentile90', 'position_model_leftsided_pe_percentile95', 'position_model_leftsided_pe_percentile99', 'position_model_pe_present_on_image_count_over0.1', 'position_model_pe_present_on_image_count_over0.1_ratio', 'position_model_pe_present_on_image_count_over0.2', 'position_model_pe_present_on_image_count_over0.2_ratio', 'position_model_pe_present_on_image_count_over0.3', 'position_model_pe_present_on_image_count_over0.3_ratio', 'position_model_pe_present_on_image_count_over0.4', 'position_model_pe_present_on_image_count_over0.4_ratio', 'position_model_pe_present_on_image_count_over0.5', 'position_model_pe_present_on_image_count_over0.5_ratio', 'position_model_pe_present_on_image_count_over0.6', 'position_model_pe_present_on_image_count_over0.6_ratio', 'position_model_pe_present_on_image_count_over0.7', 'position_model_pe_present_on_image_count_over0.7_ratio', 'position_model_pe_present_on_image_count_over0.8', 'position_model_pe_present_on_image_count_over0.8_ratio', 'position_model_pe_present_on_image_count_over0.9', 'position_model_pe_present_on_image_count_over0.9_ratio', 'position_model_pe_present_on_image_max', 'position_model_pe_present_on_image_mean', 'position_model_pe_present_on_image_percentile30', 'position_model_pe_present_on_image_percentile50', 'position_model_pe_present_on_image_percentile70', 'position_model_pe_present_on_image_percentile80', 'position_model_pe_present_on_image_percentile90', 'position_model_pe_present_on_image_percentile95', 'position_model_pe_present_on_image_percentile99', 'position_model_rightsided_pe_count_over0.1', 'position_model_rightsided_pe_count_over0.1_ratio', 'position_model_rightsided_pe_count_over0.2', 'position_model_rightsided_pe_count_over0.2_ratio', 'position_model_rightsided_pe_count_over0.3', 'position_model_rightsided_pe_count_over0.3_ratio', 'position_model_rightsided_pe_count_over0.4', 'position_model_rightsided_pe_count_over0.4_ratio', 'position_model_rightsided_pe_count_over0.5', 'position_model_rightsided_pe_count_over0.5_ratio', 'position_model_rightsided_pe_count_over0.6', 'position_model_rightsided_pe_count_over0.6_ratio', 'position_model_rightsided_pe_count_over0.7', 'position_model_rightsided_pe_count_over0.7_ratio', 'position_model_rightsided_pe_count_over0.8', 'position_model_rightsided_pe_count_over0.8_ratio', 'position_model_rightsided_pe_count_over0.9', 'position_model_rightsided_pe_count_over0.9_ratio', 'position_model_rightsided_pe_max', 'position_model_rightsided_pe_mean', 'position_model_rightsided_pe_percentile30', 'position_model_rightsided_pe_percentile50', 'position_model_rightsided_pe_percentile70', 'position_model_rightsided_pe_percentile80', 'position_model_rightsided_pe_percentile90', 'position_model_rightsided_pe_percentile95', 'position_model_rightsided_pe_percentile99']\n"
     ]
    }
   ],
   "source": [
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "141\n"
     ]
    }
   ],
   "source": [
    "print(len(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "~~~~~~~~~~~~~~~~~~~~~ positive_exam_for_pe ~~~~~~~~~~~~~~~~~ \n",
      "\n",
      "==================0================\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[48]\ttrain's binary_logloss: 0.187879\tval's binary_logloss: 0.309329\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[28]\ttrain's binary_logloss: 0.239731\tval's binary_logloss: 0.318527\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[32]\ttrain's binary_logloss: 0.226629\tval's binary_logloss: 0.315525\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[31]\ttrain's binary_logloss: 0.222006\tval's binary_logloss: 0.349983\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[38]\ttrain's binary_logloss: 0.21148\tval's binary_logloss: 0.320348\n",
      "--------------------------------------------------------------------roc: 0.9004374518310364\n",
      "----------------------------------------------------------------logloss: 0.32274285812780806\n"
     ]
    }
   ],
   "source": [
    "\"\"\" WITH MONAI FEATURE \"\"\"\n",
    "\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "import pickle\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "### for target in targets_monai + ['rv_lv_ratio_lt_1']:\n",
    "### for target in ['positive_exam_for_pe']:\n",
    "for target in targets_monai + ['rv_lv_ratio_lt_1'] + ['positive_exam_for_pe']:\n",
    "\n",
    "    print(f\"~~~~~~~~~~~~~~~~~~~~~ {target} ~~~~~~~~~~~~~~~~~ \\n\")\n",
    "\n",
    "\n",
    "\n",
    "    oof_preds_list = []\n",
    "    test_preds_list = []\n",
    "    models_list = []\n",
    "\n",
    "    for i in range(1):\n",
    "        print(f'=================={i}================')\n",
    "        params = {'boosting_type': 'gbdt',\n",
    "            'objective': 'binary',\n",
    "    #             'metric': 'None',\n",
    "            'subsample': 0.75,\n",
    "            'subsample_freq': 1,\n",
    "            'learning_rate': 0.1,\n",
    "            'feature_fraction': 0.9,\n",
    "            'max_depth': 15,\n",
    "            'lambda_l1': 1,  \n",
    "            'lambda_l2': 1,\n",
    "            'verbose': 100,\n",
    "            'early_stopping_rounds': 100,\n",
    "            'verbose': -1,\n",
    "            } \n",
    "            \n",
    "        oof_preds = np.zeros(train_grouped.shape[0])\n",
    "\n",
    "        ### test_preds = np.zeros(test_grouped.shape[0])\n",
    "        val_results = {}\n",
    "        models = []\n",
    "        params['random_state'] = i\n",
    "        iter = 100000\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=72)\n",
    "\n",
    "        for n_fold in range( 5 ):\n",
    "            tr = train_grouped[train_grouped.fold != n_fold]\n",
    "            val = train_grouped[train_grouped.fold == n_fold]\n",
    "            trn_data = lgb.Dataset(tr[features], label=tr[target])\n",
    "            val_data = lgb.Dataset(val[features], label=val[target])\n",
    "            \n",
    "            clf = lgb.train(params, trn_data, num_boost_round=iter, valid_sets=[trn_data, val_data], valid_names=['train', 'val'],\n",
    "    #                         feval=feval, verbose_eval=10, early_stopping_rounds = 10/params['learning_rate'], evals_result=val_results,)\n",
    "                            verbose_eval=200, early_stopping_rounds = 10/params['learning_rate'], evals_result=val_results,)\n",
    "            file = f'lgbs/{target}_monai_lgb_seed{i}_fold{n_fold}.pkl'\n",
    "            pickle.dump(clf, open(file, 'wb'))\n",
    "            models.append(clf)\n",
    "            \n",
    "            oof_preds[train_grouped.fold == n_fold] = clf.predict(val[features])\n",
    "        oof_preds_list.append(oof_preds)\n",
    "\n",
    "    print(f'--------------------------------------------------------------------roc: {roc_auc_score(train_grouped[target], np.mean(oof_preds_list, axis=0))}')\n",
    "    print(f'----------------------------------------------------------------logloss: {log_loss(train_grouped[target], np.mean(oof_preds_list, axis=0))}')\n",
    "    lgb_oof_exam = np.mean(oof_preds_list, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"WITHOUT MONAI\"\"\"\n",
    "\n",
    "# target = \"positive_exam_for_pe\"\n",
    "\n",
    "# features_without_monai = [e for e in features if not e.startswith(\"monai_\")]\n",
    "# print( features_without_monai )\n",
    "# features = sorted( features_without_monai )\n",
    "\n",
    "# import lightgbm as lgb\n",
    "# import numpy as np\n",
    "# from sklearn.metrics import roc_auc_score\n",
    "# import warnings\n",
    "# warnings.simplefilter('ignore')\n",
    "# import pickle\n",
    "# from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "# oof_preds_list = []\n",
    "# test_preds_list = []\n",
    "# models_list = []\n",
    "\n",
    "# for i in range(1):\n",
    "#     print(f'=================={i}================')\n",
    "#     params = {'boosting_type': 'gbdt',\n",
    "#         'objective': 'binary',\n",
    "# #             'metric': 'None',\n",
    "#         'subsample': 0.75,\n",
    "#         'subsample_freq': 1,\n",
    "#         'learning_rate': 0.1,\n",
    "#         'feature_fraction': 0.9,\n",
    "#         'max_depth': 15,\n",
    "#         'lambda_l1': 1,  \n",
    "#         'lambda_l2': 1,\n",
    "#         'verbose': 100,\n",
    "#         'early_stopping_rounds': 100,\n",
    "#         'verbose': -1,\n",
    "#         } \n",
    "        \n",
    "#     oof_preds = np.zeros(train_grouped.shape[0])\n",
    "\n",
    "#     ### test_preds = np.zeros(test_grouped.shape[0])\n",
    "#     val_results = {}\n",
    "#     models = []\n",
    "#     params['random_state'] = i\n",
    "#     iter = 100000\n",
    "#     kf = KFold(n_splits=5, shuffle=True, random_state=72)\n",
    "\n",
    "#     for n_fold in range( 5 ):\n",
    "#         tr = train_grouped[train_grouped.fold != n_fold]\n",
    "#         val = train_grouped[train_grouped.fold == n_fold]\n",
    "#         trn_data = lgb.Dataset(tr[features_without_monai], label=tr[target])\n",
    "#         val_data = lgb.Dataset(val[features_without_monai], label=val[target])\n",
    "        \n",
    "#         clf = lgb.train(params, trn_data, num_boost_round=iter, valid_sets=[trn_data, val_data], valid_names=['train', 'val'],\n",
    "# #                         feval=feval, verbose_eval=10, early_stopping_rounds = 10/params['learning_rate'], evals_result=val_results,)\n",
    "#                         verbose_eval=200, early_stopping_rounds = 10/params['learning_rate'], evals_result=val_results,)\n",
    "#         file = f'lgbs/posexam_lgb_seed{i}_fold{n_fold}.pkl'\n",
    "#         pickle.dump(clf, open(file, 'wb'))\n",
    "#         models.append(clf)\n",
    "        \n",
    "#         oof_preds[train_grouped.fold == n_fold] = clf.predict(val[features_without_monai])\n",
    "#     oof_preds_list.append(oof_preds)\n",
    "\n",
    "# print(f'--------------------------------------------------------------------roc: {roc_auc_score(train_grouped[target], np.mean(oof_preds_list, axis=0))}')\n",
    "# print(f'----------------------------------------------------------------logloss: {log_loss(train_grouped[target], np.mean(oof_preds_list, axis=0))}')\n",
    "# lgb_oof_exam = np.mean(oof_preds_list, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.32274290919303894"
      ]
     },
     "metadata": {},
     "execution_count": 150
    }
   ],
   "source": [
    "bce_func = torch.nn.BCELoss(reduction='mean')\n",
    "lgb_losses = bce_func(torch.FloatTensor(oof_preds), torch.FloatTensor(train_grouped['positive_exam_for_pe']))\n",
    "\n",
    "torch.mean(lgb_losses).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.35468363761901855"
      ]
     },
     "metadata": {},
     "execution_count": 151
    }
   ],
   "source": [
    "bce_func = torch.nn.BCELoss(reduction='mean')\n",
    "lgb_losses = bce_func(\n",
    "    ( 1 - torch.FloatTensor(oof_preds) ) * (4911) / (4911 + 157), \n",
    "    torch.FloatTensor(train_grouped['negative_exam_for_pe']))\n",
    "\n",
    "torch.mean(lgb_losses).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # current yama's pipeline for fold0-ep1\n",
    "# def calib_p(arr, factor):  # set factor>1 to enhance positive prob\n",
    "#     return arr * factor / (arr * factor + (1-arr))\n",
    "# def post_yama(arr):\n",
    "#     return calib_p( np.percentile(arr, 95), factor=1/8.5550)\n",
    "\n",
    "# lgb_losses = bce_func(torch.FloatTensor(train[['StudyInstanceUID','pred']].groupby('StudyInstanceUID').apply(post_yama)), torch.FloatTensor(train_grouped['positive_exam_for_pe'])).item()\n",
    "# lgb_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}