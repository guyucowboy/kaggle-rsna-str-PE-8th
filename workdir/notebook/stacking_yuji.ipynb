{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.5 64-bit",
   "display_name": "Python 3.8.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/home/haito/kaggle/rsna-str/workdir\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "sys.path.append(\".\")\n",
    "from src.factory import *\n",
    "from src.utils import *\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = Path(\"../input/rsna-str-pulmonary-embolism-detection/\")\n",
    "\n",
    "train = pd.read_csv(DATADIR / \"train.csv\")\n",
    "\n",
    "pre = pd.read_csv(DATADIR / \"split.csv\")\n",
    "train = train.merge(pre, on=\"StudyInstanceUID\")\n",
    "\n",
    "portion = pd.read_csv(DATADIR / \"study_pos_portion.csv\")\n",
    "train = train.merge(portion, on=\"StudyInstanceUID\")\n",
    "\n",
    "z_pos_df = pd.read_csv(DATADIR / \"sop_to_prefix.csv\").rename(columns={'img_prefix': 'z_pos'})\n",
    "train = train.merge(z_pos_df, on=\"SOPInstanceUID\")\n",
    "\n",
    "\n",
    "### train = train.query(\"fold == 0 or fold == 1\")  # now I have fold0,1 only\n",
    "\n",
    "studies = train.StudyInstanceUID.unique()\n",
    "\n",
    "# agg = t.groupby(\"StudyInstanceUID\")[\"SOPInstanceUID\"].apply(list)\n",
    "# agg_one = t.groupby(\"StudyInstanceUID\").first()\n",
    "# t = t.set_index(\"SOPInstanceUID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred(_path):\n",
    "    res = load_pickle(_path)\n",
    "    raw_pred = pd.DataFrame({\n",
    "        \"SOPInstanceUID\": res[\"ids\"],\n",
    "        **res[\"outputs\"],\n",
    "    })\n",
    "    return raw_pred\n",
    "    # return raw_pred.set_index(\"sop\")\n",
    "\n",
    "def calib_p(arr, factor):  # set factor>1 to enhance positive prob\n",
    "    return arr * factor / (arr * factor + (1-arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_f0, fold0_calib_f = get_pred(\"output/035_pe_present___448/valid.fold0-ep1.picle\"), 8.555037588568537\n",
    "oof_f1, fold1_calib_f = get_pred(\"output/035_pe_present___448___apex___resume/valid.fold1-ep1.pickle\"), 5.72045\n",
    "oof_f2, fold2_calib_f = get_pred(\"output/035_pe_present___448___apex___resume/fold2_ep1.pt.valid.pickle\"), 3.3448360180113497\n",
    "oof_f3, fold3_calib_f = get_pred(\"output/035_pe_present___448___apex/fold3_ep1.pt.valid.pickle\"), 11.18792798738602,\n",
    "oof_f4, fold4_calib_f = get_pred(\"output/035_pe_present___448___apex___resume/fold4_ep1.pt.valid.pickle\"), 5.720451550292213\n",
    "\n",
    "# BAD\n",
    "if False:  # pick best one which yields weighted-logloss after calib\n",
    "    oof_f3, fold3_calib_f = get_pred(\"output/035_pe_present___448___apex/fold3_ep0.pt.valid.pickle\"), 6.541753595870311\n",
    "    oof_f4, fold4_calib_f = get_pred(\"output/035_pe_present___448___apex/fold4_ep0.pt.valid.pickle\"), 3.8250639579850194\n",
    "    \n",
    "\n",
    "if True: ### ==== do calib for each fold\n",
    "    oof_f0[\"pe_present_on_image\"] = calib_p(oof_f0[\"pe_present_on_image\"], fold0_calib_f)\n",
    "    oof_f1[\"pe_present_on_image\"] = calib_p(oof_f1[\"pe_present_on_image\"], fold1_calib_f)\n",
    "    oof_f2[\"pe_present_on_image\"] = calib_p(oof_f2[\"pe_present_on_image\"], fold2_calib_f)\n",
    "    oof_f3[\"pe_present_on_image\"] = calib_p(oof_f3[\"pe_present_on_image\"], fold3_calib_f)\n",
    "    oof_f4[\"pe_present_on_image\"] = calib_p(oof_f4[\"pe_present_on_image\"], fold4_calib_f)\n",
    "\n",
    "oof = pd.concat([oof_f0, oof_f1, oof_f2, oof_f3, oof_f4]).rename(columns={'pe_present_on_image': 'pred0'})\n",
    "\n",
    "train = train.merge(oof[['pred0', 'SOPInstanceUID']], on=\"SOPInstanceUID\")  # add pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Index(['StudyInstanceUID', 'SeriesInstanceUID', 'SOPInstanceUID',\n",
       "       'pe_present_on_image', 'negative_exam_for_pe', 'qa_motion',\n",
       "       'qa_contrast', 'flow_artifact', 'rv_lv_ratio_gte_1', 'rv_lv_ratio_lt_1',\n",
       "       'leftsided_pe', 'chronic_pe', 'true_filling_defect_not_pe',\n",
       "       'rightsided_pe', 'acute_and_chronic_pe', 'central_pe', 'indeterminate',\n",
       "       'exam_type', 'fold', 'pe_present_portion', 'z_pos', 'pred0'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "train_copyed = train.copy()\n",
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "' feature engineer '"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "\"\"\" feature engineer \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = pd.read_pickle('../input/train_with_position.pkl')\n",
    "train = train.sort_values(['StudyInstanceUID', 'z_pos'])\n",
    "# train = train.merge(oof[['pred', 'SOPInstanceUID']], on='SOPInstanceUID')\n",
    "\n",
    "# test = pd.read_pickle('../input/test_with_position.pkl')\n",
    "# test = test.sort_values(['StudyInstanceUID', 'z_pos'])\n",
    "# test = test.merge(test_pred[pe_present_portion], on='SOPInstanceUID')\n",
    "\n",
    "# train = train.merge(oof_non_weight[['pred_non_weight', 'SOPInstanceUID']], on='SOPInstanceUID')\n",
    "# test = test.merge(test_pred_non_weight[['pred_non_weight', 'SOPInstanceUID']], on='SOPInstanceUID')\n",
    "# train['pred_mean'] = (train['pred'] + train['pred_non_weight']) / 2\n",
    "# test['pred_mean'] = (test['pred'] + test['pred_non_weight']) / 2\n",
    "\n",
    "train_current_z_pos = train.groupby('StudyInstanceUID')['z_pos'].shift(0)\n",
    "# test_current_z_pos = test.groupby('StudyInstanceUID')['z_pos'].shift(0)\n",
    "### for i in range(1, 20):\n",
    "for i in range(1, 10):\n",
    "\n",
    "    # train[f'pre_mean{i}'] = train.groupby('StudyInstanceUID')['pred_mean'].shift(i)\n",
    "    # train[f'post_mean{i}'] = train.groupby('StudyInstanceUID')['pred_mean'].shift(-i)\n",
    "    train[f'pred0_pre{i}'] = train.groupby('StudyInstanceUID')['pred0'].shift(i)\n",
    "    train[f'pred0_post{i}'] = train.groupby('StudyInstanceUID')['pred0'].shift(-i)\n",
    "    # train[f'pre_non_weight{i}'] = train.groupby('StudyInstanceUID')['pred_non_weight'].shift(i)\n",
    "    # train[f'post_non_weight{i}'] = train.groupby('StudyInstanceUID')['pred_non_weight'].shift(-i)\n",
    "    \n",
    "    # test[f'pre_mean{i}'] = test.groupby('StudyInstanceUID')['pred_mean'].shift(i)\n",
    "    # test[f'post_mean{i}'] = test.groupby('StudyInstanceUID')['pred_mean'].shift(-i)\n",
    "    # test[f'pre{i}'] = test.groupby('StudyInstanceUID')['pred'].shift(i)\n",
    "    # test[f'post{i}'] = test.groupby('StudyInstanceUID')['pred'].shift(-i)\n",
    "    # test[f'pre_non_weight{i}'] = test.groupby('StudyInstanceUID')['pred_non_weight'].shift(i)\n",
    "    # test[f'post_non_weight{i}'] = test.groupby('StudyInstanceUID')['pred_non_weight'].shift(-i)\n",
    "\n",
    "\n",
    "# for i in [1]:\n",
    "#     train[f'pre_z_pos_diff{i}'] = train_current_z_pos - train.groupby('StudyInstanceUID')['pred'].shift(i)\n",
    "#     # test[f'pre_z_pos_diff{i}'] = test_current_z_pos - test.groupby('StudyInstanceUID')['pred'].shift(i)\n",
    "\n",
    "# NORMALIZED Z POS\n",
    "z_max = train.groupby('StudyInstanceUID').z_pos.max().rename('z_pos_max')\n",
    "train = train.merge(z_max, on='StudyInstanceUID')\n",
    "train['z_pos_norm'] = train['z_pos'] / train['z_pos_max']\n",
    "train = train.drop('z_pos_max', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "        StudyInstanceUID SeriesInstanceUID SOPInstanceUID  \\\n",
       "1790589     fffda3f22362      39ca5eaafffe   29e855db7f2b   \n",
       "1790590     fffda3f22362      39ca5eaafffe   f7ca277a66c2   \n",
       "1790591     fffda3f22362      39ca5eaafffe   59714fd8dd25   \n",
       "1790592     fffda3f22362      39ca5eaafffe   b33567349fae   \n",
       "1790593     fffda3f22362      39ca5eaafffe   53d378d07811   \n",
       "\n",
       "         pe_present_on_image  negative_exam_for_pe  qa_motion  qa_contrast  \\\n",
       "1790589                    0                     1          0            0   \n",
       "1790590                    0                     1          0            0   \n",
       "1790591                    0                     1          0            0   \n",
       "1790592                    0                     1          0            0   \n",
       "1790593                    0                     1          0            0   \n",
       "\n",
       "         flow_artifact  rv_lv_ratio_gte_1  rv_lv_ratio_lt_1  ...  pred0_post5  \\\n",
       "1790589              0                  0                 0  ...          NaN   \n",
       "1790590              0                  0                 0  ...          NaN   \n",
       "1790591              0                  0                 0  ...          NaN   \n",
       "1790592              0                  0                 0  ...          NaN   \n",
       "1790593              0                  0                 0  ...          NaN   \n",
       "\n",
       "         pred0_pre6  pred0_post6  pred0_pre7  pred0_post7  pred0_pre8  \\\n",
       "1790589    0.001988          NaN    0.002619          NaN    0.003691   \n",
       "1790590    0.001816          NaN    0.001988          NaN    0.002619   \n",
       "1790591    0.001840          NaN    0.001816          NaN    0.001988   \n",
       "1790592    0.001995          NaN    0.001840          NaN    0.001816   \n",
       "1790593    0.001884          NaN    0.001995          NaN    0.001840   \n",
       "\n",
       "         pred0_post8 pred0_pre9  pred0_post9  z_pos_norm  \n",
       "1790589          NaN   0.002995          NaN    0.976190  \n",
       "1790590          NaN   0.003691          NaN    0.982143  \n",
       "1790591          NaN   0.002619          NaN    0.988095  \n",
       "1790592          NaN   0.001988          NaN    0.994048  \n",
       "1790593          NaN   0.001816          NaN    1.000000  \n",
       "\n",
       "[5 rows x 41 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>StudyInstanceUID</th>\n      <th>SeriesInstanceUID</th>\n      <th>SOPInstanceUID</th>\n      <th>pe_present_on_image</th>\n      <th>negative_exam_for_pe</th>\n      <th>qa_motion</th>\n      <th>qa_contrast</th>\n      <th>flow_artifact</th>\n      <th>rv_lv_ratio_gte_1</th>\n      <th>rv_lv_ratio_lt_1</th>\n      <th>...</th>\n      <th>pred0_post5</th>\n      <th>pred0_pre6</th>\n      <th>pred0_post6</th>\n      <th>pred0_pre7</th>\n      <th>pred0_post7</th>\n      <th>pred0_pre8</th>\n      <th>pred0_post8</th>\n      <th>pred0_pre9</th>\n      <th>pred0_post9</th>\n      <th>z_pos_norm</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1790589</th>\n      <td>fffda3f22362</td>\n      <td>39ca5eaafffe</td>\n      <td>29e855db7f2b</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>0.001988</td>\n      <td>NaN</td>\n      <td>0.002619</td>\n      <td>NaN</td>\n      <td>0.003691</td>\n      <td>NaN</td>\n      <td>0.002995</td>\n      <td>NaN</td>\n      <td>0.976190</td>\n    </tr>\n    <tr>\n      <th>1790590</th>\n      <td>fffda3f22362</td>\n      <td>39ca5eaafffe</td>\n      <td>f7ca277a66c2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>0.001816</td>\n      <td>NaN</td>\n      <td>0.001988</td>\n      <td>NaN</td>\n      <td>0.002619</td>\n      <td>NaN</td>\n      <td>0.003691</td>\n      <td>NaN</td>\n      <td>0.982143</td>\n    </tr>\n    <tr>\n      <th>1790591</th>\n      <td>fffda3f22362</td>\n      <td>39ca5eaafffe</td>\n      <td>59714fd8dd25</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>0.001840</td>\n      <td>NaN</td>\n      <td>0.001816</td>\n      <td>NaN</td>\n      <td>0.001988</td>\n      <td>NaN</td>\n      <td>0.002619</td>\n      <td>NaN</td>\n      <td>0.988095</td>\n    </tr>\n    <tr>\n      <th>1790592</th>\n      <td>fffda3f22362</td>\n      <td>39ca5eaafffe</td>\n      <td>b33567349fae</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>0.001995</td>\n      <td>NaN</td>\n      <td>0.001840</td>\n      <td>NaN</td>\n      <td>0.001816</td>\n      <td>NaN</td>\n      <td>0.001988</td>\n      <td>NaN</td>\n      <td>0.994048</td>\n    </tr>\n    <tr>\n      <th>1790593</th>\n      <td>fffda3f22362</td>\n      <td>39ca5eaafffe</td>\n      <td>53d378d07811</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>0.001884</td>\n      <td>NaN</td>\n      <td>0.001995</td>\n      <td>NaN</td>\n      <td>0.001840</td>\n      <td>NaN</td>\n      <td>0.001816</td>\n      <td>NaN</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 41 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['pred0', 'pred0_post1', 'pred0_post2', 'pred0_post3', 'pred0_post4', 'pred0_post5', 'pred0_post6', 'pred0_post7', 'pred0_post8', 'pred0_post9', 'pred0_pre1', 'pred0_pre2', 'pred0_pre3', 'pred0_pre4', 'pred0_pre5', 'pred0_pre6', 'pred0_pre7', 'pred0_pre8', 'pred0_pre9', 'z_pos_norm']\n"
     ]
    }
   ],
   "source": [
    "ids = [c for c in list(train) if 'UID' in c]\n",
    "targets = [\n",
    "    'negative_exam_for_pe',\n",
    "    'indeterminate',\n",
    "    'chronic_pe',\n",
    "    'acute_and_chronic_pe',\n",
    "    'central_pe',\n",
    "    'leftsided_pe',\n",
    "    'rightsided_pe',\n",
    "    'rv_lv_ratio_gte_1',\n",
    "    'rv_lv_ratio_lt_1',\n",
    "]\n",
    "other_targets = [c for c in list(train) if 'pe_present_on_image' in c]\n",
    "### remove_cols = ['fold', 'path', 'weight', 'qa_contrast', 'qa_motion'] + targets + ids + other_targets\n",
    "### remove_cols = ['fold', 'path', 'weight', 'qa_contrast', 'qa_motion'] + ['exam_type','flow_artifact','pe_present_portion', 'true_filling_defect_not_pe'] + targets + ids + other_targets\n",
    "remove_cols = ['fold', 'path', 'weight', 'qa_contrast', 'qa_motion'] + ['exam_type','flow_artifact','pe_present_portion', 'true_filling_defect_not_pe'] + targets + ids + other_targets + ['z_pos']\n",
    "\n",
    "features = sorted(list(set(list(train)) - set(remove_cols)))\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_copyed = features.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fobj(pred, data):\n",
    "    true = data.get_label()\n",
    "    label = 2*true - 1\n",
    "    weights = data.weights\n",
    "    response = -label / (1 + np.exp(label * pred))\n",
    "    abs_response = np.abs(response)\n",
    "    grad = response\n",
    "    hess = abs_response * (1 - abs_response)\n",
    "    return grad*weights, hess*weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "bce_func = torch.nn.BCELoss(reduction='none')\n",
    "\n",
    "def feval2(preds, data):\n",
    "    scores = bce_func(torch.FloatTensor(preds), torch.FloatTensor(data.label))\n",
    "    scores = scores * torch.FloatTensor(data.weights)\n",
    "    return 'weighted logloss', torch.mean(scores), False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 推測して作成したもの. Yujiに確認する必要がある\n",
    "import torch\n",
    "bce_func_logit = torch.nn.BCEWithLogitsLoss(reduction='none')\n",
    "\n",
    "def feval(preds, data):\n",
    "    scores = bce_func_logit(torch.FloatTensor(preds), torch.FloatTensor(data.label))\n",
    "    scores = scores * torch.FloatTensor(data.weights)\n",
    "    return 'weighted logloss', torch.mean(scores), False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['pred']\n",
    "features = features_copyed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "==================0================\n",
      "    ==============fold0================\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[88]\ttrain's weighted logloss: 0.0108274\tval's weighted logloss: 0.0115188\n",
      "    ==============fold1================\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[99]\ttrain's weighted logloss: 0.0106117\tval's weighted logloss: 0.0121366\n",
      "    ==============fold2================\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[68]\ttrain's weighted logloss: 0.0110975\tval's weighted logloss: 0.0108253\n",
      "    ==============fold3================\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[102]\ttrain's weighted logloss: 0.0107973\tval's weighted logloss: 0.0113527\n",
      "    ==============fold4================\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[58]\ttrain's weighted logloss: 0.011062\tval's weighted logloss: 0.0113463\n",
      "-------------------------------------------------------------------------roc_auc: 0.9617052543270864\n",
      "----------------------------------------------------------roc_auc using raw pred: 0.9485047588286195\n",
      "------------------------------------------------------------------------------AP: 0.7815652580157645\n",
      "---------------------------------------------------------------AP using raw pred: 0.7264076541068915\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "import pickle\n",
    "\n",
    "oof_preds_list = []\n",
    "test_preds_list = []\n",
    "models_list = []\n",
    "target = 'pe_present_on_image'\n",
    "\n",
    "for i in range(1):\n",
    "    print(f'=================={i}================')\n",
    "    if i % 4 == 0:\n",
    "        params = {'boosting_type': 'gbdt',\n",
    "            'objective': 'binary',\n",
    "#             'metric': 'None',\n",
    "            'subsample': 0.75,\n",
    "            'subsample_freq': 1,\n",
    "            'learning_rate': 0.1,\n",
    "            'feature_fraction': 0.9,\n",
    "            'max_depth': 15,\n",
    "            'lambda_l1': 1,  \n",
    "            'lambda_l2': 1,\n",
    "            'verbose': 100,\n",
    "            'early_stopping_rounds': 100,\n",
    "            'verbose': -1,\n",
    "            } \n",
    "    elif i % 4 == 1:\n",
    "        params = {\n",
    "            'max_depth': 4,\n",
    "            'max_leave': int(0.2 * 2 ** 4),\n",
    "            'reg_lambda': 1,\n",
    "            'reg_alpha': 1,\n",
    "            'subsamples': 0.8,\n",
    "            'colsample_bytree': 0.7,\n",
    "            'objective': 'binary',\n",
    "            'min_data_in_leaf': 0,\n",
    "            'boosting': 'gbdt',\n",
    "            'metric': 'None',\n",
    "            'learning_rate': 0.1,\n",
    "                      }\n",
    "    elif i % 4 == 2:\n",
    "        params = {\n",
    "            'num_leaves': 19, \n",
    "            'min_data_in_leaf': 160,\n",
    "            'min_child_weight': 0.03,\n",
    "            'bagging_fraction' : 0.7,\n",
    "            'feature_fraction' : 0.8,\n",
    "            'learning_rate' : 0.1,\n",
    "            'max_depth': -1,\n",
    "            'reg_alpha': 0.02,\n",
    "            'reg_lambda': 0.12,\n",
    "            'objective': 'binary',\n",
    "            'verbose': 100,\n",
    "            'boost_from_average': False,\n",
    "            'metric': 'None',\n",
    "        }  \n",
    "    else:\n",
    "        params = {\n",
    "            'objective': \"binary\",\n",
    "            'metric': 'None',\n",
    "            'boost_from_average': \"false\",\n",
    "            'tree_learner': \"serial\",\n",
    "            'max_depth': -1,\n",
    "            'learning_rate': 0.1,\n",
    "            'num_leaves': 197,\n",
    "            'feature_fraction': 0.3,\n",
    "            'bagging_freq': 1,\n",
    "            'bagging_fraction': 0.7,\n",
    "            'min_data_in_leaf': 100,\n",
    "            'bagging_seed': 11,\n",
    "            'max_bin': 255,\n",
    "            'verbosity': -1}    \n",
    "        \n",
    "    oof_preds = np.zeros(train.shape[0])\n",
    "    ### test_preds = np.zeros(test.shape[0])\n",
    "    val_results = {}\n",
    "    models = []\n",
    "    params['random_state'] = i\n",
    "    iter = 100000\n",
    "#     for n_fold, (trn_idx, val_idx) in enumerate(kf.split(train, train[target])):\n",
    "\n",
    "    for n_fold in range(5):\n",
    "    ### for n_fold in range(2):\n",
    "        print(f'    ==============fold{n_fold}================')\n",
    "        tr = train.query(f'fold != {n_fold}')\n",
    "        val = train.query(f'fold == {n_fold}')\n",
    "        trn_data = lgb.Dataset(tr[features], label=tr[target])\n",
    "        trn_data.weights = tr.pe_present_portion.values\n",
    "        val_data = lgb.Dataset(val[features], label=val[target])\n",
    "        val_data.weights = val.pe_present_portion.values\n",
    "        \n",
    "        clf = lgb.train(params, trn_data, num_boost_round=iter, valid_sets=[trn_data, val_data], valid_names=['train', 'val'],\n",
    "#                         verbose_eval=200, early_stopping_rounds = 10/params['learning_rate'], evals_result=val_results,)\n",
    "                        feval=feval, fobj = fobj, verbose_eval=2000, early_stopping_rounds = 10/params['learning_rate'], evals_result=val_results, )\n",
    "        file = f'lgbs/lgb_seed{i}_fold{n_fold}.pkl'\n",
    "        pickle.dump(clf, open(file, 'wb'))\n",
    "#         models.append(clf)\n",
    "        oof_preds[train.fold==n_fold] = clf.predict(val[features])\n",
    "        ### test_preds += clf.predict(test[features]) / 5\n",
    "#     models_list.append(models)\n",
    "    oof_preds_list.append(oof_preds)\n",
    "    ### test_preds_list.append(test_preds)\n",
    "\n",
    "print(f'-------------------------------------------------------------------------roc_auc: {roc_auc_score(train[target], np.mean(oof_preds_list, axis=0))}')\n",
    "print(f'----------------------------------------------------------roc_auc using raw pred: {roc_auc_score(train[target], train[\"pred0\"])}')\n",
    "print(f'------------------------------------------------------------------------------AP: {average_precision_score(train[target], np.mean(oof_preds_list, axis=0))}')\n",
    "print(f'---------------------------------------------------------------AP using raw pred: {average_precision_score(train[target], train[\"pred0\"])}')\n",
    "\n",
    "lgb_oof = np.mean(oof_preds_list, axis=0)\n",
    "lgb_preds = np.mean(test_preds_list, axis=0)\n",
    "train['lgb_preds'] = sigmoid(lgb_oof)\n",
    "### test['lgb_preds'] = sigmoid(lgb_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.011431804800068808"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "bce_func = torch.nn.BCELoss(reduction='none')\n",
    "\n",
    "lgb_losses = bce_func(torch.FloatTensor(sigmoid(lgb_oof)), torch.FloatTensor(train['pe_present_on_image']))\n",
    "\n",
    "### torch.mean(lgb_losses*train['weight'].values)\n",
    "torch.mean(lgb_losses*train['pe_present_portion'].values).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.013218380246001216"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "# no stacking result\n",
    "lgb_losses = bce_func(torch.FloatTensor(train['pred0']), torch.FloatTensor(train['pe_present_on_image']))\n",
    "torch.mean(lgb_losses*train['pe_present_portion'].values).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "No active exception to reraise",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-01be21e57880>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: No active exception to reraise"
     ]
    }
   ],
   "source": [
    "raise "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PE_REPESNT -> POS_EXAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = Path(\"../input/rsna-str-pulmonary-embolism-detection/\")\n",
    "\n",
    "train = pd.read_csv(DATADIR / \"train.csv\")\n",
    "\n",
    "pre = pd.read_csv(DATADIR / \"split.csv\")\n",
    "train = train.merge(pre, on=\"StudyInstanceUID\")\n",
    "\n",
    "portion = pd.read_csv(DATADIR / \"study_pos_portion.csv\")\n",
    "train = train.merge(portion, on=\"StudyInstanceUID\")\n",
    "\n",
    "z_pos_df = pd.read_csv(DATADIR / \"sop_to_prefix.csv\").rename(columns={'img_prefix': 'z_pos'})\n",
    "train = train.merge(z_pos_df, on=\"SOPInstanceUID\")\n",
    "\n",
    "studies = train.StudyInstanceUID.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof = pd.concat([oof_f0, oof_f1, oof_f2, oof_f3, oof_f4]).rename(columns={'pe_present_on_image': 'pred'})\n",
    "\n",
    "train = train.merge(oof[['pred', 'SOPInstanceUID']], on=\"SOPInstanceUID\")  # add pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "def grouping(df):\n",
    "    grouped = pd.DataFrame(df.groupby('StudyInstanceUID')['pred'].mean())\n",
    "    grouped = grouped.rename(columns={'pred': 'mean'})\n",
    "    count = df.groupby('StudyInstanceUID')['pred'].count()\n",
    "    grouped['count_total'] = count\n",
    "\n",
    "    for i in range(1,10):\n",
    "        count = df[df.pred>i/10].groupby('StudyInstanceUID')['pred'].count()\n",
    "        grouped[f'count_over{i/10}'] = count\n",
    "        grouped[f'count_over{i/10}_ratio'] = count / grouped['count_total']\n",
    "\n",
    "    for q in [30, 50, 70, 80, 90, 95, 99]:\n",
    "    # for q in [95]:\n",
    "        grouped[f'percentile{q}'] = df.groupby('StudyInstanceUID')['pred'].apply(lambda arr: np.percentile(arr, q))\n",
    "\n",
    "    ma = pd.DataFrame(df.groupby('StudyInstanceUID')['pred'].max())\n",
    "    grouped['max'] = ma.pred\n",
    "\n",
    "    grouped = grouped.reset_index().fillna(0)\n",
    "    return grouped\n",
    "train_grouped = grouping(train)\n",
    "\n",
    "train_grouped['fold'] = train.groupby('StudyInstanceUID')['fold'].first().values\n",
    "train_grouped['negative_exam_for_pe'] = train.groupby('StudyInstanceUID')['negative_exam_for_pe'].first().values\n",
    "train_grouped['positive_exam_for_pe'] = (1 - train.groupby('StudyInstanceUID')['negative_exam_for_pe'].first().values) * (1 - train.groupby('StudyInstanceUID')['indeterminate'].first().values)\n",
    "\n",
    "# test_grouped = grouping(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features = list(set(list(train_grouped)) - set(['StudyInstanceUID', 'negative_exam_for_pe']))\n",
    "# target = 'negative_exam_for_pe'\n",
    "\n",
    "### features = list(set(list(train_grouped)) - set(['StudyInstanceUID', 'positive_exam_for_pe', 'negative_exam_for_pe', 'fold']))\n",
    "features = list(set(list(train_grouped)) - set(['StudyInstanceUID', 'positive_exam_for_pe', 'negative_exam_for_pe', 'fold']) - set(['count_total']))\n",
    "features = sorted(features)\n",
    "target = 'positive_exam_for_pe'\n",
    "# target = 'negative_exam_for_pe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['count_over0.1', 'count_over0.1_ratio', 'count_over0.2', 'count_over0.2_ratio', 'count_over0.3', 'count_over0.3_ratio', 'count_over0.4', 'count_over0.4_ratio', 'count_over0.5', 'count_over0.5_ratio', 'count_over0.6', 'count_over0.6_ratio', 'count_over0.7', 'count_over0.7_ratio', 'count_over0.8', 'count_over0.8_ratio', 'count_over0.9', 'count_over0.9_ratio', 'max', 'mean', 'percentile30', 'percentile50', 'percentile70', 'percentile80', 'percentile90', 'percentile95', 'percentile99']\n"
     ]
    }
   ],
   "source": [
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  StudyInstanceUID      mean  count_total  count_over0.1  count_over0.1_ratio  \\\n",
       "0     0003b3d648eb  0.271476          223          129.0             0.578475   \n",
       "1     000f7f114264  0.079362          239           67.0             0.280335   \n",
       "2     00102474a2db  0.067501          326           64.0             0.196319   \n",
       "3     0038fd5f09f5  0.037987          230           18.0             0.078261   \n",
       "4     0045f113e031  0.132310          257           95.0             0.369650   \n",
       "\n",
       "   count_over0.2  count_over0.2_ratio  count_over0.3  count_over0.3_ratio  \\\n",
       "0           96.0             0.430493           78.0             0.349776   \n",
       "1           43.0             0.179916           17.0             0.071130   \n",
       "2           25.0             0.076687           20.0             0.061350   \n",
       "3           10.0             0.043478            8.0             0.034783   \n",
       "4           62.0             0.241245           42.0             0.163424   \n",
       "\n",
       "   count_over0.4  ...  percentile50  percentile70  percentile80  percentile90  \\\n",
       "0           67.0  ...      0.156067      0.398198      0.580246      0.817519   \n",
       "1            7.0  ...      0.002301      0.083110      0.175883      0.266065   \n",
       "2           13.0  ...      0.010709      0.052737      0.093520      0.170775   \n",
       "3            6.0  ...      0.002431      0.014937      0.027755      0.081765   \n",
       "4           32.0  ...      0.041298      0.138919      0.253426      0.458115   \n",
       "\n",
       "   percentile95  percentile99       max  fold  negative_exam_for_pe  \\\n",
       "0      0.889579      0.930089  0.961301     0                     1   \n",
       "1      0.331230      0.578595  0.781551     3                     1   \n",
       "2      0.340288      0.717843  0.838232     3                     1   \n",
       "3      0.162589      0.655598  0.956098     4                     1   \n",
       "4      0.581166      0.705473  0.757177     4                     1   \n",
       "\n",
       "   positive_exam_for_pe  \n",
       "0                     0  \n",
       "1                     0  \n",
       "2                     0  \n",
       "3                     0  \n",
       "4                     0  \n",
       "\n",
       "[5 rows x 32 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>StudyInstanceUID</th>\n      <th>mean</th>\n      <th>count_total</th>\n      <th>count_over0.1</th>\n      <th>count_over0.1_ratio</th>\n      <th>count_over0.2</th>\n      <th>count_over0.2_ratio</th>\n      <th>count_over0.3</th>\n      <th>count_over0.3_ratio</th>\n      <th>count_over0.4</th>\n      <th>...</th>\n      <th>percentile50</th>\n      <th>percentile70</th>\n      <th>percentile80</th>\n      <th>percentile90</th>\n      <th>percentile95</th>\n      <th>percentile99</th>\n      <th>max</th>\n      <th>fold</th>\n      <th>negative_exam_for_pe</th>\n      <th>positive_exam_for_pe</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0003b3d648eb</td>\n      <td>0.271476</td>\n      <td>223</td>\n      <td>129.0</td>\n      <td>0.578475</td>\n      <td>96.0</td>\n      <td>0.430493</td>\n      <td>78.0</td>\n      <td>0.349776</td>\n      <td>67.0</td>\n      <td>...</td>\n      <td>0.156067</td>\n      <td>0.398198</td>\n      <td>0.580246</td>\n      <td>0.817519</td>\n      <td>0.889579</td>\n      <td>0.930089</td>\n      <td>0.961301</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000f7f114264</td>\n      <td>0.079362</td>\n      <td>239</td>\n      <td>67.0</td>\n      <td>0.280335</td>\n      <td>43.0</td>\n      <td>0.179916</td>\n      <td>17.0</td>\n      <td>0.071130</td>\n      <td>7.0</td>\n      <td>...</td>\n      <td>0.002301</td>\n      <td>0.083110</td>\n      <td>0.175883</td>\n      <td>0.266065</td>\n      <td>0.331230</td>\n      <td>0.578595</td>\n      <td>0.781551</td>\n      <td>3</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00102474a2db</td>\n      <td>0.067501</td>\n      <td>326</td>\n      <td>64.0</td>\n      <td>0.196319</td>\n      <td>25.0</td>\n      <td>0.076687</td>\n      <td>20.0</td>\n      <td>0.061350</td>\n      <td>13.0</td>\n      <td>...</td>\n      <td>0.010709</td>\n      <td>0.052737</td>\n      <td>0.093520</td>\n      <td>0.170775</td>\n      <td>0.340288</td>\n      <td>0.717843</td>\n      <td>0.838232</td>\n      <td>3</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0038fd5f09f5</td>\n      <td>0.037987</td>\n      <td>230</td>\n      <td>18.0</td>\n      <td>0.078261</td>\n      <td>10.0</td>\n      <td>0.043478</td>\n      <td>8.0</td>\n      <td>0.034783</td>\n      <td>6.0</td>\n      <td>...</td>\n      <td>0.002431</td>\n      <td>0.014937</td>\n      <td>0.027755</td>\n      <td>0.081765</td>\n      <td>0.162589</td>\n      <td>0.655598</td>\n      <td>0.956098</td>\n      <td>4</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0045f113e031</td>\n      <td>0.132310</td>\n      <td>257</td>\n      <td>95.0</td>\n      <td>0.369650</td>\n      <td>62.0</td>\n      <td>0.241245</td>\n      <td>42.0</td>\n      <td>0.163424</td>\n      <td>32.0</td>\n      <td>...</td>\n      <td>0.041298</td>\n      <td>0.138919</td>\n      <td>0.253426</td>\n      <td>0.458115</td>\n      <td>0.581166</td>\n      <td>0.705473</td>\n      <td>0.757177</td>\n      <td>4</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 32 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 87
    }
   ],
   "source": [
    "train_grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "==================0================\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[41]\ttrain's binary_logloss: 0.25003\tval's binary_logloss: 0.316147\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[45]\ttrain's binary_logloss: 0.24036\tval's binary_logloss: 0.328105\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[30]\ttrain's binary_logloss: 0.266725\tval's binary_logloss: 0.334468\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[36]\ttrain's binary_logloss: 0.251294\tval's binary_logloss: 0.35767\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[33]\ttrain's binary_logloss: 0.259245\tval's binary_logloss: 0.334683\n",
      "--------------------------------------------------------------------roc: 0.8938033874539193\n",
      "----------------------------------------------------------------logloss: 0.33421460972858597\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "import pickle\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "oof_preds_list = []\n",
    "test_preds_list = []\n",
    "models_list = []\n",
    "\n",
    "for i in range(1):\n",
    "    print(f'=================={i}================')\n",
    "    if i % 4 == 0:\n",
    "        params = {'boosting_type': 'gbdt',\n",
    "            'objective': 'binary',\n",
    "#             'metric': 'None',\n",
    "            'subsample': 0.75,\n",
    "            'subsample_freq': 1,\n",
    "            'learning_rate': 0.1,\n",
    "            'feature_fraction': 0.9,\n",
    "            'max_depth': 15,\n",
    "            'lambda_l1': 1,  \n",
    "            'lambda_l2': 1,\n",
    "            'verbose': 100,\n",
    "            'early_stopping_rounds': 100,\n",
    "            'verbose': -1,\n",
    "            } \n",
    "    elif i % 4 == 1:\n",
    "        params = {\n",
    "            'max_depth': 4,\n",
    "            'max_leave': int(0.2 * 2 ** 4),\n",
    "            'reg_lambda': 1,\n",
    "            'reg_alpha': 1,\n",
    "            'subsamples': 0.8,\n",
    "            'colsample_bytree': 0.7,\n",
    "            'objective': 'binary',\n",
    "            'min_data_in_leaf': 0,\n",
    "            'boosting': 'gbdt',\n",
    "#             'metric': 'None',\n",
    "            'learning_rate': 0.1,\n",
    "                      }\n",
    "    elif i % 4 == 2:\n",
    "        params = {\n",
    "            'num_leaves': 19, \n",
    "            'min_data_in_leaf': 160,\n",
    "            'min_child_weight': 0.03,\n",
    "            'bagging_fraction' : 0.7,\n",
    "            'feature_fraction' : 0.8,\n",
    "            'learning_rate' : 0.1,\n",
    "            'max_depth': -1,\n",
    "            'reg_alpha': 0.02,\n",
    "            'reg_lambda': 0.12,\n",
    "            'objective': 'binary',\n",
    "            'verbose': 100,\n",
    "            'boost_from_average': False,\n",
    "#             'metric': 'None',\n",
    "        }  \n",
    "    else:\n",
    "        params = {\n",
    "            'objective': \"binary\",\n",
    "#             'metric': 'None',\n",
    "            'boost_from_average': \"false\",\n",
    "            'tree_learner': \"serial\",\n",
    "            'max_depth': -1,\n",
    "            'learning_rate': 0.1,\n",
    "            'num_leaves': 197,\n",
    "            'feature_fraction': 0.3,\n",
    "            'bagging_freq': 1,\n",
    "            'bagging_fraction': 0.7,\n",
    "            'min_data_in_leaf': 100,\n",
    "            'bagging_seed': 11,\n",
    "            'max_bin': 255,\n",
    "            'verbosity': -1}    \n",
    "        \n",
    "    oof_preds = np.zeros(train_grouped.shape[0])\n",
    "\n",
    "    ### test_preds = np.zeros(test_grouped.shape[0])\n",
    "    val_results = {}\n",
    "    models = []\n",
    "    params['random_state'] = i\n",
    "    iter = 100000\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=72)\n",
    "\n",
    "    #for n_fold, (trn_idx, val_idx) in enumerate(kf.split(train_grouped, train_grouped[target])):\n",
    "        # tr = train_grouped.iloc[trn_idx]\n",
    "        # val = train_grouped.iloc[val_idx]\n",
    "    for n_fold in range( 5 ):\n",
    "        tr = train_grouped[train_grouped.fold != n_fold]\n",
    "        val = train_grouped[train_grouped.fold == n_fold]\n",
    "        trn_data = lgb.Dataset(tr[features], label=tr[target])\n",
    "        val_data = lgb.Dataset(val[features], label=val[target])\n",
    "        \n",
    "        clf = lgb.train(params, trn_data, num_boost_round=iter, valid_sets=[trn_data, val_data], valid_names=['train', 'val'],\n",
    "#                         feval=feval, verbose_eval=10, early_stopping_rounds = 10/params['learning_rate'], evals_result=val_results,)\n",
    "                        verbose_eval=200, early_stopping_rounds = 10/params['learning_rate'], evals_result=val_results,)\n",
    "        file = f'lgbs/lgb_seed{i}_fold{n_fold}.pkl'\n",
    "        pickle.dump(clf, open(file, 'wb'))\n",
    "        models.append(clf)\n",
    "        \n",
    "        oof_preds[train_grouped.fold == n_fold] = clf.predict(val[features])\n",
    "        ### oof_preds[val_idx] = clf.predict(val[features])\n",
    "        ### test_preds += clf.predict(test_grouped[features]) / 5\n",
    "#     models_list.append(models)\n",
    "    oof_preds_list.append(oof_preds)\n",
    "    ###test_preds_list.append(test_preds)\n",
    "\n",
    "print(f'--------------------------------------------------------------------roc: {roc_auc_score(train_grouped[target], np.mean(oof_preds_list, axis=0))}')\n",
    "print(f'----------------------------------------------------------------logloss: {log_loss(train_grouped[target], np.mean(oof_preds_list, axis=0))}')\n",
    "lgb_oof_exam = np.mean(oof_preds_list, axis=0)\n",
    "### lgb_preds = np.mean(test_preds_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.33421462774276733"
      ]
     },
     "metadata": {},
     "execution_count": 89
    }
   ],
   "source": [
    "bce_func = torch.nn.BCELoss(reduction='mean')\n",
    "lgb_losses = bce_func(torch.FloatTensor(oof_preds), torch.FloatTensor(train_grouped['positive_exam_for_pe']))\n",
    "\n",
    "torch.mean(lgb_losses).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.3707713186740875"
      ]
     },
     "metadata": {},
     "execution_count": 71
    }
   ],
   "source": [
    "bce_func = torch.nn.BCELoss(reduction='mean')\n",
    "lgb_losses = bce_func(\n",
    "    ( 1 - torch.FloatTensor(oof_preds) ) * (4911) / (4911 + 157), \n",
    "    torch.FloatTensor(train_grouped['negative_exam_for_pe']))\n",
    "\n",
    "torch.mean(lgb_losses).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # current yama's pipeline for fold0-ep1\n",
    "# def calib_p(arr, factor):  # set factor>1 to enhance positive prob\n",
    "#     return arr * factor / (arr * factor + (1-arr))\n",
    "# def post_yama(arr):\n",
    "#     return calib_p( np.percentile(arr, 95), factor=1/8.5550)\n",
    "\n",
    "# lgb_losses = bce_func(torch.FloatTensor(train[['StudyInstanceUID','pred']].groupby('StudyInstanceUID').apply(post_yama)), torch.FloatTensor(train_grouped['positive_exam_for_pe'])).item()\n",
    "# lgb_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}