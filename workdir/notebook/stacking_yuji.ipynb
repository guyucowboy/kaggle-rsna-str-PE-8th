{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.5 64-bit",
   "display_name": "Python 3.8.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/home/haito/kaggle/rsna-str/workdir\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "sys.path.append(\".\")\n",
    "from src.factory import *\n",
    "from src.utils import *\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "DATADIR = Path(\"../input/rsna-str-pulmonary-embolism-detection/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(DATADIR / \"train.csv\")\n",
    "\n",
    "pre = pd.read_csv(DATADIR / \"split.csv\")\n",
    "train = train.merge(pre, on=\"StudyInstanceUID\")\n",
    "\n",
    "portion = pd.read_csv(DATADIR / \"study_pos_portion.csv\")\n",
    "train = train.merge(portion, on=\"StudyInstanceUID\")\n",
    "\n",
    "z_pos_df = pd.read_csv(DATADIR / \"sop_to_prefix.csv\").rename(columns={'img_prefix': 'z_pos'})\n",
    "train = train.merge(z_pos_df, on=\"SOPInstanceUID\")\n",
    "\n",
    "train = train.query(\"fold == 0 or fold == 1\")  # now I have fold0,1 only\n",
    "studies = train.StudyInstanceUID.unique()\n",
    "\n",
    "# agg = t.groupby(\"StudyInstanceUID\")[\"SOPInstanceUID\"].apply(list)\n",
    "# agg_one = t.groupby(\"StudyInstanceUID\").first()\n",
    "# t = t.set_index(\"SOPInstanceUID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred(_path):\n",
    "    res = load_pickle(_path)\n",
    "    raw_pred = pd.DataFrame({\n",
    "        \"SOPInstanceUID\": res[\"ids\"],\n",
    "        **res[\"outputs\"],\n",
    "    })\n",
    "    return raw_pred\n",
    "    # return raw_pred.set_index(\"sop\")\n",
    "\n",
    "def calib_p(arr, factor):  # set factor>1 to enhance positive prob\n",
    "    return arr * factor / (arr * factor + (1-arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_fold0 = \"output/035_pe_present___448/valid.fold0-ep1.picle\"\n",
    "valid_fold0_calib_f = 8.555037588568537\n",
    "valid_fold1 = \"output/035_pe_present___448___apex___resume/valid.fold1-ep1.pickle\"\n",
    "valid_fold1_calib_f = 5.72045\n",
    "\n",
    "oof_f0 = get_pred(valid_fold0)\n",
    "oof_f1 = get_pred(valid_fold1)\n",
    "\n",
    "if True: ## do calib for each fold\n",
    "    oof_f0[\"pe_present_on_image\"] = calib_p(oof_f0[\"pe_present_on_image\"], valid_fold0_calib_f)\n",
    "    oof_f1[\"pe_present_on_image\"] = calib_p(oof_f1[\"pe_present_on_image\"], valid_fold1_calib_f)\n",
    "\n",
    "oof = pd.concat([oof_f0, oof_f1]).rename(columns={'pe_present_on_image': 'pred'})\n",
    "\n",
    "train = train.merge(oof[['pred', 'SOPInstanceUID']], on=\"SOPInstanceUID\")  # add pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Index(['StudyInstanceUID', 'SeriesInstanceUID', 'SOPInstanceUID',\n",
       "       'pe_present_on_image', 'negative_exam_for_pe', 'qa_motion',\n",
       "       'qa_contrast', 'flow_artifact', 'rv_lv_ratio_gte_1', 'rv_lv_ratio_lt_1',\n",
       "       'leftsided_pe', 'chronic_pe', 'true_filling_defect_not_pe',\n",
       "       'rightsided_pe', 'acute_and_chronic_pe', 'central_pe', 'indeterminate',\n",
       "       'exam_type', 'fold', 'pe_present_portion', 'z_pos', 'pred'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "train_copyed = train.copy()\n",
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" feature engineer \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = pd.read_pickle('../input/train_with_position.pkl')\n",
    "train = train.sort_values(['StudyInstanceUID', 'z_pos'])\n",
    "# train = train.merge(oof[['pred', 'SOPInstanceUID']], on='SOPInstanceUID')\n",
    "\n",
    "# test = pd.read_pickle('../input/test_with_position.pkl')\n",
    "# test = test.sort_values(['StudyInstanceUID', 'z_pos'])\n",
    "# test = test.merge(test_pred[pe_present_portion], on='SOPInstanceUID')\n",
    "\n",
    "# train = train.merge(oof_non_weight[['pred_non_weight', 'SOPInstanceUID']], on='SOPInstanceUID')\n",
    "# test = test.merge(test_pred_non_weight[['pred_non_weight', 'SOPInstanceUID']], on='SOPInstanceUID')\n",
    "# train['pred_mean'] = (train['pred'] + train['pred_non_weight']) / 2\n",
    "# test['pred_mean'] = (test['pred'] + test['pred_non_weight']) / 2\n",
    "\n",
    "train_current_z_pos = train.groupby('StudyInstanceUID')['z_pos'].shift(0)\n",
    "# test_current_z_pos = test.groupby('StudyInstanceUID')['z_pos'].shift(0)\n",
    "for i in range(1, 20):\n",
    "    # train[f'pre_mean{i}'] = train.groupby('StudyInstanceUID')['pred_mean'].shift(i)\n",
    "    # train[f'post_mean{i}'] = train.groupby('StudyInstanceUID')['pred_mean'].shift(-i)\n",
    "    train[f'pre{i}'] = train.groupby('StudyInstanceUID')['pred'].shift(i)\n",
    "    train[f'post{i}'] = train.groupby('StudyInstanceUID')['pred'].shift(-i)\n",
    "    # train[f'pre_non_weight{i}'] = train.groupby('StudyInstanceUID')['pred_non_weight'].shift(i)\n",
    "    # train[f'post_non_weight{i}'] = train.groupby('StudyInstanceUID')['pred_non_weight'].shift(-i)\n",
    "    \n",
    "    # test[f'pre_mean{i}'] = test.groupby('StudyInstanceUID')['pred_mean'].shift(i)\n",
    "    # test[f'post_mean{i}'] = test.groupby('StudyInstanceUID')['pred_mean'].shift(-i)\n",
    "    # test[f'pre{i}'] = test.groupby('StudyInstanceUID')['pred'].shift(i)\n",
    "    # test[f'post{i}'] = test.groupby('StudyInstanceUID')['pred'].shift(-i)\n",
    "    # test[f'pre_non_weight{i}'] = test.groupby('StudyInstanceUID')['pred_non_weight'].shift(i)\n",
    "    # test[f'post_non_weight{i}'] = test.groupby('StudyInstanceUID')['pred_non_weight'].shift(-i)\n",
    "\n",
    "for i in [1]:\n",
    "    train[f'pre_z_pos_diff{i}'] = train_current_z_pos - train.groupby('StudyInstanceUID')['pred'].shift(i)\n",
    "    # test[f'pre_z_pos_diff{i}'] = test_current_z_pos - test.groupby('StudyInstanceUID')['pred'].shift(i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       StudyInstanceUID SeriesInstanceUID SOPInstanceUID  pe_present_on_image  \\\n",
       "558384     0003b3d648eb      d2b2960c2bbf   14605bcc564c                    0   \n",
       "558367     0003b3d648eb      d2b2960c2bbf   d0849d3b6507                    0   \n",
       "558324     0003b3d648eb      d2b2960c2bbf   18928b724b69                    0   \n",
       "558359     0003b3d648eb      d2b2960c2bbf   56bc011203f4                    0   \n",
       "558312     0003b3d648eb      d2b2960c2bbf   24b933c4c518                    0   \n",
       "...                 ...               ...            ...                  ...   \n",
       "98085      fffda3f22362      39ca5eaafffe   29e855db7f2b                    0   \n",
       "98023      fffda3f22362      39ca5eaafffe   f7ca277a66c2                    0   \n",
       "98131      fffda3f22362      39ca5eaafffe   59714fd8dd25                    0   \n",
       "98141      fffda3f22362      39ca5eaafffe   b33567349fae                    0   \n",
       "98010      fffda3f22362      39ca5eaafffe   53d378d07811                    0   \n",
       "\n",
       "        negative_exam_for_pe  qa_motion  qa_contrast  flow_artifact  \\\n",
       "558384                     1          0            0              0   \n",
       "558367                     1          0            0              0   \n",
       "558324                     1          0            0              0   \n",
       "558359                     1          0            0              0   \n",
       "558312                     1          0            0              0   \n",
       "...                      ...        ...          ...            ...   \n",
       "98085                      1          0            0              0   \n",
       "98023                      1          0            0              0   \n",
       "98131                      1          0            0              0   \n",
       "98141                      1          0            0              0   \n",
       "98010                      1          0            0              0   \n",
       "\n",
       "        rv_lv_ratio_gte_1  rv_lv_ratio_lt_1  ...    post15     pre16  \\\n",
       "558384                  0                 0  ...  0.000145       NaN   \n",
       "558367                  0                 0  ...  0.000095       NaN   \n",
       "558324                  0                 0  ...  0.000078       NaN   \n",
       "558359                  0                 0  ...  0.000102       NaN   \n",
       "558312                  0                 0  ...  0.000166       NaN   \n",
       "...                   ...               ...  ...       ...       ...   \n",
       "98085                   0                 0  ...       NaN  0.002196   \n",
       "98023                   0                 0  ...       NaN  0.001723   \n",
       "98131                   0                 0  ...       NaN  0.001989   \n",
       "98141                   0                 0  ...       NaN  0.002083   \n",
       "98010                   0                 0  ...       NaN  0.002783   \n",
       "\n",
       "          post16     pre17    post17     pre18    post18     pre19    post19  \\\n",
       "558384  0.000095       NaN  0.000078       NaN  0.000102       NaN  0.000166   \n",
       "558367  0.000078       NaN  0.000102       NaN  0.000166       NaN  0.000127   \n",
       "558324  0.000102       NaN  0.000166       NaN  0.000127       NaN  0.000175   \n",
       "558359  0.000166       NaN  0.000127       NaN  0.000175       NaN  0.000263   \n",
       "558312  0.000127       NaN  0.000175       NaN  0.000263       NaN  0.000334   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "98085        NaN  0.002147       NaN  0.002662       NaN  0.002301       NaN   \n",
       "98023        NaN  0.002196       NaN  0.002147       NaN  0.002662       NaN   \n",
       "98131        NaN  0.001723       NaN  0.002196       NaN  0.002147       NaN   \n",
       "98141        NaN  0.001989       NaN  0.001723       NaN  0.002196       NaN   \n",
       "98010        NaN  0.002083       NaN  0.001989       NaN  0.001723       NaN   \n",
       "\n",
       "        pre_z_pos_diff1  \n",
       "558384              NaN  \n",
       "558367         0.999928  \n",
       "558324         1.999923  \n",
       "558359         2.999976  \n",
       "558312         3.999920  \n",
       "...                 ...  \n",
       "98085        163.998713  \n",
       "98023        164.998323  \n",
       "98131        165.997458  \n",
       "98141        166.996255  \n",
       "98010        167.997703  \n",
       "\n",
       "[717601 rows x 61 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>StudyInstanceUID</th>\n      <th>SeriesInstanceUID</th>\n      <th>SOPInstanceUID</th>\n      <th>pe_present_on_image</th>\n      <th>negative_exam_for_pe</th>\n      <th>qa_motion</th>\n      <th>qa_contrast</th>\n      <th>flow_artifact</th>\n      <th>rv_lv_ratio_gte_1</th>\n      <th>rv_lv_ratio_lt_1</th>\n      <th>...</th>\n      <th>post15</th>\n      <th>pre16</th>\n      <th>post16</th>\n      <th>pre17</th>\n      <th>post17</th>\n      <th>pre18</th>\n      <th>post18</th>\n      <th>pre19</th>\n      <th>post19</th>\n      <th>pre_z_pos_diff1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>558384</th>\n      <td>0003b3d648eb</td>\n      <td>d2b2960c2bbf</td>\n      <td>14605bcc564c</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.000145</td>\n      <td>NaN</td>\n      <td>0.000095</td>\n      <td>NaN</td>\n      <td>0.000078</td>\n      <td>NaN</td>\n      <td>0.000102</td>\n      <td>NaN</td>\n      <td>0.000166</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>558367</th>\n      <td>0003b3d648eb</td>\n      <td>d2b2960c2bbf</td>\n      <td>d0849d3b6507</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.000095</td>\n      <td>NaN</td>\n      <td>0.000078</td>\n      <td>NaN</td>\n      <td>0.000102</td>\n      <td>NaN</td>\n      <td>0.000166</td>\n      <td>NaN</td>\n      <td>0.000127</td>\n      <td>0.999928</td>\n    </tr>\n    <tr>\n      <th>558324</th>\n      <td>0003b3d648eb</td>\n      <td>d2b2960c2bbf</td>\n      <td>18928b724b69</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.000078</td>\n      <td>NaN</td>\n      <td>0.000102</td>\n      <td>NaN</td>\n      <td>0.000166</td>\n      <td>NaN</td>\n      <td>0.000127</td>\n      <td>NaN</td>\n      <td>0.000175</td>\n      <td>1.999923</td>\n    </tr>\n    <tr>\n      <th>558359</th>\n      <td>0003b3d648eb</td>\n      <td>d2b2960c2bbf</td>\n      <td>56bc011203f4</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.000102</td>\n      <td>NaN</td>\n      <td>0.000166</td>\n      <td>NaN</td>\n      <td>0.000127</td>\n      <td>NaN</td>\n      <td>0.000175</td>\n      <td>NaN</td>\n      <td>0.000263</td>\n      <td>2.999976</td>\n    </tr>\n    <tr>\n      <th>558312</th>\n      <td>0003b3d648eb</td>\n      <td>d2b2960c2bbf</td>\n      <td>24b933c4c518</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0.000166</td>\n      <td>NaN</td>\n      <td>0.000127</td>\n      <td>NaN</td>\n      <td>0.000175</td>\n      <td>NaN</td>\n      <td>0.000263</td>\n      <td>NaN</td>\n      <td>0.000334</td>\n      <td>3.999920</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>98085</th>\n      <td>fffda3f22362</td>\n      <td>39ca5eaafffe</td>\n      <td>29e855db7f2b</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>0.002196</td>\n      <td>NaN</td>\n      <td>0.002147</td>\n      <td>NaN</td>\n      <td>0.002662</td>\n      <td>NaN</td>\n      <td>0.002301</td>\n      <td>NaN</td>\n      <td>163.998713</td>\n    </tr>\n    <tr>\n      <th>98023</th>\n      <td>fffda3f22362</td>\n      <td>39ca5eaafffe</td>\n      <td>f7ca277a66c2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>0.001723</td>\n      <td>NaN</td>\n      <td>0.002196</td>\n      <td>NaN</td>\n      <td>0.002147</td>\n      <td>NaN</td>\n      <td>0.002662</td>\n      <td>NaN</td>\n      <td>164.998323</td>\n    </tr>\n    <tr>\n      <th>98131</th>\n      <td>fffda3f22362</td>\n      <td>39ca5eaafffe</td>\n      <td>59714fd8dd25</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>0.001989</td>\n      <td>NaN</td>\n      <td>0.001723</td>\n      <td>NaN</td>\n      <td>0.002196</td>\n      <td>NaN</td>\n      <td>0.002147</td>\n      <td>NaN</td>\n      <td>165.997458</td>\n    </tr>\n    <tr>\n      <th>98141</th>\n      <td>fffda3f22362</td>\n      <td>39ca5eaafffe</td>\n      <td>b33567349fae</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>0.002083</td>\n      <td>NaN</td>\n      <td>0.001989</td>\n      <td>NaN</td>\n      <td>0.001723</td>\n      <td>NaN</td>\n      <td>0.002196</td>\n      <td>NaN</td>\n      <td>166.996255</td>\n    </tr>\n    <tr>\n      <th>98010</th>\n      <td>fffda3f22362</td>\n      <td>39ca5eaafffe</td>\n      <td>53d378d07811</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>0.002783</td>\n      <td>NaN</td>\n      <td>0.002083</td>\n      <td>NaN</td>\n      <td>0.001989</td>\n      <td>NaN</td>\n      <td>0.001723</td>\n      <td>NaN</td>\n      <td>167.997703</td>\n    </tr>\n  </tbody>\n</table>\n<p>717601 rows × 61 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 59
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['post1', 'post10', 'post11', 'post12', 'post13', 'post14', 'post15', 'post16', 'post17', 'post18', 'post19', 'post2', 'post3', 'post4', 'post5', 'post6', 'post7', 'post8', 'post9', 'pre1', 'pre10', 'pre11', 'pre12', 'pre13', 'pre14', 'pre15', 'pre16', 'pre17', 'pre18', 'pre19', 'pre2', 'pre3', 'pre4', 'pre5', 'pre6', 'pre7', 'pre8', 'pre9', 'pre_z_pos_diff1', 'pred', 'z_pos']\n"
     ]
    }
   ],
   "source": [
    "ids = [c for c in list(train) if 'UID' in c]\n",
    "targets = [\n",
    "    'negative_exam_for_pe',\n",
    "    'indeterminate',\n",
    "    'chronic_pe',\n",
    "    'acute_and_chronic_pe',\n",
    "    'central_pe',\n",
    "    'leftsided_pe',\n",
    "    'rightsided_pe',\n",
    "    'rv_lv_ratio_gte_1',\n",
    "    'rv_lv_ratio_lt_1',\n",
    "]\n",
    "other_targets = [c for c in list(train) if 'pe_present_on_image' in c]\n",
    "### remove_cols = ['fold', 'path', 'weight', 'qa_contrast', 'qa_motion'] + targets + ids + other_targets\n",
    "remove_cols = ['fold', 'path', 'weight', 'qa_contrast', 'qa_motion'] + ['exam_type','flow_artifact','pe_present_portion', 'true_filling_defect_not_pe'] + targets + ids + other_targets\n",
    "\n",
    "features = sorted(list(set(list(train)) - set(remove_cols)))\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_copyed = features.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fobj(pred, data):\n",
    "    true = data.get_label()\n",
    "    label = 2*true - 1\n",
    "    weights = data.weights\n",
    "    response = -label / (1 + np.exp(label * pred))\n",
    "    abs_response = np.abs(response)\n",
    "    grad = response\n",
    "    hess = abs_response * (1 - abs_response)\n",
    "    return grad*weights, hess*weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "bce_func = torch.nn.BCELoss(reduction='none')\n",
    "\n",
    "def feval2(preds, data):\n",
    "    scores = bce_func(torch.FloatTensor(preds), torch.FloatTensor(data.label))\n",
    "    scores = scores * torch.FloatTensor(data.weights)\n",
    "    return 'weighted logloss', torch.mean(scores), False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 推測して作成したもの. Yujiに確認する必要がある\n",
    "import torch\n",
    "bce_func_logit = torch.nn.BCEWithLogitsLoss(reduction='none')\n",
    "\n",
    "def feval(preds, data):\n",
    "    scores = bce_func_logit(torch.FloatTensor(preds), torch.FloatTensor(data.label))\n",
    "    scores = scores * torch.FloatTensor(data.weights)\n",
    "    return 'weighted logloss', torch.mean(scores), False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['pred']\n",
    "features = features_copyed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ata to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n",
      "[LightGBM] [Debug] Re-bagging, using 270700 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n",
      "[LightGBM] [Debug] Re-bagging, using 270502 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n",
      "[LightGBM] [Debug] Re-bagging, using 270551 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n",
      "[LightGBM] [Debug] Re-bagging, using 270633 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n",
      "[LightGBM] [Debug] Re-bagging, using 270652 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 10\n",
      "[LightGBM] [Debug] Re-bagging, using 270746 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n",
      "[LightGBM] [Debug] Re-bagging, using 270784 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 11\n",
      "[LightGBM] [Debug] Re-bagging, using 270940 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n",
      "[LightGBM] [Debug] Re-bagging, using 270372 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n",
      "[LightGBM] [Debug] Re-bagging, using 270240 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n",
      "[LightGBM] [Debug] Re-bagging, using 270318 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n",
      "[LightGBM] [Debug] Re-bagging, using 270534 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n",
      "[LightGBM] [Debug] Re-bagging, using 270652 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 10\n",
      "[LightGBM] [Debug] Re-bagging, using 270697 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n",
      "[LightGBM] [Debug] Re-bagging, using 270045 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 10\n",
      "[LightGBM] [Debug] Re-bagging, using 270565 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n",
      "[LightGBM] [Debug] Re-bagging, using 270547 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n",
      "[LightGBM] [Debug] Re-bagging, using 270667 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n",
      "[LightGBM] [Debug] Re-bagging, using 270677 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n",
      "[LightGBM] [Debug] Re-bagging, using 270888 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 13\n",
      "[LightGBM] [Debug] Re-bagging, using 270638 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n",
      "[LightGBM] [Debug] Re-bagging, using 270773 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 11\n",
      "[LightGBM] [Debug] Re-bagging, using 270663 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 13\n",
      "[LightGBM] [Debug] Re-bagging, using 270259 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n",
      "[LightGBM] [Debug] Re-bagging, using 270672 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n",
      "[LightGBM] [Debug] Re-bagging, using 270313 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n",
      "[LightGBM] [Debug] Re-bagging, using 270635 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 10\n",
      "[LightGBM] [Debug] Re-bagging, using 270730 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n",
      "[LightGBM] [Debug] Re-bagging, using 270367 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 12\n",
      "[LightGBM] [Debug] Re-bagging, using 270571 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 10\n",
      "[LightGBM] [Debug] Re-bagging, using 270535 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 11\n",
      "[LightGBM] [Debug] Re-bagging, using 270143 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 12\n",
      "[LightGBM] [Debug] Re-bagging, using 270749 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n",
      "[LightGBM] [Debug] Re-bagging, using 270573 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 12\n",
      "[LightGBM] [Debug] Re-bagging, using 270652 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 10\n",
      "[LightGBM] [Debug] Re-bagging, using 270364 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 13\n",
      "[LightGBM] [Debug] Re-bagging, using 270875 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n",
      "[LightGBM] [Debug] Re-bagging, using 270340 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 11\n",
      "[LightGBM] [Debug] Re-bagging, using 270643 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 13\n",
      "[LightGBM] [Debug] Re-bagging, using 270350 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n",
      "[LightGBM] [Debug] Re-bagging, using 270710 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 10\n",
      "[LightGBM] [Debug] Re-bagging, using 270228 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n",
      "[LightGBM] [Debug] Re-bagging, using 270528 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n",
      "[LightGBM] [Debug] Re-bagging, using 270607 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n",
      "[LightGBM] [Debug] Re-bagging, using 271135 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 11\n",
      "[LightGBM] [Debug] Re-bagging, using 270698 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 11\n",
      "[LightGBM] [Debug] Re-bagging, using 270485 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 12\n",
      "[LightGBM] [Debug] Re-bagging, using 270676 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 11\n",
      "[LightGBM] [Debug] Re-bagging, using 270675 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 13\n",
      "[LightGBM] [Debug] Re-bagging, using 270092 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 12\n",
      "[LightGBM] [Debug] Re-bagging, using 270775 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 13\n",
      "[LightGBM] [Debug] Re-bagging, using 270506 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 12\n",
      "[LightGBM] [Debug] Re-bagging, using 270431 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 11\n",
      "[LightGBM] [Debug] Re-bagging, using 271085 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 11\n",
      "[LightGBM] [Debug] Re-bagging, using 270604 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 13\n",
      "[LightGBM] [Debug] Re-bagging, using 270448 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n",
      "[LightGBM] [Debug] Re-bagging, using 270610 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 10\n",
      "[LightGBM] [Debug] Re-bagging, using 270512 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 14\n",
      "[LightGBM] [Debug] Re-bagging, using 270209 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 12\n",
      "[LightGBM] [Debug] Re-bagging, using 270736 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 12\n",
      "[LightGBM] [Debug] Re-bagging, using 270422 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 12\n",
      "[LightGBM] [Debug] Re-bagging, using 270731 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 11\n",
      "[LightGBM] [Debug] Re-bagging, using 270661 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 15\n",
      "[LightGBM] [Debug] Re-bagging, using 270081 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 10\n",
      "[LightGBM] [Debug] Re-bagging, using 270944 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 14\n",
      "[LightGBM] [Debug] Re-bagging, using 270274 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n",
      "[LightGBM] [Debug] Re-bagging, using 270801 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 15\n",
      "[LightGBM] [Debug] Re-bagging, using 270875 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 14\n",
      "[LightGBM] [Debug] Re-bagging, using 270866 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 14\n",
      "[LightGBM] [Debug] Re-bagging, using 270543 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n",
      "[LightGBM] [Debug] Re-bagging, using 270618 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 11\n",
      "[LightGBM] [Debug] Re-bagging, using 270627 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 12\n",
      "[LightGBM] [Debug] Re-bagging, using 270582 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 15\n",
      "[LightGBM] [Debug] Re-bagging, using 270766 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 14\n",
      "[LightGBM] [Debug] Re-bagging, using 270708 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 13\n",
      "[LightGBM] [Debug] Re-bagging, using 269845 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n",
      "[LightGBM] [Debug] Re-bagging, using 270966 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 10\n",
      "[LightGBM] [Debug] Re-bagging, using 270525 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 12\n",
      "[LightGBM] [Debug] Re-bagging, using 270423 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 10\n",
      "[LightGBM] [Debug] Re-bagging, using 270778 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n",
      "[LightGBM] [Debug] Re-bagging, using 270644 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 12\n",
      "[LightGBM] [Debug] Re-bagging, using 270120 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 14\n",
      "[LightGBM] [Debug] Re-bagging, using 270479 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 11\n",
      "[LightGBM] [Debug] Re-bagging, using 270546 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 12\n",
      "[LightGBM] [Debug] Re-bagging, using 270050 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 11\n",
      "[LightGBM] [Debug] Re-bagging, using 271083 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 10\n",
      "[LightGBM] [Debug] Re-bagging, using 270150 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 15\n",
      "[LightGBM] [Debug] Re-bagging, using 270179 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 13\n",
      "[LightGBM] [Debug] Re-bagging, using 270949 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n",
      "[LightGBM] [Debug] Re-bagging, using 270630 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 13\n",
      "[LightGBM] [Debug] Re-bagging, using 270469 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 11\n",
      "[LightGBM] [Debug] Re-bagging, using 270810 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n",
      "[LightGBM] [Debug] Re-bagging, using 270610 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 11\n",
      "[LightGBM] [Debug] Re-bagging, using 270677 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 11\n",
      "[LightGBM] [Debug] Re-bagging, using 270960 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 13\n",
      "[LightGBM] [Debug] Re-bagging, using 270772 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n",
      "[LightGBM] [Debug] Re-bagging, using 270609 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 11\n",
      "[LightGBM] [Debug] Re-bagging, using 270597 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n",
      "[LightGBM] [Debug] Re-bagging, using 270446 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 10\n",
      "[LightGBM] [Debug] Re-bagging, using 270795 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 11\n",
      "[LightGBM] [Debug] Re-bagging, using 270662 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n",
      "[LightGBM] [Debug] Re-bagging, using 271012 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 10\n",
      "[LightGBM] [Debug] Re-bagging, using 270660 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n",
      "[LightGBM] [Debug] Re-bagging, using 270791 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n",
      "[LightGBM] [Debug] Re-bagging, using 270776 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 10\n",
      "[LightGBM] [Debug] Re-bagging, using 270792 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 10\n",
      "[LightGBM] [Debug] Re-bagging, using 270202 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 10\n",
      "[LightGBM] [Debug] Re-bagging, using 270990 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 11\n",
      "[LightGBM] [Debug] Re-bagging, using 270460 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 14\n",
      "[LightGBM] [Debug] Re-bagging, using 270702 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n",
      "[LightGBM] [Debug] Re-bagging, using 270602 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 13\n",
      "[LightGBM] [Debug] Re-bagging, using 270323 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 15\n",
      "[LightGBM] [Debug] Re-bagging, using 270475 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n",
      "[LightGBM] [Debug] Re-bagging, using 270461 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n",
      "[LightGBM] [Debug] Re-bagging, using 270246 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 10\n",
      "[LightGBM] [Debug] Re-bagging, using 270880 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 13\n",
      "[LightGBM] [Debug] Re-bagging, using 270257 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 11\n",
      "[LightGBM] [Debug] Re-bagging, using 270794 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 10\n",
      "[LightGBM] [Debug] Re-bagging, using 270755 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 12\n",
      "[LightGBM] [Debug] Re-bagging, using 270849 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 10\n",
      "[LightGBM] [Debug] Re-bagging, using 270726 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n",
      "[LightGBM] [Debug] Re-bagging, using 270437 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 10\n",
      "[LightGBM] [Debug] Re-bagging, using 270548 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 13\n",
      "[LightGBM] [Debug] Re-bagging, using 271057 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 12\n",
      "[LightGBM] [Debug] Re-bagging, using 270349 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 14\n",
      "[LightGBM] [Debug] Re-bagging, using 270546 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n",
      "[LightGBM] [Debug] Re-bagging, using 270650 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n",
      "[LightGBM] [Debug] Re-bagging, using 270359 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n",
      "[LightGBM] [Debug] Re-bagging, using 270181 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 12\n",
      "[LightGBM] [Debug] Re-bagging, using 270052 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 13\n",
      "[LightGBM] [Debug] Re-bagging, using 270774 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 13\n",
      "[LightGBM] [Debug] Re-bagging, using 270635 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 13\n",
      "[LightGBM] [Debug] Re-bagging, using 270525 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 13\n",
      "[LightGBM] [Debug] Re-bagging, using 270615 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n",
      "[LightGBM] [Debug] Re-bagging, using 270586 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 10\n",
      "[LightGBM] [Debug] Re-bagging, using 270358 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 12\n",
      "[LightGBM] [Debug] Re-bagging, using 270492 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 12\n",
      "[LightGBM] [Debug] Re-bagging, using 270450 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n",
      "[LightGBM] [Debug] Re-bagging, using 270781 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 10\n",
      "[LightGBM] [Debug] Re-bagging, using 270450 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 11\n",
      "[LightGBM] [Debug] Re-bagging, using 270318 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n",
      "[LightGBM] [Debug] Re-bagging, using 270665 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n",
      "[LightGBM] [Debug] Re-bagging, using 270358 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 10\n",
      "[LightGBM] [Debug] Re-bagging, using 270668 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n",
      "[LightGBM] [Debug] Re-bagging, using 269952 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 10\n",
      "[LightGBM] [Debug] Re-bagging, using 270705 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 11\n",
      "[LightGBM] [Debug] Re-bagging, using 270872 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 15\n",
      "[LightGBM] [Debug] Re-bagging, using 270530 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 11\n",
      "[LightGBM] [Debug] Re-bagging, using 270815 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 12\n",
      "[LightGBM] [Debug] Re-bagging, using 270951 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n",
      "[LightGBM] [Debug] Re-bagging, using 270468 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n",
      "[LightGBM] [Debug] Re-bagging, using 271008 data to train\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 14\n",
      "Early stopping, best iteration is:\n",
      "[56]\ttrain's weighted logloss: 0.00981342\tval's weighted logloss: 0.0127599\n",
      "-------------------------------------------------------------------------roc_auc: 0.9597734649108359\n",
      "----------------------------------------------------------roc_auc using raw pred: 0.9511998028519109\n",
      "------------------------------------------------------------------------------AP: 0.794142616926844\n",
      "---------------------------------------------------------------AP using raw pred: 0.7517683629638445\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "import pickle\n",
    "\n",
    "oof_preds_list = []\n",
    "test_preds_list = []\n",
    "models_list = []\n",
    "target = 'pe_present_on_image'\n",
    "\n",
    "for i in range(1):\n",
    "    print(f'=================={i}================')\n",
    "    if i % 4 == 0:\n",
    "        params = {'boosting_type': 'gbdt',\n",
    "            'objective': 'binary',\n",
    "#             'metric': 'None',\n",
    "            'subsample': 0.75,\n",
    "            'subsample_freq': 1,\n",
    "            'learning_rate': 0.1,\n",
    "            'feature_fraction': 0.9,\n",
    "            'max_depth': 15,\n",
    "            'lambda_l1': 1,  \n",
    "            'lambda_l2': 1,\n",
    "            'verbose': 100,\n",
    "            'early_stopping_rounds': 100,\n",
    "            } \n",
    "    elif i % 4 == 1:\n",
    "        params = {\n",
    "            'max_depth': 4,\n",
    "            'max_leave': int(0.2 * 2 ** 4),\n",
    "            'reg_lambda': 1,\n",
    "            'reg_alpha': 1,\n",
    "            'subsamples': 0.8,\n",
    "            'colsample_bytree': 0.7,\n",
    "            'objective': 'binary',\n",
    "            'min_data_in_leaf': 0,\n",
    "            'boosting': 'gbdt',\n",
    "            'metric': 'None',\n",
    "            'learning_rate': 0.1,\n",
    "                      }\n",
    "    elif i % 4 == 2:\n",
    "        params = {\n",
    "            'num_leaves': 19, \n",
    "            'min_data_in_leaf': 160,\n",
    "            'min_child_weight': 0.03,\n",
    "            'bagging_fraction' : 0.7,\n",
    "            'feature_fraction' : 0.8,\n",
    "            'learning_rate' : 0.1,\n",
    "            'max_depth': -1,\n",
    "            'reg_alpha': 0.02,\n",
    "            'reg_lambda': 0.12,\n",
    "            'objective': 'binary',\n",
    "            'verbose': 100,\n",
    "            'boost_from_average': False,\n",
    "            'metric': 'None',\n",
    "        }  \n",
    "    else:\n",
    "        params = {\n",
    "            'objective': \"binary\",\n",
    "            'metric': 'None',\n",
    "            'boost_from_average': \"false\",\n",
    "            'tree_learner': \"serial\",\n",
    "            'max_depth': -1,\n",
    "            'learning_rate': 0.1,\n",
    "            'num_leaves': 197,\n",
    "            'feature_fraction': 0.3,\n",
    "            'bagging_freq': 1,\n",
    "            'bagging_fraction': 0.7,\n",
    "            'min_data_in_leaf': 100,\n",
    "            'bagging_seed': 11,\n",
    "            'max_bin': 255,\n",
    "            'verbosity': -1}    \n",
    "        \n",
    "    oof_preds = np.zeros(train.shape[0])\n",
    "    ### test_preds = np.zeros(test.shape[0])\n",
    "    val_results = {}\n",
    "    models = []\n",
    "    params['random_state'] = i\n",
    "    iter = 100000\n",
    "#     for n_fold, (trn_idx, val_idx) in enumerate(kf.split(train, train[target])):\n",
    "\n",
    "    ### for n_fold in range(5):\n",
    "    for n_fold in range(2):\n",
    "        tr = train.query(f'fold != {n_fold}')\n",
    "        val = train.query(f'fold == {n_fold}')\n",
    "        trn_data = lgb.Dataset(tr[features], label=tr[target])\n",
    "        trn_data.weights = tr.pe_present_portion.values\n",
    "        val_data = lgb.Dataset(val[features], label=val[target])\n",
    "        val_data.weights = val.pe_present_portion.values\n",
    "        \n",
    "        clf = lgb.train(params, trn_data, num_boost_round=iter, valid_sets=[trn_data, val_data], valid_names=['train', 'val'],\n",
    "#                         verbose_eval=200, early_stopping_rounds = 10/params['learning_rate'], evals_result=val_results,)\n",
    "                        feval=feval, fobj = fobj, verbose_eval=200, early_stopping_rounds = 10/params['learning_rate'], evals_result=val_results,)\n",
    "        file = f'lgbs/lgb_seed{i}_fold{n_fold}.pkl'\n",
    "        pickle.dump(clf, open(file, 'wb'))\n",
    "#         models.append(clf)\n",
    "        oof_preds[train.fold==n_fold] = clf.predict(val[features])\n",
    "        ### test_preds += clf.predict(test[features]) / 5\n",
    "#     models_list.append(models)\n",
    "    oof_preds_list.append(oof_preds)\n",
    "    ### test_preds_list.append(test_preds)\n",
    "\n",
    "print(f'-------------------------------------------------------------------------roc_auc: {roc_auc_score(train[target], np.mean(oof_preds_list, axis=0))}')\n",
    "print(f'----------------------------------------------------------roc_auc using raw pred: {roc_auc_score(train[target], train[\"pred\"])}')\n",
    "print(f'------------------------------------------------------------------------------AP: {average_precision_score(train[target], np.mean(oof_preds_list, axis=0))}')\n",
    "print(f'---------------------------------------------------------------AP using raw pred: {average_precision_score(train[target], train[\"pred\"])}')\n",
    "\n",
    "lgb_oof = np.mean(oof_preds_list, axis=0)\n",
    "lgb_preds = np.mean(test_preds_list, axis=0)\n",
    "train['lgb_preds'] = sigmoid(lgb_oof)\n",
    "### test['lgb_preds'] = sigmoid(lgb_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "bce_func = torch.nn.BCELoss(reduction='none')\n",
    "\n",
    "lgb_losses = bce_func(torch.FloatTensor(sigmoid(lgb_oof)), torch.FloatTensor(train['pe_present_on_image']))\n",
    "\n",
    "torch.mean(lgb_losses*train['weight'].values)"
   ]
  }
 ]
}