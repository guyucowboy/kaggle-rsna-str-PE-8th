{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.5 64-bit",
   "display_name": "Python 3.8.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/home/haito/kaggle/rsna-str/workdir\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "sys.path.append(\".\")\n",
    "from src.factory import *\n",
    "from src.utils import *\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = Path(\"../input/rsna-str-pulmonary-embolism-detection/\")\n",
    "\n",
    "train = pd.read_csv(DATADIR / \"train.csv\")\n",
    "\n",
    "pre = pd.read_csv(DATADIR / \"split.csv\")\n",
    "train = train.merge(pre, on=\"StudyInstanceUID\")\n",
    "\n",
    "portion = pd.read_csv(DATADIR / \"study_pos_portion.csv\")\n",
    "train = train.merge(portion, on=\"StudyInstanceUID\")\n",
    "\n",
    "z_pos_df = pd.read_csv(DATADIR / \"sop_to_prefix.csv\").rename(columns={'img_prefix': 'z_pos'})\n",
    "train = train.merge(z_pos_df, on=\"SOPInstanceUID\")\n",
    "\n",
    "\n",
    "### train = train.query(\"fold == 0 or fold == 1\")  # now I have fold0,1 only\n",
    "\n",
    "studies = train.StudyInstanceUID.unique()\n",
    "\n",
    "# agg = t.groupby(\"StudyInstanceUID\")[\"SOPInstanceUID\"].apply(list)\n",
    "# agg_one = t.groupby(\"StudyInstanceUID\").first()\n",
    "# t = t.set_index(\"SOPInstanceUID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred(_path):\n",
    "    res = load_pickle(_path)\n",
    "    raw_pred = pd.DataFrame({\n",
    "        \"SOPInstanceUID\": res[\"ids\"],\n",
    "        **res[\"outputs\"],\n",
    "    })\n",
    "    return raw_pred\n",
    "    # return raw_pred.set_index(\"sop\")\n",
    "\n",
    "def calib_p(arr, factor):  # set factor>1 to enhance positive prob\n",
    "    return arr * factor / (arr * factor + (1-arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_f0, fold0_calib_f = get_pred(\"output/035_pe_present___448/valid.fold0-ep1.picle\"), 8.555037588568537\n",
    "oof_f1, fold1_calib_f = get_pred(\"output/035_pe_present___448___apex___resume/valid.fold1-ep1.pickle\"), 5.72045\n",
    "oof_f2, fold2_calib_f = get_pred(\"output/035_pe_present___448___apex___resume/fold2_ep1.pt.valid.pickle\"), 3.3448360180113497\n",
    "oof_f3, fold3_calib_f = get_pred(\"output/035_pe_present___448___apex/fold3_ep1.pt.valid.pickle\"), 11.18792798738602,\n",
    "oof_f4, fold4_calib_f = get_pred(\"output/035_pe_present___448___apex___resume/fold4_ep1.pt.valid.pickle\"), 5.720451550292213\n",
    "\n",
    "# BAD\n",
    "if False:  # pick best one which yields weighted-logloss after calib\n",
    "    oof_f3, fold3_calib_f = get_pred(\"output/035_pe_present___448___apex/fold3_ep0.pt.valid.pickle\"), 6.541753595870311\n",
    "    oof_f4, fold4_calib_f = get_pred(\"output/035_pe_present___448___apex/fold4_ep0.pt.valid.pickle\"), 3.8250639579850194\n",
    "    \n",
    "\n",
    "if True: ### ==== do calib for each fold\n",
    "    oof_f0[\"pe_present_on_image\"] = calib_p(oof_f0[\"pe_present_on_image\"], fold0_calib_f)\n",
    "    oof_f1[\"pe_present_on_image\"] = calib_p(oof_f1[\"pe_present_on_image\"], fold1_calib_f)\n",
    "    oof_f2[\"pe_present_on_image\"] = calib_p(oof_f2[\"pe_present_on_image\"], fold2_calib_f)\n",
    "    oof_f3[\"pe_present_on_image\"] = calib_p(oof_f3[\"pe_present_on_image\"], fold3_calib_f)\n",
    "    oof_f4[\"pe_present_on_image\"] = calib_p(oof_f4[\"pe_present_on_image\"], fold4_calib_f)\n",
    "\n",
    "oof = pd.concat([oof_f0, oof_f1, oof_f2, oof_f3, oof_f4]).rename(columns={'pe_present_on_image': 'pred'})\n",
    "\n",
    "train = train.merge(oof[['pred', 'SOPInstanceUID']], on=\"SOPInstanceUID\")  # add pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Index(['StudyInstanceUID', 'SeriesInstanceUID', 'SOPInstanceUID',\n",
       "       'pe_present_on_image', 'negative_exam_for_pe', 'qa_motion',\n",
       "       'qa_contrast', 'flow_artifact', 'rv_lv_ratio_gte_1', 'rv_lv_ratio_lt_1',\n",
       "       'leftsided_pe', 'chronic_pe', 'true_filling_defect_not_pe',\n",
       "       'rightsided_pe', 'acute_and_chronic_pe', 'central_pe', 'indeterminate',\n",
       "       'exam_type', 'fold', 'pe_present_portion', 'z_pos', 'pred'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "train_copyed = train.copy()\n",
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "' feature engineer '"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "\"\"\" feature engineer \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = pd.read_pickle('../input/train_with_position.pkl')\n",
    "train = train.sort_values(['StudyInstanceUID', 'z_pos'])\n",
    "# train = train.merge(oof[['pred', 'SOPInstanceUID']], on='SOPInstanceUID')\n",
    "\n",
    "# test = pd.read_pickle('../input/test_with_position.pkl')\n",
    "# test = test.sort_values(['StudyInstanceUID', 'z_pos'])\n",
    "# test = test.merge(test_pred[pe_present_portion], on='SOPInstanceUID')\n",
    "\n",
    "# train = train.merge(oof_non_weight[['pred_non_weight', 'SOPInstanceUID']], on='SOPInstanceUID')\n",
    "# test = test.merge(test_pred_non_weight[['pred_non_weight', 'SOPInstanceUID']], on='SOPInstanceUID')\n",
    "# train['pred_mean'] = (train['pred'] + train['pred_non_weight']) / 2\n",
    "# test['pred_mean'] = (test['pred'] + test['pred_non_weight']) / 2\n",
    "\n",
    "train_current_z_pos = train.groupby('StudyInstanceUID')['z_pos'].shift(0)\n",
    "# test_current_z_pos = test.groupby('StudyInstanceUID')['z_pos'].shift(0)\n",
    "### for i in range(1, 20):\n",
    "for i in range(1, 10):\n",
    "\n",
    "    # train[f'pre_mean{i}'] = train.groupby('StudyInstanceUID')['pred_mean'].shift(i)\n",
    "    # train[f'post_mean{i}'] = train.groupby('StudyInstanceUID')['pred_mean'].shift(-i)\n",
    "    train[f'pre{i}'] = train.groupby('StudyInstanceUID')['pred'].shift(i)\n",
    "    train[f'post{i}'] = train.groupby('StudyInstanceUID')['pred'].shift(-i)\n",
    "    # train[f'pre_non_weight{i}'] = train.groupby('StudyInstanceUID')['pred_non_weight'].shift(i)\n",
    "    # train[f'post_non_weight{i}'] = train.groupby('StudyInstanceUID')['pred_non_weight'].shift(-i)\n",
    "    \n",
    "    # test[f'pre_mean{i}'] = test.groupby('StudyInstanceUID')['pred_mean'].shift(i)\n",
    "    # test[f'post_mean{i}'] = test.groupby('StudyInstanceUID')['pred_mean'].shift(-i)\n",
    "    # test[f'pre{i}'] = test.groupby('StudyInstanceUID')['pred'].shift(i)\n",
    "    # test[f'post{i}'] = test.groupby('StudyInstanceUID')['pred'].shift(-i)\n",
    "    # test[f'pre_non_weight{i}'] = test.groupby('StudyInstanceUID')['pred_non_weight'].shift(i)\n",
    "    # test[f'post_non_weight{i}'] = test.groupby('StudyInstanceUID')['pred_non_weight'].shift(-i)\n",
    "\n",
    "\n",
    "# for i in [1]:\n",
    "#     train[f'pre_z_pos_diff{i}'] = train_current_z_pos - train.groupby('StudyInstanceUID')['pred'].shift(i)\n",
    "#     # test[f'pre_z_pos_diff{i}'] = test_current_z_pos - test.groupby('StudyInstanceUID')['pred'].shift(i)\n",
    "\n",
    "# NORMALIZED Z POS\n",
    "z_max = train.groupby('StudyInstanceUID').z_pos.max().rename('z_pos_max')\n",
    "train = train.merge(z_max, on='StudyInstanceUID')\n",
    "train['z_pos_norm'] = train['z_pos'] / train['z_pos_max']\n",
    "train = train.drop('z_pos_max', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "        StudyInstanceUID SeriesInstanceUID SOPInstanceUID  \\\n",
       "1790589     fffda3f22362      39ca5eaafffe   29e855db7f2b   \n",
       "1790590     fffda3f22362      39ca5eaafffe   f7ca277a66c2   \n",
       "1790591     fffda3f22362      39ca5eaafffe   59714fd8dd25   \n",
       "1790592     fffda3f22362      39ca5eaafffe   b33567349fae   \n",
       "1790593     fffda3f22362      39ca5eaafffe   53d378d07811   \n",
       "\n",
       "         pe_present_on_image  negative_exam_for_pe  qa_motion  qa_contrast  \\\n",
       "1790589                    0                     1          0            0   \n",
       "1790590                    0                     1          0            0   \n",
       "1790591                    0                     1          0            0   \n",
       "1790592                    0                     1          0            0   \n",
       "1790593                    0                     1          0            0   \n",
       "\n",
       "         flow_artifact  rv_lv_ratio_gte_1  rv_lv_ratio_lt_1  ...  post5  \\\n",
       "1790589              0                  0                 0  ...    NaN   \n",
       "1790590              0                  0                 0  ...    NaN   \n",
       "1790591              0                  0                 0  ...    NaN   \n",
       "1790592              0                  0                 0  ...    NaN   \n",
       "1790593              0                  0                 0  ...    NaN   \n",
       "\n",
       "             pre6  post6      pre7  post7      pre8  post8      pre9  post9  \\\n",
       "1790589  0.001988    NaN  0.002619    NaN  0.003691    NaN  0.002995    NaN   \n",
       "1790590  0.001816    NaN  0.001988    NaN  0.002619    NaN  0.003691    NaN   \n",
       "1790591  0.001840    NaN  0.001816    NaN  0.001988    NaN  0.002619    NaN   \n",
       "1790592  0.001995    NaN  0.001840    NaN  0.001816    NaN  0.001988    NaN   \n",
       "1790593  0.001884    NaN  0.001995    NaN  0.001840    NaN  0.001816    NaN   \n",
       "\n",
       "         z_pos_norm  \n",
       "1790589    0.976190  \n",
       "1790590    0.982143  \n",
       "1790591    0.988095  \n",
       "1790592    0.994048  \n",
       "1790593    1.000000  \n",
       "\n",
       "[5 rows x 41 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>StudyInstanceUID</th>\n      <th>SeriesInstanceUID</th>\n      <th>SOPInstanceUID</th>\n      <th>pe_present_on_image</th>\n      <th>negative_exam_for_pe</th>\n      <th>qa_motion</th>\n      <th>qa_contrast</th>\n      <th>flow_artifact</th>\n      <th>rv_lv_ratio_gte_1</th>\n      <th>rv_lv_ratio_lt_1</th>\n      <th>...</th>\n      <th>post5</th>\n      <th>pre6</th>\n      <th>post6</th>\n      <th>pre7</th>\n      <th>post7</th>\n      <th>pre8</th>\n      <th>post8</th>\n      <th>pre9</th>\n      <th>post9</th>\n      <th>z_pos_norm</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1790589</th>\n      <td>fffda3f22362</td>\n      <td>39ca5eaafffe</td>\n      <td>29e855db7f2b</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>0.001988</td>\n      <td>NaN</td>\n      <td>0.002619</td>\n      <td>NaN</td>\n      <td>0.003691</td>\n      <td>NaN</td>\n      <td>0.002995</td>\n      <td>NaN</td>\n      <td>0.976190</td>\n    </tr>\n    <tr>\n      <th>1790590</th>\n      <td>fffda3f22362</td>\n      <td>39ca5eaafffe</td>\n      <td>f7ca277a66c2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>0.001816</td>\n      <td>NaN</td>\n      <td>0.001988</td>\n      <td>NaN</td>\n      <td>0.002619</td>\n      <td>NaN</td>\n      <td>0.003691</td>\n      <td>NaN</td>\n      <td>0.982143</td>\n    </tr>\n    <tr>\n      <th>1790591</th>\n      <td>fffda3f22362</td>\n      <td>39ca5eaafffe</td>\n      <td>59714fd8dd25</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>0.001840</td>\n      <td>NaN</td>\n      <td>0.001816</td>\n      <td>NaN</td>\n      <td>0.001988</td>\n      <td>NaN</td>\n      <td>0.002619</td>\n      <td>NaN</td>\n      <td>0.988095</td>\n    </tr>\n    <tr>\n      <th>1790592</th>\n      <td>fffda3f22362</td>\n      <td>39ca5eaafffe</td>\n      <td>b33567349fae</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>0.001995</td>\n      <td>NaN</td>\n      <td>0.001840</td>\n      <td>NaN</td>\n      <td>0.001816</td>\n      <td>NaN</td>\n      <td>0.001988</td>\n      <td>NaN</td>\n      <td>0.994048</td>\n    </tr>\n    <tr>\n      <th>1790593</th>\n      <td>fffda3f22362</td>\n      <td>39ca5eaafffe</td>\n      <td>53d378d07811</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>0.001884</td>\n      <td>NaN</td>\n      <td>0.001995</td>\n      <td>NaN</td>\n      <td>0.001840</td>\n      <td>NaN</td>\n      <td>0.001816</td>\n      <td>NaN</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 41 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['post1', 'post2', 'post3', 'post4', 'post5', 'post6', 'post7', 'post8', 'post9', 'pre1', 'pre2', 'pre3', 'pre4', 'pre5', 'pre6', 'pre7', 'pre8', 'pre9', 'pred', 'z_pos_norm']\n"
     ]
    }
   ],
   "source": [
    "ids = [c for c in list(train) if 'UID' in c]\n",
    "targets = [\n",
    "    'negative_exam_for_pe',\n",
    "    'indeterminate',\n",
    "    'chronic_pe',\n",
    "    'acute_and_chronic_pe',\n",
    "    'central_pe',\n",
    "    'leftsided_pe',\n",
    "    'rightsided_pe',\n",
    "    'rv_lv_ratio_gte_1',\n",
    "    'rv_lv_ratio_lt_1',\n",
    "]\n",
    "other_targets = [c for c in list(train) if 'pe_present_on_image' in c]\n",
    "### remove_cols = ['fold', 'path', 'weight', 'qa_contrast', 'qa_motion'] + targets + ids + other_targets\n",
    "### remove_cols = ['fold', 'path', 'weight', 'qa_contrast', 'qa_motion'] + ['exam_type','flow_artifact','pe_present_portion', 'true_filling_defect_not_pe'] + targets + ids + other_targets\n",
    "remove_cols = ['fold', 'path', 'weight', 'qa_contrast', 'qa_motion'] + ['exam_type','flow_artifact','pe_present_portion', 'true_filling_defect_not_pe'] + targets + ids + other_targets + ['z_pos']\n",
    "\n",
    "features = sorted(list(set(list(train)) - set(remove_cols)))\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_copyed = features.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fobj(pred, data):\n",
    "    true = data.get_label()\n",
    "    label = 2*true - 1\n",
    "    weights = data.weights\n",
    "    response = -label / (1 + np.exp(label * pred))\n",
    "    abs_response = np.abs(response)\n",
    "    grad = response\n",
    "    hess = abs_response * (1 - abs_response)\n",
    "    return grad*weights, hess*weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "bce_func = torch.nn.BCELoss(reduction='none')\n",
    "\n",
    "def feval2(preds, data):\n",
    "    scores = bce_func(torch.FloatTensor(preds), torch.FloatTensor(data.label))\n",
    "    scores = scores * torch.FloatTensor(data.weights)\n",
    "    return 'weighted logloss', torch.mean(scores), False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 推測して作成したもの. Yujiに確認する必要がある\n",
    "import torch\n",
    "bce_func_logit = torch.nn.BCEWithLogitsLoss(reduction='none')\n",
    "\n",
    "def feval(preds, data):\n",
    "    scores = bce_func_logit(torch.FloatTensor(preds), torch.FloatTensor(data.label))\n",
    "    scores = scores * torch.FloatTensor(data.weights)\n",
    "    return 'weighted logloss', torch.mean(scores), False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['pred']\n",
    "features = features_copyed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "==================0================\n",
      "    ==============fold0================\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[84]\ttrain's weighted logloss: 0.01086\tval's weighted logloss: 0.0115093\n",
      "    ==============fold1================\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[102]\ttrain's weighted logloss: 0.0105961\tval's weighted logloss: 0.0121317\n",
      "    ==============fold2================\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[63]\ttrain's weighted logloss: 0.0111349\tval's weighted logloss: 0.0108246\n",
      "    ==============fold3================\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[135]\ttrain's weighted logloss: 0.0106479\tval's weighted logloss: 0.0113474\n",
      "    ==============fold4================\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[78]\ttrain's weighted logloss: 0.01093\tval's weighted logloss: 0.0113441\n",
      "-------------------------------------------------------------------------roc_auc: 0.9617108423624163\n",
      "----------------------------------------------------------roc_auc using raw pred: 0.9485047588286195\n",
      "------------------------------------------------------------------------------AP: 0.7806959752948743\n",
      "---------------------------------------------------------------AP using raw pred: 0.7264076541068915\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "import pickle\n",
    "\n",
    "oof_preds_list = []\n",
    "test_preds_list = []\n",
    "models_list = []\n",
    "target = 'pe_present_on_image'\n",
    "\n",
    "for i in range(1):\n",
    "    print(f'=================={i}================')\n",
    "    if i % 4 == 0:\n",
    "        params = {'boosting_type': 'gbdt',\n",
    "            'objective': 'binary',\n",
    "#             'metric': 'None',\n",
    "            'subsample': 0.75,\n",
    "            'subsample_freq': 1,\n",
    "            'learning_rate': 0.1,\n",
    "            'feature_fraction': 0.9,\n",
    "            'max_depth': 15,\n",
    "            'lambda_l1': 1,  \n",
    "            'lambda_l2': 1,\n",
    "            'verbose': 100,\n",
    "            'early_stopping_rounds': 100,\n",
    "            'verbose': -1,\n",
    "            } \n",
    "    elif i % 4 == 1:\n",
    "        params = {\n",
    "            'max_depth': 4,\n",
    "            'max_leave': int(0.2 * 2 ** 4),\n",
    "            'reg_lambda': 1,\n",
    "            'reg_alpha': 1,\n",
    "            'subsamples': 0.8,\n",
    "            'colsample_bytree': 0.7,\n",
    "            'objective': 'binary',\n",
    "            'min_data_in_leaf': 0,\n",
    "            'boosting': 'gbdt',\n",
    "            'metric': 'None',\n",
    "            'learning_rate': 0.1,\n",
    "                      }\n",
    "    elif i % 4 == 2:\n",
    "        params = {\n",
    "            'num_leaves': 19, \n",
    "            'min_data_in_leaf': 160,\n",
    "            'min_child_weight': 0.03,\n",
    "            'bagging_fraction' : 0.7,\n",
    "            'feature_fraction' : 0.8,\n",
    "            'learning_rate' : 0.1,\n",
    "            'max_depth': -1,\n",
    "            'reg_alpha': 0.02,\n",
    "            'reg_lambda': 0.12,\n",
    "            'objective': 'binary',\n",
    "            'verbose': 100,\n",
    "            'boost_from_average': False,\n",
    "            'metric': 'None',\n",
    "        }  \n",
    "    else:\n",
    "        params = {\n",
    "            'objective': \"binary\",\n",
    "            'metric': 'None',\n",
    "            'boost_from_average': \"false\",\n",
    "            'tree_learner': \"serial\",\n",
    "            'max_depth': -1,\n",
    "            'learning_rate': 0.1,\n",
    "            'num_leaves': 197,\n",
    "            'feature_fraction': 0.3,\n",
    "            'bagging_freq': 1,\n",
    "            'bagging_fraction': 0.7,\n",
    "            'min_data_in_leaf': 100,\n",
    "            'bagging_seed': 11,\n",
    "            'max_bin': 255,\n",
    "            'verbosity': -1}    \n",
    "        \n",
    "    oof_preds = np.zeros(train.shape[0])\n",
    "    ### test_preds = np.zeros(test.shape[0])\n",
    "    val_results = {}\n",
    "    models = []\n",
    "    params['random_state'] = i\n",
    "    iter = 100000\n",
    "#     for n_fold, (trn_idx, val_idx) in enumerate(kf.split(train, train[target])):\n",
    "\n",
    "    for n_fold in range(5):\n",
    "    ### for n_fold in range(2):\n",
    "        print(f'    ==============fold{n_fold}================')\n",
    "        tr = train.query(f'fold != {n_fold}')\n",
    "        val = train.query(f'fold == {n_fold}')\n",
    "        trn_data = lgb.Dataset(tr[features], label=tr[target])\n",
    "        trn_data.weights = tr.pe_present_portion.values\n",
    "        val_data = lgb.Dataset(val[features], label=val[target])\n",
    "        val_data.weights = val.pe_present_portion.values\n",
    "        \n",
    "        clf = lgb.train(params, trn_data, num_boost_round=iter, valid_sets=[trn_data, val_data], valid_names=['train', 'val'],\n",
    "#                         verbose_eval=200, early_stopping_rounds = 10/params['learning_rate'], evals_result=val_results,)\n",
    "                        feval=feval, fobj = fobj, verbose_eval=2000, early_stopping_rounds = 10/params['learning_rate'], evals_result=val_results, )\n",
    "        file = f'lgbs/lgb_seed{i}_fold{n_fold}.pkl'\n",
    "        pickle.dump(clf, open(file, 'wb'))\n",
    "#         models.append(clf)\n",
    "        oof_preds[train.fold==n_fold] = clf.predict(val[features])\n",
    "        ### test_preds += clf.predict(test[features]) / 5\n",
    "#     models_list.append(models)\n",
    "    oof_preds_list.append(oof_preds)\n",
    "    ### test_preds_list.append(test_preds)\n",
    "\n",
    "print(f'-------------------------------------------------------------------------roc_auc: {roc_auc_score(train[target], np.mean(oof_preds_list, axis=0))}')\n",
    "print(f'----------------------------------------------------------roc_auc using raw pred: {roc_auc_score(train[target], train[\"pred\"])}')\n",
    "print(f'------------------------------------------------------------------------------AP: {average_precision_score(train[target], np.mean(oof_preds_list, axis=0))}')\n",
    "print(f'---------------------------------------------------------------AP using raw pred: {average_precision_score(train[target], train[\"pred\"])}')\n",
    "\n",
    "lgb_oof = np.mean(oof_preds_list, axis=0)\n",
    "lgb_preds = np.mean(test_preds_list, axis=0)\n",
    "train['lgb_preds'] = sigmoid(lgb_oof)\n",
    "### test['lgb_preds'] = sigmoid(lgb_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.011431804800068808"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "bce_func = torch.nn.BCELoss(reduction='none')\n",
    "\n",
    "lgb_losses = bce_func(torch.FloatTensor(sigmoid(lgb_oof)), torch.FloatTensor(train['pe_present_on_image']))\n",
    "\n",
    "### torch.mean(lgb_losses*train['weight'].values)\n",
    "torch.mean(lgb_losses*train['pe_present_portion'].values).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.013218380246001216"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "# no stacking result\n",
    "lgb_losses = bce_func(torch.FloatTensor(train['pred']), torch.FloatTensor(train['pe_present_on_image']))\n",
    "torch.mean(lgb_losses*train['pe_present_portion'].values).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PE_REPESNT -> POS_EXAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = Path(\"../input/rsna-str-pulmonary-embolism-detection/\")\n",
    "\n",
    "train = pd.read_csv(DATADIR / \"train.csv\")\n",
    "\n",
    "pre = pd.read_csv(DATADIR / \"split.csv\")\n",
    "train = train.merge(pre, on=\"StudyInstanceUID\")\n",
    "\n",
    "portion = pd.read_csv(DATADIR / \"study_pos_portion.csv\")\n",
    "train = train.merge(portion, on=\"StudyInstanceUID\")\n",
    "\n",
    "z_pos_df = pd.read_csv(DATADIR / \"sop_to_prefix.csv\").rename(columns={'img_prefix': 'z_pos'})\n",
    "train = train.merge(z_pos_df, on=\"SOPInstanceUID\")\n",
    "\n",
    "studies = train.StudyInstanceUID.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof = pd.concat([oof_f0, oof_f1, oof_f2, oof_f3, oof_f4]).rename(columns={'pe_present_on_image': 'pred'})\n",
    "\n",
    "train = train.merge(oof[['pred', 'SOPInstanceUID']], on=\"SOPInstanceUID\")  # add pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grouping(df):\n",
    "    grouped = pd.DataFrame(df.groupby('StudyInstanceUID')['pred'].mean())\n",
    "    grouped = grouped.rename(columns={'pred': 'mean'})\n",
    "    count = df.groupby('StudyInstanceUID')['pred'].count()\n",
    "    grouped['count_total'] = count\n",
    "\n",
    "    for i in range(1,10):\n",
    "        count = df[df.pred>i/10].groupby('StudyInstanceUID')['pred'].count()\n",
    "        grouped[f'count_over{i/10}'] = count\n",
    "        grouped[f'count_over{i/10}_ratio'] = count / grouped['count_total']\n",
    "    count = df[df.pred==1].groupby('StudyInstanceUID')['pred'].count()\n",
    "    grouped['count_1'] = count\n",
    "    grouped['count_1_ratio'] = count / grouped['count_total']\n",
    "\n",
    "    ma = pd.DataFrame(df.groupby('StudyInstanceUID')['pred'].max())\n",
    "    grouped['max'] = ma.pred\n",
    "\n",
    "    grouped = grouped.reset_index().fillna(0)\n",
    "    return grouped\n",
    "train_grouped = grouping(train)\n",
    "\n",
    "train_grouped['fold'] = train.groupby('StudyInstanceUID')['fold'].first().values\n",
    "train_grouped['negative_exam_for_pe'] = train.groupby('StudyInstanceUID')['negative_exam_for_pe'].first().values\n",
    "train_grouped['positive_exam_for_pe'] = (1 - train.groupby('StudyInstanceUID')['negative_exam_for_pe'].first().values) * (1 - train.groupby('StudyInstanceUID')['indeterminate'].first().values)\n",
    "\n",
    "# test_grouped = grouping(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "     StudyInstanceUID      mean  count_total  count_over0.1  \\\n",
       "200      06d146ecc3a7  0.047192          193           29.0   \n",
       "201      06d836b00752  0.146712          239           72.0   \n",
       "202      06e70cd10c06  0.066696          188           41.0   \n",
       "203      06ec73a6abf3  0.072169          219           51.0   \n",
       "204      06f03ad6e7ea  0.175588          239           87.0   \n",
       "...               ...       ...          ...            ...   \n",
       "7274     ffe5280cd21d  0.319430          240          107.0   \n",
       "7275     ffe7efaccbb9  0.060635          250           35.0   \n",
       "7276     fff1ef450040  0.055501          272           44.0   \n",
       "7277     fff9536823b1  0.142846          388          119.0   \n",
       "7278     fffda3f22362  0.079547          169           43.0   \n",
       "\n",
       "      count_over0.1_ratio  count_over0.2  count_over0.2_ratio  count_over0.3  \\\n",
       "200              0.150259           10.0             0.051813            2.0   \n",
       "201              0.301255           41.0             0.171548           26.0   \n",
       "202              0.218085           23.0             0.122340           10.0   \n",
       "203              0.232877           21.0             0.095890           14.0   \n",
       "204              0.364017           60.0             0.251046           42.0   \n",
       "...                   ...            ...                  ...            ...   \n",
       "7274             0.445833           97.0             0.404167           91.0   \n",
       "7275             0.140000           19.0             0.076000           14.0   \n",
       "7276             0.161765           12.0             0.044118            7.0   \n",
       "7277             0.306701           79.0             0.203608           58.0   \n",
       "7278             0.254438           14.0             0.082840            8.0   \n",
       "\n",
       "      count_over0.3_ratio  count_over0.4  ...  count_over0.8  \\\n",
       "200              0.010363            0.0  ...            0.0   \n",
       "201              0.108787           25.0  ...           20.0   \n",
       "202              0.053191            5.0  ...            0.0   \n",
       "203              0.063927            8.0  ...            1.0   \n",
       "204              0.175732           38.0  ...           18.0   \n",
       "...                   ...            ...  ...            ...   \n",
       "7274             0.379167           82.0  ...           54.0   \n",
       "7275             0.056000            8.0  ...            4.0   \n",
       "7276             0.025735            2.0  ...            0.0   \n",
       "7277             0.149485           47.0  ...           20.0   \n",
       "7278             0.047337            3.0  ...            1.0   \n",
       "\n",
       "      count_over0.8_ratio  count_over0.9  count_over0.9_ratio  count_1  \\\n",
       "200              0.000000            0.0             0.000000      0.0   \n",
       "201              0.083682           18.0             0.075314      0.0   \n",
       "202              0.000000            0.0             0.000000      0.0   \n",
       "203              0.004566            1.0             0.004566      0.0   \n",
       "204              0.075314           16.0             0.066946      0.0   \n",
       "...                   ...            ...                  ...      ...   \n",
       "7274             0.225000           43.0             0.179167      0.0   \n",
       "7275             0.016000            2.0             0.008000      0.0   \n",
       "7276             0.000000            0.0             0.000000      0.0   \n",
       "7277             0.051546           16.0             0.041237      0.0   \n",
       "7278             0.005917            0.0             0.000000      0.0   \n",
       "\n",
       "      count_1_ratio       max  fold  negative_exam_for_pe  \\\n",
       "200             0.0  0.399029     2                     1   \n",
       "201             0.0  0.990752     3                     0   \n",
       "202             0.0  0.615208     3                     1   \n",
       "203             0.0  0.923399     2                     1   \n",
       "204             0.0  0.998453     3                     0   \n",
       "...             ...       ...   ...                   ...   \n",
       "7274            0.0  0.995656     4                     0   \n",
       "7275            0.0  0.970864     3                     1   \n",
       "7276            0.0  0.484924     3                     1   \n",
       "7277            0.0  0.987423     0                     0   \n",
       "7278            0.0  0.819511     0                     1   \n",
       "\n",
       "      positive_exam_for_pe  \n",
       "200                      0  \n",
       "201                      1  \n",
       "202                      0  \n",
       "203                      0  \n",
       "204                      1  \n",
       "...                    ...  \n",
       "7274                     1  \n",
       "7275                     0  \n",
       "7276                     0  \n",
       "7277                     1  \n",
       "7278                     0  \n",
       "\n",
       "[7079 rows x 27 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>StudyInstanceUID</th>\n      <th>mean</th>\n      <th>count_total</th>\n      <th>count_over0.1</th>\n      <th>count_over0.1_ratio</th>\n      <th>count_over0.2</th>\n      <th>count_over0.2_ratio</th>\n      <th>count_over0.3</th>\n      <th>count_over0.3_ratio</th>\n      <th>count_over0.4</th>\n      <th>...</th>\n      <th>count_over0.8</th>\n      <th>count_over0.8_ratio</th>\n      <th>count_over0.9</th>\n      <th>count_over0.9_ratio</th>\n      <th>count_1</th>\n      <th>count_1_ratio</th>\n      <th>max</th>\n      <th>fold</th>\n      <th>negative_exam_for_pe</th>\n      <th>positive_exam_for_pe</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>200</th>\n      <td>06d146ecc3a7</td>\n      <td>0.047192</td>\n      <td>193</td>\n      <td>29.0</td>\n      <td>0.150259</td>\n      <td>10.0</td>\n      <td>0.051813</td>\n      <td>2.0</td>\n      <td>0.010363</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.399029</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>201</th>\n      <td>06d836b00752</td>\n      <td>0.146712</td>\n      <td>239</td>\n      <td>72.0</td>\n      <td>0.301255</td>\n      <td>41.0</td>\n      <td>0.171548</td>\n      <td>26.0</td>\n      <td>0.108787</td>\n      <td>25.0</td>\n      <td>...</td>\n      <td>20.0</td>\n      <td>0.083682</td>\n      <td>18.0</td>\n      <td>0.075314</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.990752</td>\n      <td>3</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>202</th>\n      <td>06e70cd10c06</td>\n      <td>0.066696</td>\n      <td>188</td>\n      <td>41.0</td>\n      <td>0.218085</td>\n      <td>23.0</td>\n      <td>0.122340</td>\n      <td>10.0</td>\n      <td>0.053191</td>\n      <td>5.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.615208</td>\n      <td>3</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>203</th>\n      <td>06ec73a6abf3</td>\n      <td>0.072169</td>\n      <td>219</td>\n      <td>51.0</td>\n      <td>0.232877</td>\n      <td>21.0</td>\n      <td>0.095890</td>\n      <td>14.0</td>\n      <td>0.063927</td>\n      <td>8.0</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>0.004566</td>\n      <td>1.0</td>\n      <td>0.004566</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.923399</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>204</th>\n      <td>06f03ad6e7ea</td>\n      <td>0.175588</td>\n      <td>239</td>\n      <td>87.0</td>\n      <td>0.364017</td>\n      <td>60.0</td>\n      <td>0.251046</td>\n      <td>42.0</td>\n      <td>0.175732</td>\n      <td>38.0</td>\n      <td>...</td>\n      <td>18.0</td>\n      <td>0.075314</td>\n      <td>16.0</td>\n      <td>0.066946</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.998453</td>\n      <td>3</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>7274</th>\n      <td>ffe5280cd21d</td>\n      <td>0.319430</td>\n      <td>240</td>\n      <td>107.0</td>\n      <td>0.445833</td>\n      <td>97.0</td>\n      <td>0.404167</td>\n      <td>91.0</td>\n      <td>0.379167</td>\n      <td>82.0</td>\n      <td>...</td>\n      <td>54.0</td>\n      <td>0.225000</td>\n      <td>43.0</td>\n      <td>0.179167</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.995656</td>\n      <td>4</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7275</th>\n      <td>ffe7efaccbb9</td>\n      <td>0.060635</td>\n      <td>250</td>\n      <td>35.0</td>\n      <td>0.140000</td>\n      <td>19.0</td>\n      <td>0.076000</td>\n      <td>14.0</td>\n      <td>0.056000</td>\n      <td>8.0</td>\n      <td>...</td>\n      <td>4.0</td>\n      <td>0.016000</td>\n      <td>2.0</td>\n      <td>0.008000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.970864</td>\n      <td>3</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7276</th>\n      <td>fff1ef450040</td>\n      <td>0.055501</td>\n      <td>272</td>\n      <td>44.0</td>\n      <td>0.161765</td>\n      <td>12.0</td>\n      <td>0.044118</td>\n      <td>7.0</td>\n      <td>0.025735</td>\n      <td>2.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.484924</td>\n      <td>3</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7277</th>\n      <td>fff9536823b1</td>\n      <td>0.142846</td>\n      <td>388</td>\n      <td>119.0</td>\n      <td>0.306701</td>\n      <td>79.0</td>\n      <td>0.203608</td>\n      <td>58.0</td>\n      <td>0.149485</td>\n      <td>47.0</td>\n      <td>...</td>\n      <td>20.0</td>\n      <td>0.051546</td>\n      <td>16.0</td>\n      <td>0.041237</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.987423</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7278</th>\n      <td>fffda3f22362</td>\n      <td>0.079547</td>\n      <td>169</td>\n      <td>43.0</td>\n      <td>0.254438</td>\n      <td>14.0</td>\n      <td>0.082840</td>\n      <td>8.0</td>\n      <td>0.047337</td>\n      <td>3.0</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>0.005917</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.819511</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>7079 rows × 27 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "train_grouped[200:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features = list(set(list(train_grouped)) - set(['StudyInstanceUID', 'negative_exam_for_pe']))\n",
    "# target = 'negative_exam_for_pe'\n",
    "\n",
    "features = list(set(list(train_grouped)) - set(['StudyInstanceUID', 'positive_exam_for_pe', 'negative_exam_for_pe', 'fold']))\n",
    "\n",
    "# target = 'positive_exam_for_pe'\n",
    "target = 'negative_exam_for_pe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['count_over0.8', 'count_1', 'count_over0.1', 'count_total', 'count_over0.3', 'count_over0.2', 'count_over0.6_ratio', 'count_over0.5', 'count_over0.9', 'max', 'count_over0.7', 'count_over0.4', 'count_over0.1_ratio', 'count_over0.4_ratio', 'count_over0.3_ratio', 'count_over0.8_ratio', 'count_over0.5_ratio', 'count_over0.9_ratio', 'count_over0.6', 'count_over0.2_ratio', 'mean', 'count_1_ratio', 'count_over0.7_ratio']\n"
     ]
    }
   ],
   "source": [
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "==================0================\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[42]\ttrain's binary_logloss: 0.285977\tval's binary_logloss: 0.356763\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[35]\ttrain's binary_logloss: 0.297074\tval's binary_logloss: 0.371887\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[47]\ttrain's binary_logloss: 0.278049\tval's binary_logloss: 0.359584\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[35]\ttrain's binary_logloss: 0.291356\tval's binary_logloss: 0.395249\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[32]\ttrain's binary_logloss: 0.300398\tval's binary_logloss: 0.378814\n",
      "--------------------------------------------------------------------roc: 0.8735858070960392\n",
      "----------------------------------------------------------------logloss: 0.3724583953134177\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "import pickle\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "oof_preds_list = []\n",
    "test_preds_list = []\n",
    "models_list = []\n",
    "\n",
    "for i in range(1):\n",
    "    print(f'=================={i}================')\n",
    "    if i % 4 == 0:\n",
    "        params = {'boosting_type': 'gbdt',\n",
    "            'objective': 'binary',\n",
    "#             'metric': 'None',\n",
    "            'subsample': 0.75,\n",
    "            'subsample_freq': 1,\n",
    "            'learning_rate': 0.1,\n",
    "            'feature_fraction': 0.9,\n",
    "            'max_depth': 15,\n",
    "            'lambda_l1': 1,  \n",
    "            'lambda_l2': 1,\n",
    "            'verbose': 100,\n",
    "            'early_stopping_rounds': 100,\n",
    "            'verbose': -1,\n",
    "            } \n",
    "    elif i % 4 == 1:\n",
    "        params = {\n",
    "            'max_depth': 4,\n",
    "            'max_leave': int(0.2 * 2 ** 4),\n",
    "            'reg_lambda': 1,\n",
    "            'reg_alpha': 1,\n",
    "            'subsamples': 0.8,\n",
    "            'colsample_bytree': 0.7,\n",
    "            'objective': 'binary',\n",
    "            'min_data_in_leaf': 0,\n",
    "            'boosting': 'gbdt',\n",
    "#             'metric': 'None',\n",
    "            'learning_rate': 0.1,\n",
    "                      }\n",
    "    elif i % 4 == 2:\n",
    "        params = {\n",
    "            'num_leaves': 19, \n",
    "            'min_data_in_leaf': 160,\n",
    "            'min_child_weight': 0.03,\n",
    "            'bagging_fraction' : 0.7,\n",
    "            'feature_fraction' : 0.8,\n",
    "            'learning_rate' : 0.1,\n",
    "            'max_depth': -1,\n",
    "            'reg_alpha': 0.02,\n",
    "            'reg_lambda': 0.12,\n",
    "            'objective': 'binary',\n",
    "            'verbose': 100,\n",
    "            'boost_from_average': False,\n",
    "#             'metric': 'None',\n",
    "        }  \n",
    "    else:\n",
    "        params = {\n",
    "            'objective': \"binary\",\n",
    "#             'metric': 'None',\n",
    "            'boost_from_average': \"false\",\n",
    "            'tree_learner': \"serial\",\n",
    "            'max_depth': -1,\n",
    "            'learning_rate': 0.1,\n",
    "            'num_leaves': 197,\n",
    "            'feature_fraction': 0.3,\n",
    "            'bagging_freq': 1,\n",
    "            'bagging_fraction': 0.7,\n",
    "            'min_data_in_leaf': 100,\n",
    "            'bagging_seed': 11,\n",
    "            'max_bin': 255,\n",
    "            'verbosity': -1}    \n",
    "        \n",
    "    oof_preds = np.zeros(train_grouped.shape[0])\n",
    "\n",
    "    ### test_preds = np.zeros(test_grouped.shape[0])\n",
    "    val_results = {}\n",
    "    models = []\n",
    "    params['random_state'] = i\n",
    "    iter = 100000\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=72)\n",
    "\n",
    "    #for n_fold, (trn_idx, val_idx) in enumerate(kf.split(train_grouped, train_grouped[target])):\n",
    "        # tr = train_grouped.iloc[trn_idx]\n",
    "        # val = train_grouped.iloc[val_idx]\n",
    "    for n_fold in range( 5 ):\n",
    "        tr = train_grouped[train_grouped.fold != n_fold]\n",
    "        val = train_grouped[train_grouped.fold == n_fold]\n",
    "        trn_data = lgb.Dataset(tr[features], label=tr[target])\n",
    "        val_data = lgb.Dataset(val[features], label=val[target])\n",
    "        \n",
    "        clf = lgb.train(params, trn_data, num_boost_round=iter, valid_sets=[trn_data, val_data], valid_names=['train', 'val'],\n",
    "#                         feval=feval, verbose_eval=10, early_stopping_rounds = 10/params['learning_rate'], evals_result=val_results,)\n",
    "                        verbose_eval=200, early_stopping_rounds = 10/params['learning_rate'], evals_result=val_results,)\n",
    "#         file = f'lgbs/lgb_seed{i}_fold{n_fold}.pkl'\n",
    "#         pickle.dump(clf, open(file, 'wb'))\n",
    "#         models.append(clf)\n",
    "        oof_preds[train_grouped.fold == n_fold] = clf.predict(val[features])\n",
    "        ### oof_preds[val_idx] = clf.predict(val[features])\n",
    "        ### test_preds += clf.predict(test_grouped[features]) / 5\n",
    "#     models_list.append(models)\n",
    "    oof_preds_list.append(oof_preds)\n",
    "    ###test_preds_list.append(test_preds)\n",
    "\n",
    "print(f'--------------------------------------------------------------------roc: {roc_auc_score(train_grouped[target], np.mean(oof_preds_list, axis=0))}')\n",
    "print(f'----------------------------------------------------------------logloss: {log_loss(train_grouped[target], np.mean(oof_preds_list, axis=0))}')\n",
    "lgb_oof_exam = np.mean(oof_preds_list, axis=0)\n",
    "### lgb_preds = np.mean(test_preds_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.3382810950279236"
      ]
     },
     "metadata": {},
     "execution_count": 97
    }
   ],
   "source": [
    "bce_func = torch.nn.BCELoss(reduction='mean')\n",
    "lgb_losses = bce_func(torch.FloatTensor(oof_preds), torch.FloatTensor(train_grouped['positive_exam_for_pe']))\n",
    "\n",
    "torch.mean(lgb_losses).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.3719339072704315"
      ]
     },
     "metadata": {},
     "execution_count": 100
    }
   ],
   "source": [
    "bce_func = torch.nn.BCELoss(reduction='mean')\n",
    "lgb_losses = bce_func(\n",
    "    ( 1 - torch.FloatTensor(oof_preds) ) * (4911) / (4911 + 157), \n",
    "    torch.FloatTensor(train_grouped['negative_exam_for_pe']))\n",
    "\n",
    "torch.mean(lgb_losses).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.3961322605609894"
      ]
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "source": [
    "# # current yama's pipeline for fold0-ep1\n",
    "# def calib_p(arr, factor):  # set factor>1 to enhance positive prob\n",
    "#     return arr * factor / (arr * factor + (1-arr))\n",
    "# def post_yama(arr):\n",
    "#     return calib_p( np.percentile(arr, 95), factor=1/8.5550)\n",
    "\n",
    "# lgb_losses = bce_func(torch.FloatTensor(train[['StudyInstanceUID','pred']].groupby('StudyInstanceUID').apply(post_yama)), torch.FloatTensor(train_grouped['positive_exam_for_pe'])).item()\n",
    "# lgb_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}