{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.5 64-bit",
   "display_name": "Python 3.8.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/home/haito/kaggle/rsna-str-2/workdir\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "sys.path.append(\".\")\n",
    "from src.factory import *\n",
    "from src.utils import *\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = Path(\"../input/rsna-str-pulmonary-embolism-detection/\")\n",
    "\n",
    "train = pd.read_csv(DATADIR / \"train.csv\")\n",
    "\n",
    "pre = pd.read_csv(DATADIR / \"split.csv\")\n",
    "train = train.merge(pre, on=\"StudyInstanceUID\")\n",
    "\n",
    "portion = pd.read_csv(DATADIR / \"study_pos_portion.csv\")\n",
    "train = train.merge(portion, on=\"StudyInstanceUID\")\n",
    "\n",
    "z_pos_df = pd.read_csv(DATADIR / \"sop_to_prefix.csv\").rename(columns={'img_prefix': 'z_pos'})\n",
    "train = train.merge(z_pos_df, on=\"SOPInstanceUID\")\n",
    "\n",
    "\n",
    "### train = train.query(\"fold == 0 or fold == 1\")  # now I have fold0,1 only\n",
    "\n",
    "studies = train.StudyInstanceUID.unique()\n",
    "\n",
    "# agg = t.groupby(\"StudyInstanceUID\")[\"SOPInstanceUID\"].apply(list)\n",
    "# agg_one = t.groupby(\"StudyInstanceUID\").first()\n",
    "# t = t.set_index(\"SOPInstanceUID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred(_path):\n",
    "    res = load_pickle(_path)\n",
    "    raw_pred = pd.DataFrame({\n",
    "        \"SOPInstanceUID\": res[\"ids\"],\n",
    "        **res[\"outputs\"],\n",
    "    })\n",
    "    return raw_pred\n",
    "    # return raw_pred.set_index(\"sop\")\n",
    "\n",
    "def calib_p(arr, factor):  # set factor>1 to enhance positive prob\n",
    "    return arr * factor / (arr * factor + (1-arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check\n",
    "# oof_f0 = get_pred(\"output_yuji/b3_non_weight//oof_fold0_ep0.pkl\")\n",
    "# plt.hist( oof_f0.pe_present_on_image, bins=300 )\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_f0, fold0_calib_f = get_pred(\"output_yuji/b3_non_weight//oof_fold0_ep0.pkl\"), 3.8250639579850194\n",
    "oof_f1, fold1_calib_f = get_pred(\"output_yuji/b3_non_weight//oof_fold1_ep0.pkl\"), 8.555037588568537\n",
    "oof_f2, fold2_calib_f = get_pred(\"output_yuji/b3_non_weight//oof_fold2_ep0.pkl\"), 4.374239635034443\n",
    "oof_f3, fold3_calib_f = get_pred(\"output_yuji/b3_non_weight//oof_fold3_ep0.pkl\"), 7.480972390526775\n",
    "oof_f4, fold4_calib_f = get_pred(\"output_yuji/b3_non_weight//oof_fold4_ep0.pkl\"), 5.002262078458348\n",
    "\n",
    "# BAD\n",
    "if False:  # pick best one which yields weighted-logloss after calib\n",
    "    oof_f3, fold3_calib_f = get_pred(\"output/035_pe_present___448___apex/fold3_ep0.pt.valid.pickle\"), 6.541753595870311\n",
    "    oof_f4, fold4_calib_f = get_pred(\"output/035_pe_present___448___apex/fold4_ep0.pt.valid.pickle\"), 3.8250639579850194\n",
    "    \n",
    "\n",
    "if True: ### ==== do calib for each fold\n",
    "    oof_f0[\"pe_present_on_image\"] = calib_p(oof_f0[\"pe_present_on_image\"], fold0_calib_f)\n",
    "    oof_f1[\"pe_present_on_image\"] = calib_p(oof_f1[\"pe_present_on_image\"], fold1_calib_f)\n",
    "    oof_f2[\"pe_present_on_image\"] = calib_p(oof_f2[\"pe_present_on_image\"], fold2_calib_f)\n",
    "    oof_f3[\"pe_present_on_image\"] = calib_p(oof_f3[\"pe_present_on_image\"], fold3_calib_f)\n",
    "    oof_f4[\"pe_present_on_image\"] = calib_p(oof_f4[\"pe_present_on_image\"], fold4_calib_f)\n",
    "\n",
    "oof = pd.concat([oof_f0, oof_f1, oof_f2, oof_f3, oof_f4]).rename(columns={'pe_present_on_image': 'pred0'})\n",
    "\n",
    "train = train.merge(oof[['pred0', 'SOPInstanceUID']], on=\"SOPInstanceUID\")  # add pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  StudyInstanceUID SeriesInstanceUID SOPInstanceUID  pe_present_on_image  \\\n",
       "0     6897fa9de148      2bfbb7fd2e8b   c0f3cb036d06                    0   \n",
       "1     6897fa9de148      2bfbb7fd2e8b   f57ffd3883b6                    0   \n",
       "2     6897fa9de148      2bfbb7fd2e8b   41220fda34a3                    0   \n",
       "3     6897fa9de148      2bfbb7fd2e8b   13b685b4b14f                    0   \n",
       "4     6897fa9de148      2bfbb7fd2e8b   be0b7524ffb4                    0   \n",
       "\n",
       "   negative_exam_for_pe  qa_motion  qa_contrast  flow_artifact  \\\n",
       "0                     0          0            0              0   \n",
       "1                     0          0            0              0   \n",
       "2                     0          0            0              0   \n",
       "3                     0          0            0              0   \n",
       "4                     0          0            0              0   \n",
       "\n",
       "   rv_lv_ratio_gte_1  rv_lv_ratio_lt_1  ...  true_filling_defect_not_pe  \\\n",
       "0                  0                 1  ...                           0   \n",
       "1                  0                 1  ...                           0   \n",
       "2                  0                 1  ...                           0   \n",
       "3                  0                 1  ...                           0   \n",
       "4                  0                 1  ...                           0   \n",
       "\n",
       "   rightsided_pe  acute_and_chronic_pe  central_pe  indeterminate  exam_type  \\\n",
       "0              1                     0           0              0   positive   \n",
       "1              1                     0           0              0   positive   \n",
       "2              1                     0           0              0   positive   \n",
       "3              1                     0           0              0   positive   \n",
       "4              1                     0           0              0   positive   \n",
       "\n",
       "   fold pe_present_portion  z_pos     pred0  \n",
       "0     1            0.33871    123  0.002160  \n",
       "1     1            0.33871    114  0.000151  \n",
       "2     1            0.33871     24  0.218694  \n",
       "3     1            0.33871     23  0.272379  \n",
       "4     1            0.33871     22  0.065160  \n",
       "\n",
       "[5 rows x 22 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>StudyInstanceUID</th>\n      <th>SeriesInstanceUID</th>\n      <th>SOPInstanceUID</th>\n      <th>pe_present_on_image</th>\n      <th>negative_exam_for_pe</th>\n      <th>qa_motion</th>\n      <th>qa_contrast</th>\n      <th>flow_artifact</th>\n      <th>rv_lv_ratio_gte_1</th>\n      <th>rv_lv_ratio_lt_1</th>\n      <th>...</th>\n      <th>true_filling_defect_not_pe</th>\n      <th>rightsided_pe</th>\n      <th>acute_and_chronic_pe</th>\n      <th>central_pe</th>\n      <th>indeterminate</th>\n      <th>exam_type</th>\n      <th>fold</th>\n      <th>pe_present_portion</th>\n      <th>z_pos</th>\n      <th>pred0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>6897fa9de148</td>\n      <td>2bfbb7fd2e8b</td>\n      <td>c0f3cb036d06</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>positive</td>\n      <td>1</td>\n      <td>0.33871</td>\n      <td>123</td>\n      <td>0.002160</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>6897fa9de148</td>\n      <td>2bfbb7fd2e8b</td>\n      <td>f57ffd3883b6</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>positive</td>\n      <td>1</td>\n      <td>0.33871</td>\n      <td>114</td>\n      <td>0.000151</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>6897fa9de148</td>\n      <td>2bfbb7fd2e8b</td>\n      <td>41220fda34a3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>positive</td>\n      <td>1</td>\n      <td>0.33871</td>\n      <td>24</td>\n      <td>0.218694</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6897fa9de148</td>\n      <td>2bfbb7fd2e8b</td>\n      <td>13b685b4b14f</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>positive</td>\n      <td>1</td>\n      <td>0.33871</td>\n      <td>23</td>\n      <td>0.272379</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>6897fa9de148</td>\n      <td>2bfbb7fd2e8b</td>\n      <td>be0b7524ffb4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>positive</td>\n      <td>1</td>\n      <td>0.33871</td>\n      <td>22</td>\n      <td>0.065160</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 22 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  StudyInstanceUID  monai_rv_lv_ratio_gte_1  monai_central_pe  \\\n",
       "0     0003b3d648eb                 0.390070          0.119672   \n",
       "1     004a429fc727                 0.283612          0.028638   \n",
       "2     00617c9fe236                 0.583049          0.065179   \n",
       "3     00c38669b4fd                 0.626294          0.163212   \n",
       "4     00c4802978cc                 0.049200          0.029377   \n",
       "\n",
       "   monai_leftsided_pe  monai_rightsided_pe  monai_acute_and_chronic_pe  \\\n",
       "0            0.705328             0.882280                    0.054105   \n",
       "1            0.642636             0.815451                    0.043123   \n",
       "2            0.635317             0.878805                    0.061502   \n",
       "3            0.768412             0.795797                    0.091142   \n",
       "4            0.660739             0.756005                    0.043086   \n",
       "\n",
       "   monai_chronic_pe  \n",
       "0          0.108056  \n",
       "1          0.117819  \n",
       "2          0.056411  \n",
       "3          0.157873  \n",
       "4          0.071877  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>StudyInstanceUID</th>\n      <th>monai_rv_lv_ratio_gte_1</th>\n      <th>monai_central_pe</th>\n      <th>monai_leftsided_pe</th>\n      <th>monai_rightsided_pe</th>\n      <th>monai_acute_and_chronic_pe</th>\n      <th>monai_chronic_pe</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0003b3d648eb</td>\n      <td>0.390070</td>\n      <td>0.119672</td>\n      <td>0.705328</td>\n      <td>0.882280</td>\n      <td>0.054105</td>\n      <td>0.108056</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>004a429fc727</td>\n      <td>0.283612</td>\n      <td>0.028638</td>\n      <td>0.642636</td>\n      <td>0.815451</td>\n      <td>0.043123</td>\n      <td>0.117819</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00617c9fe236</td>\n      <td>0.583049</td>\n      <td>0.065179</td>\n      <td>0.635317</td>\n      <td>0.878805</td>\n      <td>0.061502</td>\n      <td>0.056411</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>00c38669b4fd</td>\n      <td>0.626294</td>\n      <td>0.163212</td>\n      <td>0.768412</td>\n      <td>0.795797</td>\n      <td>0.091142</td>\n      <td>0.157873</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>00c4802978cc</td>\n      <td>0.049200</td>\n      <td>0.029377</td>\n      <td>0.660739</td>\n      <td>0.756005</td>\n      <td>0.043086</td>\n      <td>0.071877</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Index(['StudyInstanceUID', 'SeriesInstanceUID', 'SOPInstanceUID',\n",
       "       'pe_present_on_image', 'negative_exam_for_pe', 'qa_motion',\n",
       "       'qa_contrast', 'flow_artifact', 'rv_lv_ratio_gte_1', 'rv_lv_ratio_lt_1',\n",
       "       'leftsided_pe', 'chronic_pe', 'true_filling_defect_not_pe',\n",
       "       'rightsided_pe', 'acute_and_chronic_pe', 'central_pe', 'indeterminate',\n",
       "       'exam_type', 'fold', 'pe_present_portion', 'z_pos', 'pred0'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "train_copyed = train.copy()\n",
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "' feature engineer '"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "\"\"\" feature engineer \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = pd.read_pickle('../input/train_with_position.pkl')\n",
    "train = train.sort_values(['StudyInstanceUID', 'z_pos'])\n",
    "# train = train.merge(oof[['pred', 'SOPInstanceUID']], on='SOPInstanceUID')\n",
    "\n",
    "# test = pd.read_pickle('../input/test_with_position.pkl')\n",
    "# test = test.sort_values(['StudyInstanceUID', 'z_pos'])\n",
    "# test = test.merge(test_pred[pe_present_portion], on='SOPInstanceUID')\n",
    "\n",
    "# train = train.merge(oof_non_weight[['pred_non_weight', 'SOPInstanceUID']], on='SOPInstanceUID')\n",
    "# test = test.merge(test_pred_non_weight[['pred_non_weight', 'SOPInstanceUID']], on='SOPInstanceUID')\n",
    "# train['pred_mean'] = (train['pred'] + train['pred_non_weight']) / 2\n",
    "# test['pred_mean'] = (test['pred'] + test['pred_non_weight']) / 2\n",
    "\n",
    "train_current_z_pos = train.groupby('StudyInstanceUID')['z_pos'].shift(0)\n",
    "# test_current_z_pos = test.groupby('StudyInstanceUID')['z_pos'].shift(0)\n",
    "### for i in range(1, 20):\n",
    "for i in range(1, 10):\n",
    "\n",
    "    # train[f'pre_mean{i}'] = train.groupby('StudyInstanceUID')['pred_mean'].shift(i)\n",
    "    # train[f'post_mean{i}'] = train.groupby('StudyInstanceUID')['pred_mean'].shift(-i)\n",
    "    train[f'pred0_pre{i}'] = train.groupby('StudyInstanceUID')['pred0'].shift(i)\n",
    "    train[f'pred0_post{i}'] = train.groupby('StudyInstanceUID')['pred0'].shift(-i)\n",
    "    # train[f'pre_non_weight{i}'] = train.groupby('StudyInstanceUID')['pred_non_weight'].shift(i)\n",
    "    # train[f'post_non_weight{i}'] = train.groupby('StudyInstanceUID')['pred_non_weight'].shift(-i)\n",
    "    \n",
    "    # test[f'pre_mean{i}'] = test.groupby('StudyInstanceUID')['pred_mean'].shift(i)\n",
    "    # test[f'post_mean{i}'] = test.groupby('StudyInstanceUID')['pred_mean'].shift(-i)\n",
    "    # test[f'pre{i}'] = test.groupby('StudyInstanceUID')['pred'].shift(i)\n",
    "    # test[f'post{i}'] = test.groupby('StudyInstanceUID')['pred'].shift(-i)\n",
    "    # test[f'pre_non_weight{i}'] = test.groupby('StudyInstanceUID')['pred_non_weight'].shift(i)\n",
    "    # test[f'post_non_weight{i}'] = test.groupby('StudyInstanceUID')['pred_non_weight'].shift(-i)\n",
    "\n",
    "\n",
    "# for i in [1]:\n",
    "#     train[f'pre_z_pos_diff{i}'] = train_current_z_pos - train.groupby('StudyInstanceUID')['pred'].shift(i)\n",
    "#     # test[f'pre_z_pos_diff{i}'] = test_current_z_pos - test.groupby('StudyInstanceUID')['pred'].shift(i)\n",
    "\n",
    "# NORMALIZED Z POS\n",
    "z_max = train.groupby('StudyInstanceUID').z_pos.max().rename('z_pos_max')\n",
    "train = train.merge(z_max, on='StudyInstanceUID')\n",
    "train['z_pos_norm'] = train['z_pos'] / train['z_pos_max']\n",
    "train = train.drop('z_pos_max', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "        StudyInstanceUID SeriesInstanceUID SOPInstanceUID  \\\n",
       "1790589     fffda3f22362      39ca5eaafffe   29e855db7f2b   \n",
       "1790590     fffda3f22362      39ca5eaafffe   f7ca277a66c2   \n",
       "1790591     fffda3f22362      39ca5eaafffe   59714fd8dd25   \n",
       "1790592     fffda3f22362      39ca5eaafffe   b33567349fae   \n",
       "1790593     fffda3f22362      39ca5eaafffe   53d378d07811   \n",
       "\n",
       "         pe_present_on_image  negative_exam_for_pe  qa_motion  qa_contrast  \\\n",
       "1790589                    0                     1          0            0   \n",
       "1790590                    0                     1          0            0   \n",
       "1790591                    0                     1          0            0   \n",
       "1790592                    0                     1          0            0   \n",
       "1790593                    0                     1          0            0   \n",
       "\n",
       "         flow_artifact  rv_lv_ratio_gte_1  rv_lv_ratio_lt_1  ...  pred0_post5  \\\n",
       "1790589              0                  0                 0  ...          NaN   \n",
       "1790590              0                  0                 0  ...          NaN   \n",
       "1790591              0                  0                 0  ...          NaN   \n",
       "1790592              0                  0                 0  ...          NaN   \n",
       "1790593              0                  0                 0  ...          NaN   \n",
       "\n",
       "           pred0_pre6  pred0_post6    pred0_pre7  pred0_post7    pred0_pre8  \\\n",
       "1790589  1.633664e-06          NaN  1.558988e-06          NaN  3.526268e-07   \n",
       "1790590  4.497674e-07          NaN  1.633664e-06          NaN  1.558988e-06   \n",
       "1790591  6.327988e-07          NaN  4.497674e-07          NaN  1.633664e-06   \n",
       "1790592  4.913005e-06          NaN  6.327988e-07          NaN  4.497674e-07   \n",
       "1790593  2.411817e-06          NaN  4.913005e-06          NaN  6.327988e-07   \n",
       "\n",
       "         pred0_post8    pred0_pre9  pred0_post9  z_pos_norm  \n",
       "1790589          NaN  9.371637e-07          NaN    0.976190  \n",
       "1790590          NaN  3.526268e-07          NaN    0.982143  \n",
       "1790591          NaN  1.558988e-06          NaN    0.988095  \n",
       "1790592          NaN  1.633664e-06          NaN    0.994048  \n",
       "1790593          NaN  4.497674e-07          NaN    1.000000  \n",
       "\n",
       "[5 rows x 41 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>StudyInstanceUID</th>\n      <th>SeriesInstanceUID</th>\n      <th>SOPInstanceUID</th>\n      <th>pe_present_on_image</th>\n      <th>negative_exam_for_pe</th>\n      <th>qa_motion</th>\n      <th>qa_contrast</th>\n      <th>flow_artifact</th>\n      <th>rv_lv_ratio_gte_1</th>\n      <th>rv_lv_ratio_lt_1</th>\n      <th>...</th>\n      <th>pred0_post5</th>\n      <th>pred0_pre6</th>\n      <th>pred0_post6</th>\n      <th>pred0_pre7</th>\n      <th>pred0_post7</th>\n      <th>pred0_pre8</th>\n      <th>pred0_post8</th>\n      <th>pred0_pre9</th>\n      <th>pred0_post9</th>\n      <th>z_pos_norm</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1790589</th>\n      <td>fffda3f22362</td>\n      <td>39ca5eaafffe</td>\n      <td>29e855db7f2b</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>1.633664e-06</td>\n      <td>NaN</td>\n      <td>1.558988e-06</td>\n      <td>NaN</td>\n      <td>3.526268e-07</td>\n      <td>NaN</td>\n      <td>9.371637e-07</td>\n      <td>NaN</td>\n      <td>0.976190</td>\n    </tr>\n    <tr>\n      <th>1790590</th>\n      <td>fffda3f22362</td>\n      <td>39ca5eaafffe</td>\n      <td>f7ca277a66c2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>4.497674e-07</td>\n      <td>NaN</td>\n      <td>1.633664e-06</td>\n      <td>NaN</td>\n      <td>1.558988e-06</td>\n      <td>NaN</td>\n      <td>3.526268e-07</td>\n      <td>NaN</td>\n      <td>0.982143</td>\n    </tr>\n    <tr>\n      <th>1790591</th>\n      <td>fffda3f22362</td>\n      <td>39ca5eaafffe</td>\n      <td>59714fd8dd25</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>6.327988e-07</td>\n      <td>NaN</td>\n      <td>4.497674e-07</td>\n      <td>NaN</td>\n      <td>1.633664e-06</td>\n      <td>NaN</td>\n      <td>1.558988e-06</td>\n      <td>NaN</td>\n      <td>0.988095</td>\n    </tr>\n    <tr>\n      <th>1790592</th>\n      <td>fffda3f22362</td>\n      <td>39ca5eaafffe</td>\n      <td>b33567349fae</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>4.913005e-06</td>\n      <td>NaN</td>\n      <td>6.327988e-07</td>\n      <td>NaN</td>\n      <td>4.497674e-07</td>\n      <td>NaN</td>\n      <td>1.633664e-06</td>\n      <td>NaN</td>\n      <td>0.994048</td>\n    </tr>\n    <tr>\n      <th>1790593</th>\n      <td>fffda3f22362</td>\n      <td>39ca5eaafffe</td>\n      <td>53d378d07811</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>2.411817e-06</td>\n      <td>NaN</td>\n      <td>4.913005e-06</td>\n      <td>NaN</td>\n      <td>6.327988e-07</td>\n      <td>NaN</td>\n      <td>4.497674e-07</td>\n      <td>NaN</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 41 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['pred0', 'pred0_post1', 'pred0_post2', 'pred0_post3', 'pred0_post4', 'pred0_post5', 'pred0_post6', 'pred0_post7', 'pred0_post8', 'pred0_post9', 'pred0_pre1', 'pred0_pre2', 'pred0_pre3', 'pred0_pre4', 'pred0_pre5', 'pred0_pre6', 'pred0_pre7', 'pred0_pre8', 'pred0_pre9', 'z_pos_norm']\n"
     ]
    }
   ],
   "source": [
    "ids = [c for c in list(train) if 'UID' in c]\n",
    "targets = [\n",
    "    'negative_exam_for_pe',\n",
    "    'indeterminate',\n",
    "    'chronic_pe',\n",
    "    'acute_and_chronic_pe',\n",
    "    'central_pe',\n",
    "    'leftsided_pe',\n",
    "    'rightsided_pe',\n",
    "    'rv_lv_ratio_gte_1',\n",
    "    'rv_lv_ratio_lt_1',\n",
    "]\n",
    "other_targets = [c for c in list(train) if 'pe_present_on_image' in c]\n",
    "### remove_cols = ['fold', 'path', 'weight', 'qa_contrast', 'qa_motion'] + targets + ids + other_targets\n",
    "### remove_cols = ['fold', 'path', 'weight', 'qa_contrast', 'qa_motion'] + ['exam_type','flow_artifact','pe_present_portion', 'true_filling_defect_not_pe'] + targets + ids + other_targets\n",
    "remove_cols = ['fold', 'path', 'weight', 'qa_contrast', 'qa_motion'] + ['exam_type','flow_artifact','pe_present_portion', 'true_filling_defect_not_pe'] + targets + ids + other_targets + ['z_pos']\n",
    "\n",
    "features = sorted(list(set(list(train)) - set(remove_cols)))\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_copyed = features.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fobj(pred, data):\n",
    "    true = data.get_label()\n",
    "    label = 2*true - 1\n",
    "    weights = data.weights\n",
    "    response = -label / (1 + np.exp(label * pred))\n",
    "    abs_response = np.abs(response)\n",
    "    grad = response\n",
    "    hess = abs_response * (1 - abs_response)\n",
    "    return grad*weights, hess*weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "bce_func = torch.nn.BCELoss(reduction='none')\n",
    "\n",
    "def feval2(preds, data):\n",
    "    scores = bce_func(torch.FloatTensor(preds), torch.FloatTensor(data.label))\n",
    "    scores = scores * torch.FloatTensor(data.weights)\n",
    "    return 'weighted logloss', torch.mean(scores), False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 推測して作成したもの. Yujiに確認する必要がある\n",
    "import torch\n",
    "bce_func_logit = torch.nn.BCEWithLogitsLoss(reduction='none')\n",
    "\n",
    "def feval(preds, data):\n",
    "    scores = bce_func_logit(torch.FloatTensor(preds), torch.FloatTensor(data.label))\n",
    "    scores = scores * torch.FloatTensor(data.weights)\n",
    "    return 'weighted logloss', torch.mean(scores), False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['pred']\n",
    "features = features_copyed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "==================0================\n",
      "    ==============fold0================\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[53]\ttrain's weighted logloss: 0.0110919\tval's weighted logloss: 0.0110957\n",
      "    ==============fold1================\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[76]\ttrain's weighted logloss: 0.0107546\tval's weighted logloss: 0.0116312\n",
      "    ==============fold2================\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[70]\ttrain's weighted logloss: 0.0108946\tval's weighted logloss: 0.01141\n",
      "    ==============fold3================\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[182]\ttrain's weighted logloss: 0.0103165\tval's weighted logloss: 0.0117116\n",
      "    ==============fold4================\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[60]\ttrain's weighted logloss: 0.01096\tval's weighted logloss: 0.0112557\n",
      "-------------------------------------------------------------------------roc_auc: 0.9623247655817194\n",
      "----------------------------------------------------------roc_auc using raw pred: 0.950916101960868\n",
      "------------------------------------------------------------------------------AP: 0.7864683523887135\n",
      "---------------------------------------------------------------AP using raw pred: 0.7370492969022756\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "import pickle\n",
    "\n",
    "oof_preds_list = []\n",
    "test_preds_list = []\n",
    "models_list = []\n",
    "target = 'pe_present_on_image'\n",
    "\n",
    "for i in range(1):\n",
    "    print(f'=================={i}================')\n",
    "    if i % 4 == 0:\n",
    "        params = {'boosting_type': 'gbdt',\n",
    "            'objective': 'binary',\n",
    "#             'metric': 'None',\n",
    "            'subsample': 0.75,\n",
    "            'subsample_freq': 1,\n",
    "            'learning_rate': 0.1,\n",
    "            'feature_fraction': 0.9,\n",
    "            'max_depth': 15,\n",
    "            'lambda_l1': 1,  \n",
    "            'lambda_l2': 1,\n",
    "            'verbose': 100,\n",
    "            'early_stopping_rounds': 100,\n",
    "            'verbose': -1,\n",
    "            } \n",
    "    elif i % 4 == 1:\n",
    "        params = {\n",
    "            'max_depth': 4,\n",
    "            'max_leave': int(0.2 * 2 ** 4),\n",
    "            'reg_lambda': 1,\n",
    "            'reg_alpha': 1,\n",
    "            'subsamples': 0.8,\n",
    "            'colsample_bytree': 0.7,\n",
    "            'objective': 'binary',\n",
    "            'min_data_in_leaf': 0,\n",
    "            'boosting': 'gbdt',\n",
    "            'metric': 'None',\n",
    "            'learning_rate': 0.1,\n",
    "                      }\n",
    "    elif i % 4 == 2:\n",
    "        params = {\n",
    "            'num_leaves': 19, \n",
    "            'min_data_in_leaf': 160,\n",
    "            'min_child_weight': 0.03,\n",
    "            'bagging_fraction' : 0.7,\n",
    "            'feature_fraction' : 0.8,\n",
    "            'learning_rate' : 0.1,\n",
    "            'max_depth': -1,\n",
    "            'reg_alpha': 0.02,\n",
    "            'reg_lambda': 0.12,\n",
    "            'objective': 'binary',\n",
    "            'verbose': 100,\n",
    "            'boost_from_average': False,\n",
    "            'metric': 'None',\n",
    "        }  \n",
    "    else:\n",
    "        params = {\n",
    "            'objective': \"binary\",\n",
    "            'metric': 'None',\n",
    "            'boost_from_average': \"false\",\n",
    "            'tree_learner': \"serial\",\n",
    "            'max_depth': -1,\n",
    "            'learning_rate': 0.1,\n",
    "            'num_leaves': 197,\n",
    "            'feature_fraction': 0.3,\n",
    "            'bagging_freq': 1,\n",
    "            'bagging_fraction': 0.7,\n",
    "            'min_data_in_leaf': 100,\n",
    "            'bagging_seed': 11,\n",
    "            'max_bin': 255,\n",
    "            'verbosity': -1}    \n",
    "        \n",
    "    oof_preds = np.zeros(train.shape[0])\n",
    "    ### test_preds = np.zeros(test.shape[0])\n",
    "    val_results = {}\n",
    "    models = []\n",
    "    params['random_state'] = i\n",
    "    iter = 100000\n",
    "#     for n_fold, (trn_idx, val_idx) in enumerate(kf.split(train, train[target])):\n",
    "\n",
    "    for n_fold in range(5):\n",
    "    ### for n_fold in range(2):\n",
    "        print(f'    ==============fold{n_fold}================')\n",
    "        tr = train.query(f'fold != {n_fold}')\n",
    "        val = train.query(f'fold == {n_fold}')\n",
    "        trn_data = lgb.Dataset(tr[features], label=tr[target])\n",
    "        trn_data.weights = tr.pe_present_portion.values\n",
    "        val_data = lgb.Dataset(val[features], label=val[target])\n",
    "        val_data.weights = val.pe_present_portion.values\n",
    "        \n",
    "        clf = lgb.train(params, trn_data, num_boost_round=iter, valid_sets=[trn_data, val_data], valid_names=['train', 'val'],\n",
    "#                         verbose_eval=200, early_stopping_rounds = 10/params['learning_rate'], evals_result=val_results,)\n",
    "                        feval=feval, fobj = fobj, verbose_eval=2000, early_stopping_rounds = 10/params['learning_rate'], evals_result=val_results, )\n",
    "        file = f'lgbs/lgb_seed{i}_fold{n_fold}.pkl'\n",
    "        pickle.dump(clf, open(file, 'wb'))\n",
    "#         models.append(clf)\n",
    "        oof_preds[train.fold==n_fold] = clf.predict(val[features])\n",
    "        ### test_preds += clf.predict(test[features]) / 5\n",
    "#     models_list.append(models)\n",
    "    oof_preds_list.append(oof_preds)\n",
    "    ### test_preds_list.append(test_preds)\n",
    "\n",
    "print(f'-------------------------------------------------------------------------roc_auc: {roc_auc_score(train[target], np.mean(oof_preds_list, axis=0))}')\n",
    "print(f'----------------------------------------------------------roc_auc using raw pred: {roc_auc_score(train[target], train[\"pred0\"])}')\n",
    "print(f'------------------------------------------------------------------------------AP: {average_precision_score(train[target], np.mean(oof_preds_list, axis=0))}')\n",
    "print(f'---------------------------------------------------------------AP using raw pred: {average_precision_score(train[target], train[\"pred0\"])}')\n",
    "\n",
    "lgb_oof = np.mean(oof_preds_list, axis=0)\n",
    "lgb_preds = np.mean(test_preds_list, axis=0)\n",
    "train['lgb_preds'] = sigmoid(lgb_oof)\n",
    "### test['lgb_preds'] = sigmoid(lgb_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.011420365194671893"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "bce_func = torch.nn.BCELoss(reduction='none')\n",
    "\n",
    "lgb_losses = bce_func(torch.FloatTensor(sigmoid(lgb_oof)), torch.FloatTensor(train['pe_present_on_image']))\n",
    "\n",
    "### torch.mean(lgb_losses*train['weight'].values)\n",
    "torch.mean(lgb_losses*train['pe_present_portion'].values).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.01322563879510154"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "# no stacking result\n",
    "lgb_losses = bce_func(torch.FloatTensor(train['pred0']), torch.FloatTensor(train['pe_present_on_image']))\n",
    "torch.mean(lgb_losses*train['pe_present_portion'].values).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raise "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PE_REPESNT -> POS_EXAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = Path(\"../input/rsna-str-pulmonary-embolism-detection/\")\n",
    "\n",
    "train = pd.read_csv(DATADIR / \"train.csv\")\n",
    "\n",
    "pre = pd.read_csv(DATADIR / \"split.csv\")\n",
    "train = train.merge(pre, on=\"StudyInstanceUID\")\n",
    "\n",
    "portion = pd.read_csv(DATADIR / \"study_pos_portion.csv\")\n",
    "train = train.merge(portion, on=\"StudyInstanceUID\")\n",
    "\n",
    "z_pos_df = pd.read_csv(DATADIR / \"sop_to_prefix.csv\").rename(columns={'img_prefix': 'z_pos'})\n",
    "train = train.merge(z_pos_df, on=\"SOPInstanceUID\")\n",
    "\n",
    "studies = train.StudyInstanceUID.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof = pd.concat([oof_f0, oof_f1, oof_f2, oof_f3, oof_f4]).rename(columns={'pe_present_on_image': 'pred'})\n",
    "\n",
    "train = train.merge(oof[['pred', 'SOPInstanceUID']], on=\"SOPInstanceUID\")  # add pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" ADD MOANI \"\"\"\n",
    "oof_monai = pd.read_csv(\"output_jan/5foldmonai/monai_6targets_oof.csv\").drop(\n",
    "    ['LOGITS_rv_lv_ratio_gte_1', 'LOGITS_central_pe', 'LOGITS_leftsided_pe', \n",
    "    'LOGITS_rightsided_pe', 'LOGITS_acute_and_chronic_pe', 'LOGITS_chronic_pe', 'fold'],\n",
    "    axis=1)\n",
    "for key in oof_monai.columns:\n",
    "    oof_monai = oof_monai.rename(columns={key: \"monai_\" + key})\n",
    "oof_monai = oof_monai.rename(columns={\"monai_study_id\": \"StudyInstanceUID\"})\n",
    "# print( oof_monai.head() )\n",
    "\n",
    "# train = train.merge(oof_monai, on=\"StudyInstanceUID\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "def grouping(df):\n",
    "    grouped = pd.DataFrame(df.groupby('StudyInstanceUID')['pred'].mean())\n",
    "    grouped = grouped.rename(columns={'pred': 'mean'})\n",
    "    count = df.groupby('StudyInstanceUID')['pred'].count()\n",
    "    grouped['count_total'] = count\n",
    "\n",
    "    for i in range(1,10):\n",
    "        count = df[df.pred>i/10].groupby('StudyInstanceUID')['pred'].count()\n",
    "        grouped[f'count_over{i/10}'] = count\n",
    "        grouped[f'count_over{i/10}_ratio'] = count / grouped['count_total']\n",
    "\n",
    "    for q in [30, 50, 70, 80, 90, 95, 99]:\n",
    "        grouped[f'percentile{q}'] = df.groupby('StudyInstanceUID')['pred'].apply(lambda arr: np.percentile(arr, q))\n",
    "\n",
    "    ma = pd.DataFrame(df.groupby('StudyInstanceUID')['pred'].max())\n",
    "    grouped['max'] = ma.pred\n",
    "\n",
    "    grouped = grouped.reset_index().fillna(0)\n",
    "    return grouped\n",
    "train_grouped = grouping(train)\n",
    "# ADD MONAI\n",
    "train_grouped = train_grouped.merge(oof_monai, on=\"StudyInstanceUID\")\n",
    "\n",
    "train_grouped['fold'] = train.groupby('StudyInstanceUID')['fold'].first().values\n",
    "# add target\n",
    "train_grouped['negative_exam_for_pe'] = train.groupby('StudyInstanceUID')['negative_exam_for_pe'].first().values\n",
    "train_grouped['positive_exam_for_pe'] = (1 - train.groupby('StudyInstanceUID')['negative_exam_for_pe'].first().values) * (1 - train.groupby('StudyInstanceUID')['indeterminate'].first().values)\n",
    "for key in [\n",
    "    'chronic_pe',\n",
    "    'acute_and_chronic_pe',\n",
    "    'central_pe',\n",
    "    'leftsided_pe',\n",
    "    'rightsided_pe',\n",
    "    'rv_lv_ratio_gte_1',\n",
    "    'rv_lv_ratio_lt_1',]:\n",
    "    train_grouped[ key ] = train.groupby('StudyInstanceUID')[ key ].first().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  StudyInstanceUID      mean  count_total  count_over0.1  count_over0.1_ratio  \\\n",
       "0     0003b3d648eb  0.271391          223          121.0             0.542601   \n",
       "1     000f7f114264  0.091386          239           65.0             0.271967   \n",
       "2     00102474a2db  0.055245          326           58.0             0.177914   \n",
       "3     0038fd5f09f5  0.054540          230           34.0             0.147826   \n",
       "4     0045f113e031  0.133363          257          111.0             0.431907   \n",
       "\n",
       "   count_over0.2  count_over0.2_ratio  count_over0.3  count_over0.3_ratio  \\\n",
       "0           94.0             0.421525           80.0             0.358744   \n",
       "1           45.0             0.188285           28.0             0.117155   \n",
       "2           21.0             0.064417           10.0             0.030675   \n",
       "3           13.0             0.056522            6.0             0.026087   \n",
       "4           75.0             0.291829           45.0             0.175097   \n",
       "\n",
       "   count_over0.4  ...  fold  negative_exam_for_pe  positive_exam_for_pe  \\\n",
       "0           61.0  ...     0                     1                     0   \n",
       "1           16.0  ...     3                     1                     0   \n",
       "2            4.0  ...     3                     1                     0   \n",
       "3            6.0  ...     4                     1                     0   \n",
       "4           22.0  ...     4                     1                     0   \n",
       "\n",
       "   chronic_pe  acute_and_chronic_pe  central_pe  leftsided_pe  rightsided_pe  \\\n",
       "0           0                     0           0             0              0   \n",
       "1           0                     0           0             0              0   \n",
       "2           0                     0           0             0              0   \n",
       "3           0                     0           0             0              0   \n",
       "4           0                     0           0             0              0   \n",
       "\n",
       "   rv_lv_ratio_gte_1  rv_lv_ratio_lt_1  \n",
       "0                  0                 0  \n",
       "1                  0                 0  \n",
       "2                  0                 0  \n",
       "3                  0                 0  \n",
       "4                  0                 0  \n",
       "\n",
       "[5 rows x 45 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>StudyInstanceUID</th>\n      <th>mean</th>\n      <th>count_total</th>\n      <th>count_over0.1</th>\n      <th>count_over0.1_ratio</th>\n      <th>count_over0.2</th>\n      <th>count_over0.2_ratio</th>\n      <th>count_over0.3</th>\n      <th>count_over0.3_ratio</th>\n      <th>count_over0.4</th>\n      <th>...</th>\n      <th>fold</th>\n      <th>negative_exam_for_pe</th>\n      <th>positive_exam_for_pe</th>\n      <th>chronic_pe</th>\n      <th>acute_and_chronic_pe</th>\n      <th>central_pe</th>\n      <th>leftsided_pe</th>\n      <th>rightsided_pe</th>\n      <th>rv_lv_ratio_gte_1</th>\n      <th>rv_lv_ratio_lt_1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0003b3d648eb</td>\n      <td>0.271391</td>\n      <td>223</td>\n      <td>121.0</td>\n      <td>0.542601</td>\n      <td>94.0</td>\n      <td>0.421525</td>\n      <td>80.0</td>\n      <td>0.358744</td>\n      <td>61.0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000f7f114264</td>\n      <td>0.091386</td>\n      <td>239</td>\n      <td>65.0</td>\n      <td>0.271967</td>\n      <td>45.0</td>\n      <td>0.188285</td>\n      <td>28.0</td>\n      <td>0.117155</td>\n      <td>16.0</td>\n      <td>...</td>\n      <td>3</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00102474a2db</td>\n      <td>0.055245</td>\n      <td>326</td>\n      <td>58.0</td>\n      <td>0.177914</td>\n      <td>21.0</td>\n      <td>0.064417</td>\n      <td>10.0</td>\n      <td>0.030675</td>\n      <td>4.0</td>\n      <td>...</td>\n      <td>3</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0038fd5f09f5</td>\n      <td>0.054540</td>\n      <td>230</td>\n      <td>34.0</td>\n      <td>0.147826</td>\n      <td>13.0</td>\n      <td>0.056522</td>\n      <td>6.0</td>\n      <td>0.026087</td>\n      <td>6.0</td>\n      <td>...</td>\n      <td>4</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0045f113e031</td>\n      <td>0.133363</td>\n      <td>257</td>\n      <td>111.0</td>\n      <td>0.431907</td>\n      <td>75.0</td>\n      <td>0.291829</td>\n      <td>45.0</td>\n      <td>0.175097</td>\n      <td>22.0</td>\n      <td>...</td>\n      <td>4</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 45 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 191
    }
   ],
   "source": [
    "train_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "target rv_lv_ratio_lt_1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# features = list(set(list(train_grouped)) - set(['StudyInstanceUID', 'positive_exam_for_pe', 'negative_exam_for_pe', 'fold']) - set(['count_total']))\n",
    "\n",
    "features = list( set(list(train_grouped)) - set(['StudyInstanceUID', 'positive_exam_for_pe', 'negative_exam_for_pe', 'fold']) - set(['count_total']) - set( [\n",
    "    'chronic_pe',\n",
    "    'acute_and_chronic_pe',\n",
    "    'central_pe',\n",
    "    'leftsided_pe',\n",
    "    'rightsided_pe',\n",
    "    'rv_lv_ratio_gte_1',\n",
    "    'rv_lv_ratio_lt_1',]  ) )\n",
    "features = sorted(features)\n",
    "\n",
    "### target = 'positive_exam_for_pe'\n",
    "\n",
    "targets_monai = [\n",
    "        'rv_lv_ratio_gte_1', # exam level\n",
    "        \"central_pe\",\n",
    "        \"leftsided_pe\",\n",
    "        \"rightsided_pe\",\n",
    "        \"acute_and_chronic_pe\",\n",
    "        \"chronic_pe\"\n",
    "    ]\n",
    "#target = targets_monai[ 1 ]\n",
    "target = \"rv_lv_ratio_lt_1\"\n",
    "\n",
    "print(\"target\", target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['count_over0.1', 'count_over0.1_ratio', 'count_over0.2', 'count_over0.2_ratio', 'count_over0.3', 'count_over0.3_ratio', 'count_over0.4', 'count_over0.4_ratio', 'count_over0.5', 'count_over0.5_ratio', 'count_over0.6', 'count_over0.6_ratio', 'count_over0.7', 'count_over0.7_ratio', 'count_over0.8', 'count_over0.8_ratio', 'count_over0.9', 'count_over0.9_ratio', 'max', 'mean', 'monai_acute_and_chronic_pe', 'monai_central_pe', 'monai_chronic_pe', 'monai_leftsided_pe', 'monai_rightsided_pe', 'monai_rv_lv_ratio_gte_1', 'percentile30', 'percentile50', 'percentile70', 'percentile80', 'percentile90', 'percentile95', 'percentile99']\n"
     ]
    }
   ],
   "source": [
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "~~~~~~~~~~~~~~~~~~~~~ rv_lv_ratio_gte_1 ~~~~~~~~~~~~~~~~~ \n",
      "\n",
      "==================0================\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[35]\ttrain's binary_logloss: 0.151137\tval's binary_logloss: 0.220998\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[33]\ttrain's binary_logloss: 0.153356\tval's binary_logloss: 0.23821\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[35]\ttrain's binary_logloss: 0.154339\tval's binary_logloss: 0.215908\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[23]\ttrain's binary_logloss: 0.169859\tval's binary_logloss: 0.2516\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[30]\ttrain's binary_logloss: 0.161736\tval's binary_logloss: 0.227675\n",
      "--------------------------------------------------------------------roc: 0.8960575028613815\n",
      "----------------------------------------------------------------logloss: 0.23087880441542427\n",
      "~~~~~~~~~~~~~~~~~~~~~ central_pe ~~~~~~~~~~~~~~~~~ \n",
      "\n",
      "==================0================\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[40]\ttrain's binary_logloss: 0.0602286\tval's binary_logloss: 0.112004\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[38]\ttrain's binary_logloss: 0.0624626\tval's binary_logloss: 0.116626\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[28]\ttrain's binary_logloss: 0.0724661\tval's binary_logloss: 0.119511\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[29]\ttrain's binary_logloss: 0.0689511\tval's binary_logloss: 0.130648\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[32]\ttrain's binary_logloss: 0.068241\tval's binary_logloss: 0.114786\n",
      "--------------------------------------------------------------------roc: 0.9353602399932127\n",
      "----------------------------------------------------------------logloss: 0.11871569461154911\n",
      "~~~~~~~~~~~~~~~~~~~~~ leftsided_pe ~~~~~~~~~~~~~~~~~ \n",
      "\n",
      "==================0================\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[37]\ttrain's binary_logloss: 0.206677\tval's binary_logloss: 0.299809\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[37]\ttrain's binary_logloss: 0.20518\tval's binary_logloss: 0.30019\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[32]\ttrain's binary_logloss: 0.220305\tval's binary_logloss: 0.28403\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[37]\ttrain's binary_logloss: 0.203742\tval's binary_logloss: 0.297638\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[45]\ttrain's binary_logloss: 0.192884\tval's binary_logloss: 0.287988\n",
      "--------------------------------------------------------------------roc: 0.893217664011998\n",
      "----------------------------------------------------------------logloss: 0.29393169334910074\n",
      "~~~~~~~~~~~~~~~~~~~~~ rightsided_pe ~~~~~~~~~~~~~~~~~ \n",
      "\n",
      "==================0================\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[34]\ttrain's binary_logloss: 0.224153\tval's binary_logloss: 0.307771\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[41]\ttrain's binary_logloss: 0.212425\tval's binary_logloss: 0.275249\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[28]\ttrain's binary_logloss: 0.231839\tval's binary_logloss: 0.313407\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[22]\ttrain's binary_logloss: 0.251121\tval's binary_logloss: 0.318742\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[47]\ttrain's binary_logloss: 0.196303\tval's binary_logloss: 0.307036\n",
      "--------------------------------------------------------------------roc: 0.9075168023686159\n",
      "----------------------------------------------------------------logloss: 0.3044407011747937\n",
      "~~~~~~~~~~~~~~~~~~~~~ acute_and_chronic_pe ~~~~~~~~~~~~~~~~~ \n",
      "\n",
      "==================0================\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[13]\ttrain's binary_logloss: 0.0618672\tval's binary_logloss: 0.089252\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[14]\ttrain's binary_logloss: 0.0577632\tval's binary_logloss: 0.0939253\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[17]\ttrain's binary_logloss: 0.0556255\tval's binary_logloss: 0.0777966\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[14]\ttrain's binary_logloss: 0.057987\tval's binary_logloss: 0.0891955\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[19]\ttrain's binary_logloss: 0.0524648\tval's binary_logloss: 0.0870538\n",
      "--------------------------------------------------------------------roc: 0.8000406020707056\n",
      "----------------------------------------------------------------logloss: 0.08744468785933844\n",
      "~~~~~~~~~~~~~~~~~~~~~ chronic_pe ~~~~~~~~~~~~~~~~~ \n",
      "\n",
      "==================0================\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[20]\ttrain's binary_logloss: 0.110956\tval's binary_logloss: 0.136648\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[9]\ttrain's binary_logloss: 0.134798\tval's binary_logloss: 0.148823\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[17]\ttrain's binary_logloss: 0.108205\tval's binary_logloss: 0.183378\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[8]\ttrain's binary_logloss: 0.13256\tval's binary_logloss: 0.170174\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[31]\ttrain's binary_logloss: 0.0882696\tval's binary_logloss: 0.157045\n",
      "--------------------------------------------------------------------roc: 0.6545918937518012\n",
      "----------------------------------------------------------------logloss: 0.15921379744669392\n",
      "~~~~~~~~~~~~~~~~~~~~~ rv_lv_ratio_lt_1 ~~~~~~~~~~~~~~~~~ \n",
      "\n",
      "==================0================\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[29]\ttrain's binary_logloss: 0.262296\tval's binary_logloss: 0.330198\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[23]\ttrain's binary_logloss: 0.273733\tval's binary_logloss: 0.331098\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[22]\ttrain's binary_logloss: 0.270007\tval's binary_logloss: 0.348787\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[23]\ttrain's binary_logloss: 0.26795\tval's binary_logloss: 0.348821\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[34]\ttrain's binary_logloss: 0.247471\tval's binary_logloss: 0.327525\n",
      "--------------------------------------------------------------------roc: 0.8346626868345485\n",
      "----------------------------------------------------------------logloss: 0.33728695888566423\n"
     ]
    }
   ],
   "source": [
    "\"\"\" WITH MONAI FEATURE \"\"\"\n",
    "\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "import pickle\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "for target in targets_monai + ['rv_lv_ratio_lt_1']:\n",
    "    print(f\"~~~~~~~~~~~~~~~~~~~~~ {target} ~~~~~~~~~~~~~~~~~ \\n\")\n",
    "\n",
    "\n",
    "\n",
    "    oof_preds_list = []\n",
    "    test_preds_list = []\n",
    "    models_list = []\n",
    "\n",
    "    for i in range(1):\n",
    "        print(f'=================={i}================')\n",
    "        params = {'boosting_type': 'gbdt',\n",
    "            'objective': 'binary',\n",
    "    #             'metric': 'None',\n",
    "            'subsample': 0.75,\n",
    "            'subsample_freq': 1,\n",
    "            'learning_rate': 0.1,\n",
    "            'feature_fraction': 0.9,\n",
    "            'max_depth': 15,\n",
    "            'lambda_l1': 1,  \n",
    "            'lambda_l2': 1,\n",
    "            'verbose': 100,\n",
    "            'early_stopping_rounds': 100,\n",
    "            'verbose': -1,\n",
    "            } \n",
    "            \n",
    "        oof_preds = np.zeros(train_grouped.shape[0])\n",
    "\n",
    "        ### test_preds = np.zeros(test_grouped.shape[0])\n",
    "        val_results = {}\n",
    "        models = []\n",
    "        params['random_state'] = i\n",
    "        iter = 100000\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=72)\n",
    "\n",
    "        for n_fold in range( 5 ):\n",
    "            tr = train_grouped[train_grouped.fold != n_fold]\n",
    "            val = train_grouped[train_grouped.fold == n_fold]\n",
    "            trn_data = lgb.Dataset(tr[features], label=tr[target])\n",
    "            val_data = lgb.Dataset(val[features], label=val[target])\n",
    "            \n",
    "            clf = lgb.train(params, trn_data, num_boost_round=iter, valid_sets=[trn_data, val_data], valid_names=['train', 'val'],\n",
    "    #                         feval=feval, verbose_eval=10, early_stopping_rounds = 10/params['learning_rate'], evals_result=val_results,)\n",
    "                            verbose_eval=200, early_stopping_rounds = 10/params['learning_rate'], evals_result=val_results,)\n",
    "            file = f'lgbs/{target}_monai_lgb_seed{i}_fold{n_fold}.pkl'\n",
    "            pickle.dump(clf, open(file, 'wb'))\n",
    "            models.append(clf)\n",
    "            \n",
    "            oof_preds[train_grouped.fold == n_fold] = clf.predict(val[features])\n",
    "        oof_preds_list.append(oof_preds)\n",
    "\n",
    "    print(f'--------------------------------------------------------------------roc: {roc_auc_score(train_grouped[target], np.mean(oof_preds_list, axis=0))}')\n",
    "    print(f'----------------------------------------------------------------logloss: {log_loss(train_grouped[target], np.mean(oof_preds_list, axis=0))}')\n",
    "    lgb_oof_exam = np.mean(oof_preds_list, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'WITHOUT MONAI'"
      ]
     },
     "metadata": {},
     "execution_count": 197
    }
   ],
   "source": [
    "\"\"\"WITHOUT MONAI\"\"\"\n",
    "\n",
    "# features_without_monai = [e for e in features if not e.startswith(\"monai_\")]\n",
    "# print( features_without_monai )\n",
    "# features = sorted( features_without_monai )\n",
    "\n",
    "# import lightgbm as lgb\n",
    "# import numpy as np\n",
    "# from sklearn.metrics import roc_auc_score\n",
    "# import warnings\n",
    "# warnings.simplefilter('ignore')\n",
    "# import pickle\n",
    "# from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "# oof_preds_list = []\n",
    "# test_preds_list = []\n",
    "# models_list = []\n",
    "\n",
    "# for i in range(1):\n",
    "#     print(f'=================={i}================')\n",
    "#     params = {'boosting_type': 'gbdt',\n",
    "#         'objective': 'binary',\n",
    "# #             'metric': 'None',\n",
    "#         'subsample': 0.75,\n",
    "#         'subsample_freq': 1,\n",
    "#         'learning_rate': 0.1,\n",
    "#         'feature_fraction': 0.9,\n",
    "#         'max_depth': 15,\n",
    "#         'lambda_l1': 1,  \n",
    "#         'lambda_l2': 1,\n",
    "#         'verbose': 100,\n",
    "#         'early_stopping_rounds': 100,\n",
    "#         'verbose': -1,\n",
    "#         } \n",
    "        \n",
    "#     oof_preds = np.zeros(train_grouped.shape[0])\n",
    "\n",
    "#     ### test_preds = np.zeros(test_grouped.shape[0])\n",
    "#     val_results = {}\n",
    "#     models = []\n",
    "#     params['random_state'] = i\n",
    "#     iter = 100000\n",
    "#     kf = KFold(n_splits=5, shuffle=True, random_state=72)\n",
    "\n",
    "#     for n_fold in range( 5 ):\n",
    "#         tr = train_grouped[train_grouped.fold != n_fold]\n",
    "#         val = train_grouped[train_grouped.fold == n_fold]\n",
    "#         trn_data = lgb.Dataset(tr[features_without_monai], label=tr[target])\n",
    "#         val_data = lgb.Dataset(val[features_without_monai], label=val[target])\n",
    "        \n",
    "#         clf = lgb.train(params, trn_data, num_boost_round=iter, valid_sets=[trn_data, val_data], valid_names=['train', 'val'],\n",
    "# #                         feval=feval, verbose_eval=10, early_stopping_rounds = 10/params['learning_rate'], evals_result=val_results,)\n",
    "#                         verbose_eval=200, early_stopping_rounds = 10/params['learning_rate'], evals_result=val_results,)\n",
    "#         file = f'lgbs/posexam_lgb_seed{i}_fold{n_fold}.pkl'\n",
    "#         pickle.dump(clf, open(file, 'wb'))\n",
    "#         models.append(clf)\n",
    "        \n",
    "#         oof_preds[train_grouped.fold == n_fold] = clf.predict(val[features_without_monai])\n",
    "#     oof_preds_list.append(oof_preds)\n",
    "\n",
    "# print(f'--------------------------------------------------------------------roc: {roc_auc_score(train_grouped[target], np.mean(oof_preds_list, axis=0))}')\n",
    "# print(f'----------------------------------------------------------------logloss: {log_loss(train_grouped[target], np.mean(oof_preds_list, axis=0))}')\n",
    "# lgb_oof_exam = np.mean(oof_preds_list, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.5202781558036804"
      ]
     },
     "metadata": {},
     "execution_count": 198
    }
   ],
   "source": [
    "bce_func = torch.nn.BCELoss(reduction='mean')\n",
    "lgb_losses = bce_func(torch.FloatTensor(oof_preds), torch.FloatTensor(train_grouped['positive_exam_for_pe']))\n",
    "\n",
    "torch.mean(lgb_losses).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.5339002013206482"
      ]
     },
     "metadata": {},
     "execution_count": 199
    }
   ],
   "source": [
    "bce_func = torch.nn.BCELoss(reduction='mean')\n",
    "lgb_losses = bce_func(\n",
    "    ( 1 - torch.FloatTensor(oof_preds) ) * (4911) / (4911 + 157), \n",
    "    torch.FloatTensor(train_grouped['negative_exam_for_pe']))\n",
    "\n",
    "torch.mean(lgb_losses).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # current yama's pipeline for fold0-ep1\n",
    "# def calib_p(arr, factor):  # set factor>1 to enhance positive prob\n",
    "#     return arr * factor / (arr * factor + (1-arr))\n",
    "# def post_yama(arr):\n",
    "#     return calib_p( np.percentile(arr, 95), factor=1/8.5550)\n",
    "\n",
    "# lgb_losses = bce_func(torch.FloatTensor(train[['StudyInstanceUID','pred']].groupby('StudyInstanceUID').apply(post_yama)), torch.FloatTensor(train_grouped['positive_exam_for_pe'])).item()\n",
    "# lgb_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}